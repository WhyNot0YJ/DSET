#!/usr/bin/env python3
"""è‡ªé€‚åº”ä¸“å®¶RT-DETRè®­ç»ƒè„šæœ¬ - DAIR-V2Xæ•°æ®é›†

ç»†ç²’åº¦MoEæ¶æ„ï¼šDecoder FFNå±‚é›†æˆè‡ªé€‚åº”ä¸“å®¶å±‚

ä¸»è¦ç‰¹æ€§ï¼š
- ç»†ç²’åº¦MoEï¼šæ¯ä¸ªDecoderå±‚çš„FFNä½¿ç”¨è‡ªé€‚åº”ä¸“å®¶å±‚
- æ”¯æŒå¤šç§backboneæ¶æ„
- æ··åˆç²¾åº¦è®­ç»ƒ
- EMAæ¨¡å‹
- å­¦ä¹ ç‡é¢„çƒ­
- COCOæ ¼å¼è¯„ä¼°
- æ£€æŸ¥ç‚¹æ¢å¤
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import re
from torch.utils.data import DataLoader
from pathlib import Path
import logging
from datetime import datetime
import json
import numpy as np
from typing import Dict, List, Tuple, Optional
from pycocotools.cocoeval import COCOeval
from pycocotools.coco import COCO

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.resolve()
# ç¡®ä¿å½“å‰å·¥ä½œç›®å½•åœ¨è·¯å¾„ä¸­ï¼ˆé‡è¦ï¼šå½“ä»ä¸åŒç›®å½•è¿è¡Œæ—¶ï¼‰
if str(os.getcwd()) not in sys.path:
    sys.path.insert(0, os.getcwd())
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root.parent))  # æ·»åŠ experimentsç›®å½•

# å¯¼å…¥éšæœºç§å­å·¥å…·
from seed_utils import set_seed, seed_worker

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.misc.training_visualizer import TrainingVisualizer
from src.misc.early_stopping import EarlyStopping

# å¯¼å…¥RT-DETRç»„ä»¶
from src.zoo.rtdetr import HybridEncoder, RTDETRTransformerv2, RTDETRCriterionv2, HungarianMatcher
from src.nn.backbone.presnet import PResNet
from src.nn.backbone.hgnetv2 import HGNetv2
from src.nn.backbone.csp_resnet import CSPResNet
from src.nn.backbone.csp_darknet import CSPDarkNet
from src.nn.backbone.test_resnet import MResNet

# å¯¼å…¥ä¼˜åŒ–å™¨å¢å¼ºæ¨¡å—
from src.optim.ema import ModelEMA
from src.optim.amp import GradScaler
from src.optim.warmup import WarmupLR

# å¯¼å…¥DAIR-V2Xæ•°æ®é›†
from src.data.dataset.dairv2x_detection import DAIRV2XDetection


def create_backbone(backbone_type: str, **kwargs) -> nn.Module:
    """åˆ›å»ºbackboneçš„å·¥å‚å‡½æ•°ã€‚
    
    Args:
        backbone_type: backboneç±»å‹ï¼ˆpresnet18/34/50/101, hgnetv2_lç­‰ï¼‰
        **kwargs: backboneç‰¹å®šå‚æ•°ï¼ˆä¼šè¦†ç›–é»˜è®¤é…ç½®ï¼‰
    
    Returns:
        nn.Module: backboneæ¨¡å‹å®ä¾‹
        
    Raises:
        ValueError: ä¸æ”¯æŒçš„backboneç±»å‹
    """
    # PResNeté…ç½®ï¼ˆé€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è§£ædepthï¼‰
    if backbone_type.startswith('presnet'):
        depth_match = re.search(r'(\d+)', backbone_type)
        if depth_match:
            depth = int(depth_match.group(1))
        else:
            raise ValueError(f"æ— æ³•ä»backboneç±»å‹ {backbone_type} è§£ædepth")
        
        default_params = {
            'depth': depth,
            'variant': 'd',
            'return_idx': [1, 2, 3],
            'freeze_at': -1,  # moe-rtdeträ¸ä½¿ç”¨å†»ç»“
            'freeze_norm': False,
            'pretrained': False
        }
        default_params.update(kwargs)
        return PResNet(**default_params)
    
    # HGNetv2é…ç½®
    elif backbone_type.startswith('hgnetv2'):
        name_map = {'hgnetv2_l': 'L', 'hgnetv2_x': 'X', 'hgnetv2_h': 'H'}
        if backbone_type not in name_map:
            raise ValueError(f"ä¸æ”¯æŒçš„HGNetv2ç±»å‹: {backbone_type}")
        
        default_params = {
            'name': name_map[backbone_type],
            'return_idx': [1, 2, 3],
            'freeze_at': -1,
            'freeze_norm': False,
            'pretrained': False
        }
        default_params.update(kwargs)
        return HGNetv2(**default_params)
    
    # CSPResNeté…ç½®
    elif backbone_type.startswith('cspresnet'):
        name_map = {'cspresnet_s': 's', 'cspresnet_m': 'm', 'cspresnet_l': 'l', 'cspresnet_x': 'x'}
        if backbone_type not in name_map:
            raise ValueError(f"ä¸æ”¯æŒçš„CSPResNetç±»å‹: {backbone_type}")
        
        default_params = {
            'name': name_map[backbone_type],
            'return_idx': [1, 2, 3],
            'pretrained': False
        }
        default_params.update(kwargs)
        return CSPResNet(**default_params)
    
    # CSPDarkNeté…ç½®
    elif backbone_type == 'cspdarknet':
        default_params = {'return_idx': [2, 3, -1]}
        default_params.update(kwargs)
        return CSPDarkNet(**default_params)
    
    # Modified ResNet
    elif backbone_type == 'mresnet':
        default_params = {'num_blocks': [2, 2, 2, 2]}
        default_params.update(kwargs)
        return MResNet(**default_params)
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„backboneç±»å‹: {backbone_type}")






class AdaptiveExpertRTDETR(nn.Module):
    """è‡ªé€‚åº”ä¸“å®¶RT-DETRæ¨¡å‹ï¼ˆç»†ç²’åº¦MoEæ¶æ„ï¼‰ã€‚
    
    æ¶æ„è®¾è®¡ï¼š
    1. å…±äº«Backboneï¼šæå–å¤šå°ºåº¦ç‰¹å¾
    2. å…±äº«Encoderï¼šå¢å¼ºç‰¹å¾è¡¨è¾¾
    3. è‡ªé€‚åº”ä¸“å®¶Decoderï¼šFFNå±‚ä½¿ç”¨AdaptiveExpertLayerï¼ˆæ¯å±‚ç‹¬ç«‹Router + Nä¸ªä¸“å®¶FFNï¼‰
    4. ç»Ÿä¸€è¾“å‡ºï¼šç›´æ¥è¾“å‡ºæ£€æµ‹ç»“æœï¼Œæ— éœ€é¢å¤–èåˆ
    """
    
    def __init__(self, config_name: str = "A", hidden_dim: int = 256, 
                 num_queries: int = 300, top_k: int = 2, backbone_type: str = "presnet34",
                 num_decoder_layers: int = 3, encoder_in_channels: list = None, 
                 encoder_expansion: float = 1.0, num_experts: int = None,
                 moe_balance_weight: float = None):
        """åˆå§‹åŒ–è‡ªé€‚åº”ä¸“å®¶RT-DETRæ¨¡å‹ã€‚
        
        Args:
            config_name: ä¸“å®¶é…ç½®åç§°ï¼ˆä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œä½†ä¸å†ç”¨äºç¡®å®šä¸“å®¶æ•°é‡ï¼‰
            hidden_dim: éšè—å±‚ç»´åº¦
            num_queries: æŸ¥è¯¢æ•°é‡
            top_k: è·¯ç”±å™¨Top-Ké€‰æ‹©
            backbone_type: Backboneç±»å‹
            num_decoder_layers: Decoderå±‚æ•°
            encoder_in_channels: Encoderè¾“å…¥é€šé“æ•°
            encoder_expansion: Encoder expansionå‚æ•°
            num_experts: ä¸“å®¶æ•°é‡ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼Œå¦‚æœæœªæä¾›åˆ™é€šè¿‡config_nameæ˜ å°„ï¼‰
            moe_balance_weight: MoEè´Ÿè½½å‡è¡¡æŸå¤±æƒé‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨è°ƒæ•´ï¼‰
        """
        super().__init__()
        
        self.config_name = config_name
        self.hidden_dim = hidden_dim
        self.num_queries = num_queries
        self.top_k = top_k
        self.backbone_type = backbone_type
        self.image_size = 640
        self.num_decoder_layers = num_decoder_layers
        
        # Encoderé…ç½®
        self.encoder_in_channels = encoder_in_channels or [512, 1024, 2048]
        self.encoder_expansion = encoder_expansion
        
        # âœ… MoEé…ç½®ï¼šæ”¯æŒè‡ªå®šä¹‰æƒé‡
        if moe_balance_weight is not None:
            self.moe_balance_weight = moe_balance_weight
        
        # è·å–ä¸“å®¶æ•°é‡ï¼šä¼˜å…ˆä½¿ç”¨ç›´æ¥ä¼ å…¥çš„num_expertsï¼Œå¦åˆ™é€šè¿‡config_nameæ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
        if num_experts is not None:
            self.num_experts = num_experts
        else:
            configs = {"A": 6, "B": 3, "C": 2}
            self.num_experts = configs.get(config_name, 6)
        
        # ========== å…±äº«ç»„ä»¶ ==========
        self.backbone = self._build_backbone()
        self.encoder = self._build_encoder()
        
        # ========== ç»†ç²’åº¦MoE Decoder ==========
        # ä½¿ç”¨ä¼ å…¥çš„decoderå±‚æ•°å‚æ•°
        
        self.decoder = RTDETRTransformerv2(
            num_classes=7,
            hidden_dim=hidden_dim,
            num_queries=num_queries,
            num_layers=num_decoder_layers,
            nhead=8,
            dim_feedforward=1024,
            dropout=0.1,
            activation='relu',
            feat_channels=[256, 256, 256],
            feat_strides=[8, 16, 32],
            num_levels=3,
            # ç»†ç²’åº¦MoEé…ç½®
            use_moe=True,
            num_experts=self.num_experts,
            moe_top_k=top_k
        )
        
        print(f"âœ“ MoE Decoderé…ç½®: {num_decoder_layers}å±‚, {self.num_experts}ä¸ªä¸“å®¶, top_k={top_k}")
        
        # RT-DETRæŸå¤±å‡½æ•°
        self.detr_criterion = self._build_detr_criterion()
        
    def _build_backbone(self) -> nn.Module:
        """æ„å»ºbackboneã€‚"""
        return create_backbone(self.backbone_type)
    
    def _build_encoder(self) -> nn.Module:
        """æ„å»ºencoder - ä½¿ç”¨é…ç½®å‚æ•°ã€‚"""
        input_size = [self.image_size, self.image_size]
        
        return HybridEncoder(
            in_channels=self.encoder_in_channels,
            feat_strides=[8, 16, 32],
            hidden_dim=256,
            use_encoder_idx=[2],
            num_encoder_layers=1,
            expansion=self.encoder_expansion,
            nhead=8,
            dropout=0.0,
            act='silu',
            eval_spatial_size=input_size
        )
    
    def _build_detr_criterion(self) -> RTDETRCriterionv2:
        """æ„å»ºRT-DETRæŸå¤±å‡½æ•°ã€‚"""
        matcher = HungarianMatcher(
            weight_dict={'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2},
            use_focal_loss=False,
            alpha=0.25,
            gamma=2.0
        )
        
        # ä¸»æŸå¤±æƒé‡
        main_weight_dict = {
            'loss_vfl': 1.0,
            'loss_bbox': 5.0,
            'loss_giou': 2.0
        }
        
        # âœ… ä¿®å¤ï¼šä»å®ä¾‹å˜é‡åŠ¨æ€è¯»å–decoderå±‚æ•°ï¼Œè€Œéç¡¬ç¼–ç 
        num_decoder_layers = self.num_decoder_layers
        aux_weight_dict = {}
        for i in range(num_decoder_layers - 1):  # å‰N-1å±‚
            aux_weight_dict[f'loss_vfl_aux_{i}'] = 1.0
            aux_weight_dict[f'loss_bbox_aux_{i}'] = 5.0
            aux_weight_dict[f'loss_giou_aux_{i}'] = 2.0
        
        # Encoderè¾…åŠ©æŸå¤±ï¼ˆé€šå¸¸1å±‚ï¼‰
        aux_weight_dict['loss_vfl_enc_0'] = 1.0
        aux_weight_dict['loss_bbox_enc_0'] = 5.0
        aux_weight_dict['loss_giou_enc_0'] = 2.0
        
        # Denoisingè¾…åŠ©æŸå¤±ï¼ˆå¦‚æœå¯ç”¨num_denoising>0ï¼‰
        # RT-DETRé»˜è®¤num_denoising=100ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦æ·»åŠ è¿™äº›æŸå¤±çš„æƒé‡
        # ä½¿ç”¨åŠ¨æ€è¯»å–çš„å±‚æ•°
        num_denoising_layers = num_decoder_layers  # å’Œdecoderå±‚æ•°ä¸€è‡´
        for i in range(num_denoising_layers):
            aux_weight_dict[f'loss_vfl_dn_{i}'] = 1.0
            aux_weight_dict[f'loss_bbox_dn_{i}'] = 5.0
            aux_weight_dict[f'loss_giou_dn_{i}'] = 2.0
        
        # åˆå¹¶æ‰€æœ‰æƒé‡
        weight_dict = {**main_weight_dict, **aux_weight_dict}
        
        criterion = RTDETRCriterionv2(
            matcher=matcher,
            weight_dict=weight_dict,
            losses=['vfl', 'boxes'],
            alpha=0.75,
            gamma=2.0,
            num_classes=7,
            boxes_weight_format=None,
            share_matched_indices=False
        )
        
        return criterion
    
    
    def forward(self, images: torch.Tensor, 
                targets: Optional[List[Dict]] = None) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­ã€‚
        
        Args:
            images: [B, C, H, W] è¾“å…¥å›¾åƒ
            targets: è®­ç»ƒç›®æ ‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            Dict: åŒ…å«æ£€æµ‹ç»“æœå’ŒæŸå¤±çš„å­—å…¸
        """
        # å…±äº«ç‰¹å¾æå–
        backbone_features = self.backbone(images)
        encoder_features = self.encoder(backbone_features)
        
        # MoE Decoderå‰å‘ï¼ˆå†…éƒ¨è‡ªåŠ¨å¤„ç†è·¯ç”±å’Œä¸“å®¶èåˆï¼‰
        decoder_output = self.decoder(encoder_features, targets)
        
        # æ„å»ºè¾“å‡ºå­—å…¸
        output = {
            'pred_logits': decoder_output.get('pred_logits'),
            'pred_boxes': decoder_output.get('pred_boxes'),
            'bboxes': decoder_output.get('pred_boxes'),
            'class_scores': decoder_output.get('pred_logits'),
        }
        
        if targets is not None:
            # è®¡ç®—æ£€æµ‹æŸå¤±ï¼ˆè®­ç»ƒå’ŒéªŒè¯éƒ½éœ€è¦ï¼‰
            detection_loss_dict = self.detr_criterion(decoder_output, targets)
            detection_loss = sum(v for v in detection_loss_dict.values() 
                               if isinstance(v, torch.Tensor))
            
            # è·å–MoEè´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
            if self.training:
                moe_load_balance_loss = decoder_output.get('moe_load_balance_loss', 
                                                          torch.tensor(0.0, device=images.device))
            else:
                moe_load_balance_loss = torch.tensor(0.0, device=images.device)
            
            # æ€»æŸå¤±ï¼šæ£€æµ‹æŸå¤± + MoEè´Ÿè½½å‡è¡¡æŸå¤±
            # æ”¯æŒä»å®ä¾‹å˜é‡è¯»å–MoEæƒé‡ï¼ˆå¦‚æœè®¾ç½®ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
            if hasattr(self, 'moe_balance_weight'):
                balance_weight = self.moe_balance_weight
            else:
                # åŠ¨æ€è°ƒæ•´MoEæŸå¤±æƒé‡ï¼ˆtop_k=1æ—¶éœ€è¦æ›´å¼ºçš„çº¦æŸï¼‰
                if hasattr(self.decoder, 'moe_top_k') and self.decoder.moe_top_k == 1:
                    balance_weight = 0.1  # top_k=1æ—¶ä½¿ç”¨æ›´å¤§çš„æƒé‡
                else:
                    balance_weight = 0.05  # top_k>1æ—¶ä½¿ç”¨è¾ƒå°çš„æƒé‡
            
            total_loss = detection_loss + balance_weight * moe_load_balance_loss
            
            output['detection_loss'] = detection_loss
            output['moe_load_balance_loss'] = moe_load_balance_loss
            output['total_loss'] = total_loss
            output['loss_dict'] = detection_loss_dict
        
        return output


class AdaptiveExpertTrainer:
    """è‡ªé€‚åº”ä¸“å®¶RT-DETRè®­ç»ƒå™¨ã€‚
    
    è´Ÿè´£æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€æ£€æŸ¥ç‚¹ç®¡ç†ç­‰åŠŸèƒ½ã€‚
    """
    
    def __init__(self, config: Dict, config_file_path: Optional[str] = None):
        """åˆå§‹åŒ–è®­ç»ƒå™¨ã€‚
        
        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
            config_file_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰ï¼Œç”¨äºéªŒè¯
        """
        self.config = config
        self.config_file_path = config_file_path
        
        # å¦‚æœä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ŒéªŒè¯å¿…éœ€çš„é…ç½®é¡¹
        if config_file_path:
            self._validate_config_file()
        
        # å¦‚æœä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œdeviceå¿…é¡»å­˜åœ¨ï¼Œå¦åˆ™æŠ¥é”™
        if config_file_path:
            if 'misc' not in self.config or 'device' not in self.config['misc']:
                raise ValueError(f"é…ç½®æ–‡ä»¶ {config_file_path} ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹: misc.device")
            device_str = self.config['misc']['device']
        else:
            device_str = self.config.get('misc', {}).get('device', 'cuda')
        self.device = torch.device(device_str)
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.best_map = 0.0  # è®°å½•æœ€ä½³mAP
        self.global_step = 0
        # ä»é…ç½®ä¸­è¯»å– resume_from_checkpointï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰
        self.resume_from_checkpoint = self.config.get('resume_from_checkpoint', None)
        if self.resume_from_checkpoint is None and 'checkpoint' in self.config:
            self.resume_from_checkpoint = self.config['checkpoint'].get('resume_from_checkpoint', None)
        
        # æ¢¯åº¦è£å‰ªå‚æ•°ï¼ˆä»é…ç½®è¯»å–ï¼‰
        self.clip_max_norm = self.config.get('training', {}).get('clip_max_norm', 10.0)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._setup_logging()
        self.model = self._create_model()
        self.train_loader, self.val_loader = self._create_data_loaders()
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        self.warmup_scheduler = self._create_warmup_scheduler()
        self.ema = self._create_ema()
        self.scaler = self._create_scaler()
        self.visualizer = TrainingVisualizer(log_dir=self.log_dir, model_type='moe', experiment_name=self.experiment_name)
        self.early_stopping = self._create_early_stopping()
        
        # æ¢å¤æ£€æŸ¥ç‚¹
        if self.resume_from_checkpoint:
            self._resume_from_checkpoint()
    
    def _validate_config_file(self):
        """éªŒè¯é…ç½®æ–‡ä»¶æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„é…ç½®é¡¹"""
        required_keys = {
            'model': ['config_name', 'backbone', 'hidden_dim', 'num_queries', 'num_decoder_layers', 'top_k'],
            'training': ['epochs', 'batch_size', 'pretrained_lr', 'new_lr', 'warmup_epochs'],
            'data': ['data_root'],
            'misc': ['device', 'num_workers']
        }
        
        missing_keys = []
        for section, keys in required_keys.items():
            if section not in self.config:
                missing_keys.append(f"ç¼ºå°‘é…ç½®èŠ‚: {section}")
                continue
            for key in keys:
                if key not in self.config[section]:
                    missing_keys.append(f"{section}.{key}")
        
        if missing_keys:
            error_msg = f"é…ç½®æ–‡ä»¶ {self.config_file_path} ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹:\n"
            error_msg += "\n".join(f"  - {key}" for key in missing_keys)
            raise ValueError(error_msg)
    
    def _setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿã€‚"""
        if self.resume_from_checkpoint:
            checkpoint_path = Path(self.resume_from_checkpoint)
            self.log_dir = checkpoint_path.parent
            # ä»ç›®å½•åä¸­æå–å®éªŒåç§°ï¼ˆå»æ‰æ—¶é—´æˆ³éƒ¨åˆ†ï¼‰
            dir_name = self.log_dir.name
            # å‡è®¾æ ¼å¼ä¸º moe6_rtdetr_r50_20240101_120000ï¼Œæå– moe6_rtdetr_r50
            parts = dir_name.rsplit('_', 2)  # åˆ†å‰²æœ€åä¸¤éƒ¨åˆ†ï¼ˆæ—¥æœŸå’Œæ—¶é—´ï¼‰
            if len(parts) >= 2:
                self.experiment_name = '_'.join(parts[:-2]) if len(parts) > 2 else parts[0]
            else:
                self.experiment_name = dir_name
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # ä»é…ç½®ä¸­è·å–backboneç±»å‹ï¼ŒåŠ å…¥åˆ°ç›®å½•åä¸­
            backbone_type = self.config.get('model', {}).get('backbone', 'unknown')
            # ç§»é™¤presnetå‰ç¼€ï¼Œåªä¿ç•™æ•°å­—éƒ¨åˆ†ï¼ˆå¦‚presnet18 -> r18, presnet34 -> r34ï¼‰
            backbone_short = backbone_type.replace('presnet', 'r').replace('pres', 'r') if 'presnet' in backbone_type or 'pres' in backbone_type else backbone_type
            # ç›´æ¥ä»é…ç½®æ–‡ä»¶è¯»å–ä¸“å®¶æ•°é‡ï¼Œå¦‚æœæœªé…ç½®åˆ™é€šè¿‡config_nameæ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
            num_experts = self.config.get('model', {}).get('num_experts', None)
            if num_experts is None:
                # å‘åå…¼å®¹ï¼šé€šè¿‡config_nameæ˜ å°„
                config_name = self.config.get('model', {}).get('config_name', 'A')
                configs = {'A': 6, 'B': 3, 'C': 2}
                num_experts = configs.get(config_name, 6)
            expert_num = str(num_experts)
            # ç”Ÿæˆå®éªŒåç§°ï¼ˆä¸å¸¦æ—¶é—´æˆ³ï¼‰
            self.experiment_name = f"moe{expert_num}_rtdetr_{backbone_short}"
            self.log_dir = Path(f"logs/{self.experiment_name}_{timestamp}")
            self.log_dir.mkdir(parents=True, exist_ok=True)
        
        handlers = [
            logging.FileHandler(self.log_dir / 'training.log', mode='a'),
            logging.StreamHandler()
        ]
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=handlers,
            force=True
        )
        
        self.logger = logging.getLogger(__name__)
        
        if self.resume_from_checkpoint:
            self.logger.info(f"æ¢å¤è®­ç»ƒï¼Œæ—¥å¿—ç›®å½•: {self.log_dir}")
        
        if not self.resume_from_checkpoint:
            with open(self.log_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, default_flow_style=False)
    
    def _create_model(self) -> AdaptiveExpertRTDETR:
        """åˆ›å»ºæ¨¡å‹ã€‚"""
        # ä»é…ç½®æ–‡ä»¶è¯»å–encoderé…ç½®
        encoder_config = self.config['model']['encoder']
        encoder_in_channels = encoder_config['in_channels']
        encoder_expansion = encoder_config['expansion']
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–ä¸“å®¶æ•°é‡ï¼Œå¦‚æœæœªé…ç½®åˆ™ä½¿ç”¨Noneï¼ˆä¼šé€šè¿‡config_nameæ˜ å°„ï¼‰
        num_experts = self.config['model'].get('num_experts', None)
        
        # âœ… ä»é…ç½®æ–‡ä»¶è¯»å–MoEæƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        moe_balance_weight = self.config.get('training', {}).get('moe_balance_weight', None)
        
        model = AdaptiveExpertRTDETR(
            config_name=self.config['model'].get('config_name', 'A'),
            hidden_dim=self.config['model']['hidden_dim'],
            num_queries=self.config['model']['num_queries'],
            top_k=self.config['model']['top_k'],
            backbone_type=self.config['model']['backbone'],
            num_decoder_layers=self.config['model']['num_decoder_layers'],
            encoder_in_channels=encoder_in_channels,
            encoder_expansion=encoder_expansion,
            num_experts=num_experts,
            moe_balance_weight=moe_balance_weight
        )
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        pretrained_weights = self.config['model'].get('pretrained_weights', None)
        if pretrained_weights:
            self._load_pretrained_weights(model, pretrained_weights)
        
        model = model.to(self.device)
        
        self.logger.info(f"âœ“ åˆ›å»ºMOE RT-DETRæ¨¡å‹")
        self.logger.info(f"  ä¸“å®¶æ•°é‡: {model.num_experts}")
        self.logger.info(f"  é…ç½®: {model.config_name}")
        self.logger.info(f"  Backbone: {model.backbone_type}")
        self.logger.info(f"  Encoder: in_channels={encoder_in_channels}, expansion={encoder_expansion}")
        
        return model
    
    def _load_pretrained_weights(self, model: AdaptiveExpertRTDETR, pretrained_path: str) -> None:
        """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæƒé‡
        
        Args:
            pretrained_path: æœ¬åœ°æƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ 'pretrained/rtdetrv2_r50vd_6x_coco_ema.pth'ï¼‰
        """
        try:
            pretrained_file = Path(pretrained_path)
            if not pretrained_file.exists():
                self.logger.warning(f"é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {pretrained_path}")
                self.logger.info("å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")
                return
            
            self.logger.info(f"ä»æœ¬åœ°æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
            checkpoint = torch.load(pretrained_file, map_location='cpu')
            
            # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
            if isinstance(checkpoint, dict):
                if 'ema' in checkpoint and 'module' in checkpoint['ema']:
                    # EMAæ ¼å¼: {'ema': {'module': {...}}}
                    state_dict = checkpoint['ema']['module']
                    self.logger.info("âœ“ æ£€æµ‹åˆ°EMA checkpointæ ¼å¼")
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                elif 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            # è¿‡æ»¤æ‰ç±»åˆ«ç›¸å…³å‚æ•°ï¼ˆå½¢çŠ¶ä¸åŒ¹é…ï¼‰
            filtered_state_dict = {}
            skipped_class_params = 0
            
            for k, v in state_dict.items():
                # è·³è¿‡ç±»åˆ«ç›¸å…³çš„å‚æ•°ï¼ˆè¿™äº›å‚æ•°çš„å½¢çŠ¶ä¼šä¸åŒ¹é…ï¼‰
                if any(keyword in k for keyword in ['class_embed', 'score_head', 'denoising_class_embed']):
                    skipped_class_params += 1
                    continue
                filtered_state_dict[k] = v
            
            # åŠ è½½è¿‡æ»¤åçš„å‚æ•°
            missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)
            
            # ç»Ÿè®¡åŠ è½½ç»“æœ
            total_params = len(filtered_state_dict)
            loaded_params = total_params - len(missing_keys)
            
            self.logger.info(f"âœ“ æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {loaded_params}/{total_params} ä¸ªå‚æ•°")
            
            # æŠ¥å‘Šè·³è¿‡çš„ç±»åˆ«å‚æ•°
            if skipped_class_params > 0:
                self.logger.info(f"  - è·³è¿‡ç±»åˆ«ç›¸å…³å‚æ•°: {skipped_class_params} ä¸ªï¼ˆCOCO 80ç±» â†’ DAIR-V2X 7ç±»ï¼‰")
            
            # ç»Ÿè®¡å„éƒ¨åˆ†çš„å‚æ•°
            backbone_loaded = sum(1 for k in filtered_state_dict.keys() if k not in missing_keys and 'backbone' in k)
            encoder_loaded = sum(1 for k in filtered_state_dict.keys() if k not in missing_keys and 'encoder' in k)
            decoder_loaded = sum(1 for k in filtered_state_dict.keys() if k not in missing_keys and 'decoder' in k)
            
            self.logger.info(f"  - Backbone: {backbone_loaded} ä¸ªå‚æ•°")
            self.logger.info(f"  - Encoder: {encoder_loaded} ä¸ªå‚æ•°")
            self.logger.info(f"  - Decoder: {decoder_loaded} ä¸ªå‚æ•°")
            
            if len(missing_keys) > 0:
                self.logger.info(f"  - é¢„è®­ç»ƒæ¨¡å‹ç¼ºå°‘å‚æ•°: {len(missing_keys)} ä¸ªï¼ˆå½“å‰æ¨¡å‹æ–°å¢ï¼‰")
                if len(missing_keys) <= 5:
                    self.logger.info(f"    ç¤ºä¾‹: {list(missing_keys)}")
                else:
                    self.logger.info(f"    ç¤ºä¾‹: {list(missing_keys)[:3]} ...")
            
            if len(unexpected_keys) > 0:
                self.logger.info(f"  - æ¨¡å‹æ–°å¢å‚æ•°: {len(unexpected_keys)} ä¸ªï¼ˆå°†éšæœºåˆå§‹åŒ–ï¼‰")
                
        except Exception as e:
            self.logger.error(f"âœ— åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
            self.logger.info("å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")
    
    def _create_data_loaders(self) -> Tuple[DataLoader, DataLoader]:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨ã€‚"""
        # ä¿®æ”¹ï¼šç§»é™¤ä¸å¿…è¦çš„max()ï¼Œä½¿ç”¨é…ç½®å€¼
        batch_size = self.config['training']['batch_size']
        target_size = self.model.image_size
        
        # ä¿®æ”¹ï¼šè®­ç»ƒæ—¶å¯ç”¨mosaicå¢å¼º
        use_mosaic = self.config['training'].get('use_mosaic', True)
        
        train_dataset = DAIRV2XDetection(
            data_root=self.config['data']['data_root'],
            split='train',
            use_mosaic=use_mosaic,
            target_size=target_size
        )
        
        val_dataset = DAIRV2XDetection(
            data_root=self.config['data']['data_root'],
            split='val',
            use_mosaic=False,
            target_size=target_size
        )
        
        # ä»miscé…ç½®ä¸­è¯»å–num_workerså’Œpin_memory
        num_workers = self.config.get('misc', {}).get('num_workers', 8)
        pin_memory = self.config.get('misc', {}).get('pin_memory', True)
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=num_workers,
            collate_fn=self._collate_fn,
            pin_memory=pin_memory,
            persistent_workers=True,
            prefetch_factor=2
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size, 
            shuffle=False,
            num_workers=num_workers,
            collate_fn=self._collate_fn,
            pin_memory=pin_memory,
            persistent_workers=True,
            prefetch_factor=2
        )
        
        self.val_dataset = val_dataset
        
        self.logger.info(f"âœ“ åˆ›å»ºæ•°æ®åŠ è½½å™¨")
        self.logger.info(f"  è®­ç»ƒé›†: {len(train_dataset)} | éªŒè¯é›†: {len(val_dataset)}")
        
        return train_loader, val_loader
    
    def _collate_fn(self, batch: List[Tuple]) -> Tuple[torch.Tensor, List[Dict]]:
        """æ•°æ®æ•´ç†å‡½æ•°ã€‚"""
        images, targets = zip(*batch)
        
        if isinstance(images[0], np.ndarray):
            images = torch.stack([
                torch.from_numpy(img).permute(2, 0, 1).float() / 255.0 
                for img in images
            ], dim=0)
        else:
            images = torch.stack(images, 0)
        
        return images, list(targets)
    
    def _create_optimizer(self) -> optim.Adam:
        """åˆ›å»ºä¼˜åŒ–å™¨ã€‚"""
        # é¢„è®­ç»ƒå‚æ•°ï¼šbackbone + encoder
        pretrained_params = list(self.model.backbone.parameters()) + \
                           list(self.model.encoder.parameters())
        
        # æ–°å‚æ•°ï¼šDecoderï¼ˆåŒ…å«å†…éƒ¨çš„è‡ªé€‚åº”ä¸“å®¶å±‚ï¼‰
        decoder_params = list(self.model.decoder.parameters())
        
        # ç¡®ä¿å­¦ä¹ ç‡æ˜¯æµ®ç‚¹æ•°ç±»å‹
        pretrained_lr = float(self.config['training']['pretrained_lr'])
        new_lr = float(self.config['training']['new_lr'])
        weight_decay = float(self.config['training'].get('weight_decay', 0.0001))
        
        optimizer = optim.Adam([
            {'params': pretrained_params, 'lr': pretrained_lr},
            {'params': decoder_params, 'lr': new_lr}
        ], weight_decay=weight_decay)
        
        self.logger.info(f"âœ“ åˆ›å»ºä¼˜åŒ–å™¨ (pretrained_lr={pretrained_lr}, new_lr={new_lr}, weight_decay={weight_decay})")
        self.logger.info(f"  é¢„è®­ç»ƒå‚æ•°: {len(pretrained_params)} | Decoderå‚æ•°: {len(decoder_params)}")
        
        return optimizer
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚"""
        scheduler_type = self.config.get('training', {}).get('scheduler', 'cosine')
        
        if scheduler_type == 'cosine':
            # ä»é…ç½®æ–‡ä»¶è¯»å–eta_minï¼Œé»˜è®¤1e-7
            eta_min = self.config.get('training', {}).get('eta_min', 1e-7)
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, 
                T_max=self.config['training']['epochs'],
                eta_min=eta_min
            )
            self.logger.info(f"âœ“ ä½¿ç”¨CosineAnnealingLRè°ƒåº¦å™¨ (eta_min={eta_min})")
        else:
            # MultiStepLR
            milestones = self.config.get('training', {}).get('milestones', [60, 80])
            gamma = float(self.config.get('training', {}).get('gamma', 0.1))
            scheduler = optim.lr_scheduler.MultiStepLR(
                self.optimizer,
                milestones=milestones,
                gamma=gamma
            )
            self.logger.info(f"âœ“ ä½¿ç”¨MultiStepLRè°ƒåº¦å™¨ (milestones={milestones})")
        
        return scheduler
    
    def _create_warmup_scheduler(self) -> WarmupLR:
        """åˆ›å»ºå­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨ã€‚"""
        # ä¿®æ”¹ï¼šwarmup epochsä»é»˜è®¤10æ”¹ä¸º3
        warmup_epochs = self.config.get('training', {}).get('warmup_epochs', 3)
        # ç¡®ä¿warmup_end_lræ˜¯æµ®ç‚¹æ•°
        warmup_end_lr = float(self.config['training']['new_lr'])
        warmup_scheduler = WarmupLR(
            self.optimizer,
            warmup_epochs=warmup_epochs,
            warmup_start_lr=1e-7,
            warmup_end_lr=warmup_end_lr
        )
        self.logger.info(f"âœ“ å­¦ä¹ ç‡é¢„çƒ­ {warmup_epochs} epochs")
        return warmup_scheduler
    
    def _create_ema(self) -> ModelEMA:
        """åˆ›å»ºEMAæ¨¡å‹ã€‚"""
        ema_decay = self.config.get('training', {}).get('ema_decay', 0.9999)
        return ModelEMA(self.model, decay=ema_decay)
    
    def _create_scaler(self) -> GradScaler:
        """åˆ›å»ºæ··åˆç²¾åº¦è®­ç»ƒå™¨ã€‚"""
        return GradScaler()
    
    def _create_early_stopping(self) -> Optional[EarlyStopping]:
        """åˆ›å»ºEarly Stoppingã€‚"""
        training_config = self.config.get('training', {})
        patience = training_config.get('early_stopping_patience', None)
        
        if patience is None or patience <= 0:
            self.logger.info("â±ï¸  Early Stopping: æœªå¯ç”¨")
            return None
        
        metric_name = training_config.get('early_stopping_metric', 'mAP_0.5_0.95')
        mode = 'max' if 'mAP' in metric_name or 'AP' in metric_name else 'min'
        
        self.logger.info(f"â±ï¸  Early Stopping: å¯ç”¨ (patience={patience}, metric={metric_name}, mode={mode})")
        
        return EarlyStopping(
            patience=patience,
            mode=mode,
            min_delta=0.0001,
            metric_name=metric_name,
            logger=self.logger
        )
    
    def _resume_from_checkpoint(self) -> None:
        """ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚"""
        try:
            checkpoint_path = Path(self.resume_from_checkpoint)
            if not checkpoint_path.exists():
                self.logger.warning(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
                return
            
            self.logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # æ¢å¤çŠ¶æ€
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.current_epoch = checkpoint.get('epoch', 0) + 1
            self.best_loss = checkpoint.get('best_loss', float('inf'))
            self.best_map = checkpoint.get('best_map', 0.0)
            self.global_step = checkpoint.get('global_step', 0)
            
            if 'ema_state_dict' in checkpoint:
                self.ema.load_state_dict(checkpoint['ema_state_dict'])
            if 'scaler_state_dict' in checkpoint:
                self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
            if 'visualizer_state' in checkpoint:
                self.visualizer.load_state_dict(checkpoint['visualizer_state'])
            if 'early_stopping_state' in checkpoint and self.early_stopping:
                self.early_stopping.load_state_dict(checkpoint['early_stopping_state'])
            
            self.logger.info(f"âœ“ æ¢å¤æˆåŠŸ (epoch={self.current_epoch}, step={self.global_step}, "
                           f"best_loss={self.best_loss:.4f})")
            
        except Exception as e:
            self.logger.error(f"æ¢å¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepochã€‚"""
        self.model.train()
        total_loss = 0.0
        detection_loss = 0.0
        moe_lb_loss = 0.0  # MoE load balance loss
        
        # ç»Ÿè®¡ç»†ç²’åº¦MoEçš„ä¸“å®¶ä½¿ç”¨ç‡ï¼ˆè·¨æ‰€æœ‰Decoderå±‚èšåˆï¼‰
        expert_usage_count = [0] * self.model.num_experts
        total_tokens = 0
        
        for batch_idx, (images, targets) in enumerate(self.train_loader):
            
            images = images.to(self.device)
            targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                       for k, v in t.items()} for t in targets]
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            with torch.cuda.amp.autocast():
                outputs = self.model(images, targets)
                loss = outputs.get('total_loss', torch.tensor(0.0, device=self.device))
            
            # åå‘ä¼ æ’­ï¼ˆæ·»åŠ æ¢¯åº¦è£å‰ªï¼‰
            self.scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.clip_max_norm)
            
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.ema.update(self.model)
            
            # ç»Ÿè®¡æŸå¤±
            total_loss += loss.item()
            if isinstance(outputs, dict):
                if 'detection_loss' in outputs:
                    detection_loss += outputs['detection_loss'].item()
                if 'moe_load_balance_loss' in outputs:
                    moe_lb_loss += outputs['moe_load_balance_loss'].item()
            
            # æ”¶é›†ç»†ç²’åº¦MoEçš„ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
            if self.model.decoder.use_moe:
                for layer in self.model.decoder.decoder.layers:
                    if hasattr(layer, 'adaptive_expert_layer') and layer.adaptive_expert_layer.router_logits_cache is not None:
                        router_logits = layer.adaptive_expert_layer.router_logits_cache  # [N, num_experts]
                        # è®¡ç®—æ¯ä¸ªtokené€‰æ‹©çš„top-kä¸“å®¶
                        _, top_indices = torch.topk(router_logits, self.model.decoder.moe_top_k, dim=-1)  # [N, K]
                        # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¬¡æ•°
                        for expert_id in range(self.model.num_experts):
                            expert_usage_count[expert_id] += (top_indices == expert_id).sum().item()
                        total_tokens += router_logits.shape[0] * self.model.decoder.moe_top_k
            
            if batch_idx % 50 == 0:
                det_loss_val = outputs.get('detection_loss', torch.tensor(0.0)).item() if isinstance(outputs, dict) else 0.0
                moe_loss_val = outputs.get('moe_load_balance_loss', torch.tensor(0.0)).item() if isinstance(outputs, dict) else 0.0
                self.logger.info(f'Epoch {self.current_epoch} | Batch {batch_idx} | '
                               f'Loss: {loss.item():.2f} (Det: {det_loss_val:.2f}, MoE: {moe_loss_val:.4f})')
            
            self.global_step += 1
        
        # è®¡ç®—å¹³å‡å€¼
        num_batches = len(self.train_loader)
        avg_loss = total_loss / num_batches
        avg_detection_loss = detection_loss / num_batches
        avg_moe_lb_loss = moe_lb_loss / num_batches
        
        # è®¡ç®—ä¸“å®¶ä½¿ç”¨ç‡
        expert_usage_rate = []
        if total_tokens > 0:
            for count in expert_usage_count:
                expert_usage_rate.append(count / total_tokens)
        else:
            expert_usage_rate = [1.0 / self.model.num_experts] * self.model.num_experts
        
        return {
            'total_loss': avg_loss,
            'detection_loss': avg_detection_loss,
            'moe_load_balance_loss': avg_moe_lb_loss,
            'expert_usage': expert_usage_count,
            'expert_usage_rate': expert_usage_rate
        }
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹å¹¶è®¡ç®—mAPã€‚"""
        self.ema.module.eval()
        total_loss = 0.0
        all_predictions = []
        all_targets = []
        total_raw_predictions = 0  # åŸå§‹queryæ€»æ•°
        
        with torch.no_grad():
            for batch_idx, (images, targets) in enumerate(self.val_loader):
                images = images.to(self.device)
                targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                           for k, v in t.items()} for t in targets]
                
                outputs = self.ema.module(images, targets)
                
                if isinstance(outputs, dict):
                    if 'total_loss' in outputs:
                        total_loss += outputs['total_loss'].item()
                    
                    # ç»Ÿè®¡åŸå§‹é¢„æµ‹æ•°
                    if 'class_scores' in outputs:
                        total_raw_predictions += outputs['class_scores'].shape[0] * outputs['class_scores'].shape[1]
                    
                    # æ”¶é›†é¢„æµ‹ç»“æœ
                    if 'class_scores' in outputs and 'bboxes' in outputs:
                        self._collect_predictions(outputs, targets, batch_idx, all_predictions, all_targets)
        
        # è®¡ç®—mAP
        mAP_metrics = self._compute_map_metrics(all_predictions, all_targets)
        
        avg_loss = total_loss / len(self.val_loader)
        
        return {
            'total_loss': avg_loss,
            'mAP_0.5': mAP_metrics.get('mAP_0.5', 0.0),
            'mAP_0.75': mAP_metrics.get('mAP_0.75', 0.0),
            'mAP_0.5_0.95': mAP_metrics.get('mAP_0.5_0.95', 0.0),
            'num_predictions': len(all_predictions),
            'num_raw_predictions': total_raw_predictions,
            'num_targets': len(all_targets)
        }
    
    def _collect_predictions(self, outputs: Dict, targets: List[Dict], batch_idx: int,
                            all_predictions: List, all_targets: List) -> None:
        """æ”¶é›†é¢„æµ‹ç»“æœç”¨äºmAPè®¡ç®—ã€‚ä¿ç•™æ‰€æœ‰æœ‰æ•ˆé¢„æµ‹æ¡†ï¼Œä¸åštop-ké™åˆ¶ã€‚"""
        pred_logits = outputs['class_scores']  # [B, Q, C]
        pred_boxes = outputs['bboxes']  # [B, Q, 4]
        
        batch_size = pred_logits.shape[0]
        
        for i in range(batch_size):
            # VFLæŸå¤±ä½¿ç”¨sigmoidï¼Œæ‰€ä»¥æ¨ç†æ—¶ä¹Ÿåº”è¯¥ä½¿ç”¨sigmoid
            pred_scores_sigmoid = torch.sigmoid(pred_logits[i])  # [Q, C]
            max_scores, pred_classes = torch.max(pred_scores_sigmoid, dim=-1)  # [Q]
            
            # è¿‡æ»¤æ— æ•ˆæ¡†ï¼ˆpaddingæ¡†ï¼‰ï¼Œä¿ç•™æ‰€æœ‰æœ‰æ•ˆé¢„æµ‹æ¡†
            valid_boxes_mask = ~torch.all(pred_boxes[i] == 1.0, dim=1)
            valid_indices = torch.where(valid_boxes_mask)[0]
            if len(valid_indices) > 0:
                filtered_boxes = pred_boxes[i][valid_indices]
                filtered_classes = pred_classes[valid_indices]
                filtered_scores = max_scores[valid_indices]
                
                # è½¬æ¢ä¸ºCOCOæ ¼å¼
                if filtered_boxes.shape[0] > 0:
                    boxes_coco = torch.zeros_like(filtered_boxes)
                    if filtered_boxes.max() <= 1.0:
                        # å½’ä¸€åŒ–åæ ‡ -> åƒç´ åæ ‡
                        boxes_coco[:, 0] = (filtered_boxes[:, 0] - filtered_boxes[:, 2] / 2) * self.model.image_size
                        boxes_coco[:, 1] = (filtered_boxes[:, 1] - filtered_boxes[:, 3] / 2) * self.model.image_size
                        boxes_coco[:, 2] = filtered_boxes[:, 2] * self.model.image_size
                        boxes_coco[:, 3] = filtered_boxes[:, 3] * self.model.image_size
                    else:
                        boxes_coco = filtered_boxes.clone()
                    
                    # Clampåæ ‡
                    boxes_coco[:, 0] = torch.clamp(boxes_coco[:, 0], 0, self.model.image_size)
                    boxes_coco[:, 1] = torch.clamp(boxes_coco[:, 1], 0, self.model.image_size)
                    boxes_coco[:, 2] = torch.clamp(boxes_coco[:, 2], 1, self.model.image_size)
                    boxes_coco[:, 3] = torch.clamp(boxes_coco[:, 3], 1, self.model.image_size)
                    
                    for j in range(filtered_boxes.shape[0]):
                        all_predictions.append({
                            'image_id': batch_idx * self.config['training']['batch_size'] + i,
                            'category_id': int(filtered_classes[j].item()) + 1,
                            'bbox': boxes_coco[j].cpu().numpy().tolist(),
                            'score': float(filtered_scores[j].item())
                        })
            
            # å¤„ç†çœŸå®æ ‡ç­¾
            if i < len(targets) and 'labels' in targets[i] and 'boxes' in targets[i]:
                true_labels = targets[i]['labels']
                true_boxes = targets[i]['boxes']
                
                if len(true_labels) > 0:
                    img_size = self.model.image_size
                    max_val = float(true_boxes.max().item()) if true_boxes.numel() > 0 else 0.0
                    scale = img_size if max_val <= 1.0 + 1e-6 else 1.0
                    
                    true_boxes_coco = torch.zeros_like(true_boxes)
                    true_boxes_coco[:, 0] = (true_boxes[:, 0] - true_boxes[:, 2] / 2) * scale
                    true_boxes_coco[:, 1] = (true_boxes[:, 1] - true_boxes[:, 3] / 2) * scale
                    true_boxes_coco[:, 2] = true_boxes[:, 2] * scale
                    true_boxes_coco[:, 3] = true_boxes[:, 3] * scale
                    
                    true_boxes_coco[:, 0] = torch.clamp(true_boxes_coco[:, 0], 0, img_size)
                    true_boxes_coco[:, 1] = torch.clamp(true_boxes_coco[:, 1], 0, img_size)
                    true_boxes_coco[:, 2] = torch.clamp(true_boxes_coco[:, 2], 1, img_size)
                    true_boxes_coco[:, 3] = torch.clamp(true_boxes_coco[:, 3], 1, img_size)
                    
                    for j in range(len(true_labels)):
                        all_targets.append({
                            'image_id': batch_idx * self.config['training']['batch_size'] + i,
                            'category_id': int(true_labels[j].item()) + 1,
                            'bbox': true_boxes_coco[j].cpu().numpy().tolist(),
                            'area': float((true_boxes_coco[j, 2] * true_boxes_coco[j, 3]).item()),
                            'iscrowd': 0
                        })
    
    def _compute_map_metrics(self, predictions: List[Dict], targets: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—mAPæŒ‡æ ‡ã€‚"""
        try:
            if len(predictions) == 0:
                return {
                    'mAP_0.5': 0.0,
                    'mAP_0.75': 0.0,
                    'mAP_0.5_0.95': 0.0
                }
            
            # è·å–ç±»åˆ«ä¿¡æ¯
            if hasattr(self, 'val_dataset') and hasattr(self.val_dataset, 'get_categories'):
                categories = self.val_dataset.get_categories()
            else:
                categories = [
                    {'id': 1, 'name': 'Car'},
                    {'id': 2, 'name': 'Truck'},
                    {'id': 3, 'name': 'Bus'},
                    {'id': 4, 'name': 'Van'},
                    {'id': 5, 'name': 'Pedestrian'},
                    {'id': 6, 'name': 'Cyclist'},
                    {'id': 7, 'name': 'Motorcyclist'}
                ]
            
            # åˆ›å»ºCOCOæ ¼å¼æ•°æ®
            coco_gt = {
                'images': [],
                'annotations': [],
                'categories': categories,
                'info': {
                    'description': 'DAIR-V2X Dataset',
                    'version': '1.0',
                    'year': 2024
                }
            }
            
            # æ·»åŠ å›¾åƒä¿¡æ¯
            image_ids = set(target['image_id'] for target in targets)
            for img_id in image_ids:
                coco_gt['images'].append({
                    'id': img_id, 
                    'width': self.model.image_size, 
                    'height': self.model.image_size
                })
            
            # æ·»åŠ æ ‡æ³¨
            for i, target in enumerate(targets):
                target['id'] = i + 1
                coco_gt['annotations'].append(target)
            
            # ä½¿ç”¨pycocotoolsè¯„ä¼°
            coco_gt_obj = COCO()
            coco_gt_obj.dataset = coco_gt
            coco_gt_obj.createIndex()
            
            coco_dt = coco_gt_obj.loadRes(predictions)
            
            coco_eval = COCOeval(coco_gt_obj, coco_dt, 'bbox')
            coco_eval.evaluate()
            coco_eval.accumulate()
            coco_eval.summarize()
            
            return {
                'mAP_0.5': coco_eval.stats[1],
                'mAP_0.75': coco_eval.stats[2],
                'mAP_0.5_0.95': coco_eval.stats[0]
            }
            
        except Exception as e:
            self.logger.warning(f"mAPè®¡ç®—å¤±è´¥: {e}")
            return {
                'mAP_0.5': 0.0,
                'mAP_0.75': 0.0,
                'mAP_0.5_0.95': 0.0
            }
    
    def _safe_save(self, checkpoint: Dict, path: Path, desc: str = "æ£€æŸ¥ç‚¹") -> bool:
        """å®‰å…¨ä¿å­˜checkpoint - å¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†ã€‚"""
        import time
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # å…ˆä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_path = path.with_suffix('.pth.tmp')
                torch.save(checkpoint, temp_path)
                
                # ç¡®ä¿å†™å…¥å®Œæˆ
                import os
                os.sync()
                
                # é‡å‘½åä¸ºç›®æ ‡æ–‡ä»¶ï¼ˆåŸå­æ“ä½œï¼‰
                temp_path.replace(path)
                self.logger.info(f"ğŸ’¾ ä¿å­˜{desc}: {path}")
                return True
                
            except Exception as e:
                self.logger.warning(f"ä¿å­˜{desc}å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                else:
                    self.logger.error(f"âš ï¸  ä¿å­˜{desc}æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡å¹¶ç»§ç»­è®­ç»ƒ")
                    return False
        
        return False
    
    def save_checkpoint(self, epoch: int, is_best: bool = False) -> None:
        """ä¿å­˜æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'warmup_scheduler_state_dict': self.warmup_scheduler.state_dict(),
            'ema_state_dict': self.ema.state_dict(),
            'scaler_state_dict': self.scaler.state_dict(),
            'config': self.config,
            'best_loss': self.best_loss,
            'best_map': self.best_map,
            'global_step': self.global_step,
            'visualizer_state': self.visualizer.state_dict()
        }
        
        if self.early_stopping:
            checkpoint['early_stopping_state'] = self.early_stopping.state_dict()
        
        if is_best:
            best_path = self.log_dir / 'best_model.pth'
            self._safe_save(checkpoint, best_path, "æœ€ä½³æ¨¡å‹")
    
    def save_latest_checkpoint(self, epoch: int) -> None:
        """ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ç”¨äºæ–­ç‚¹ç»­è®­ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'warmup_scheduler_state_dict': self.warmup_scheduler.state_dict(),
            'ema_state_dict': self.ema.state_dict(),
            'scaler_state_dict': self.scaler.state_dict(),
            'config': self.config,
            'best_loss': self.best_loss,
            'best_map': self.best_map,
            'global_step': self.global_step,
            'visualizer_state': self.visualizer.state_dict()
        }
        
        if self.early_stopping:
            checkpoint['early_stopping_state'] = self.early_stopping.state_dict()
        
        latest_path = self.log_dir / 'latest_checkpoint.pth'
        self._safe_save(checkpoint, latest_path, "æœ€æ–°æ£€æŸ¥ç‚¹")
    
    def train(self) -> None:
        """ä¸»è®­ç»ƒå¾ªç¯ã€‚"""
        epochs = self.config['training']['epochs']
        self.logger.info(f"å¼€å§‹è®­ç»ƒ {epochs} epochs")
        self.logger.info(f"âœ“ æ¢¯åº¦è£å‰ª: max_norm={self.clip_max_norm}")
        
        for epoch in range(self.current_epoch, epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            
            # éªŒè¯
            val_metrics = self.validate()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.current_epoch < self.warmup_scheduler.warmup_epochs:
                self.warmup_scheduler.step()
            else:
                self.scheduler.step()
            
            # è¾“å‡ºæ—¥å¿—
            self.logger.info(f"Epoch {epoch}:")
            self.logger.info(f"  è®­ç»ƒæŸå¤±: {train_metrics.get('total_loss', 0.0):.2f} | éªŒè¯æŸå¤±: {val_metrics.get('total_loss', 0.0):.2f}")
            self.logger.info(f"  mAP@0.5: {val_metrics.get('mAP_0.5', 0.0):.4f} | mAP@0.75: {val_metrics.get('mAP_0.75', 0.0):.4f} | "
                           f"mAP@[0.5:0.95]: {val_metrics.get('mAP_0.5_0.95', 0.0):.4f}")
            self.logger.info(f"  é¢„æµ‹/ç›®æ ‡: {val_metrics['num_predictions']}/{val_metrics['num_targets']}")
            
            # æ˜¾ç¤ºè¯¦ç»†æŸå¤±ï¼ˆå‰20ä¸ªepochæ¯æ¬¡æ˜¾ç¤ºï¼Œä¹‹åæ¯5ä¸ªepochæ˜¾ç¤ºï¼‰
            should_show_details = (epoch < 20) or (epoch % 5 == 0)
            if should_show_details:
                self.logger.info(f"  æ£€æµ‹æŸå¤±: {train_metrics['detection_loss']:.2f}")
                self.logger.info(f"  MoEè´Ÿè½½å‡è¡¡æŸå¤±: {train_metrics['moe_load_balance_loss']:.4f}")
                # æ˜¾ç¤ºä¸“å®¶ä½¿ç”¨ç‡
                usage_str = [f"{rate*100:.2f}%" for rate in train_metrics['expert_usage_rate']]
                self.logger.info(f"  ä¸“å®¶ä½¿ç”¨ç‡: [{', '.join(usage_str)}]")
            
            # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°å¯è§†åŒ–å™¨
            current_lr = self.optimizer.param_groups[0]['lr']
            self.visualizer.record(
                epoch=epoch,
                train_loss=train_metrics.get('total_loss', 0.0),
                val_loss=val_metrics.get('total_loss', 0.0),
                mAP_0_5=val_metrics.get('mAP_0.5', 0.0),
                mAP_0_75=val_metrics.get('mAP_0.75', 0.0),
                mAP_0_5_0_95=val_metrics.get('mAP_0.5_0.95', 0.0),
                learning_rate=current_lr,
                expert_usage=train_metrics.get('expert_usage_rate', []),  # ç»†ç²’åº¦MoEä¸“å®¶ä½¿ç”¨ç‡
                router_loss=train_metrics.get('moe_load_balance_loss', 0.0)  # è®°å½•MoEè´Ÿè½½å‡è¡¡æŸå¤±
            )
            
            # ä¿å­˜æ£€æŸ¥ç‚¹ - åŒæ—¶è€ƒè™‘losså’ŒmAP
            is_best_loss = val_metrics.get('total_loss', float('inf')) < self.best_loss
            is_best_map = val_metrics.get('mAP_0.5_0.95', 0.0) > self.best_map
            
            if is_best_loss:
                self.best_loss = val_metrics.get('total_loss', float('inf'))
                self.logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.2f}")
            
            if is_best_map:
                self.best_map = val_metrics.get('mAP_0.5_0.95', 0.0)
                self.logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³mAP: {self.best_map:.4f}")
                self.save_checkpoint(epoch, is_best=True)
            
            # Early Stoppingæ£€æŸ¥
            if self.early_stopping:
                # è·å–è¦ç›‘æ§çš„æŒ‡æ ‡å€¼
                metric_name = self.early_stopping.metric_name
                if 'mAP_0.5_0.95' in metric_name or 'mAP_0.5:0.95' in metric_name:
                    metric_value = val_metrics.get('mAP_0.5_0.95', 0.0)
                elif 'mAP_0.5' in metric_name:
                    metric_value = val_metrics.get('mAP_0.5', 0.0)
                elif 'mAP_0.75' in metric_name:
                    metric_value = val_metrics.get('mAP_0.75', 0.0)
                elif 'loss' in metric_name.lower():
                    metric_value = val_metrics.get('total_loss', float('inf'))
                else:
                    metric_value = val_metrics.get('mAP_0.5_0.95', 0.0)  # é»˜è®¤
                
                if self.early_stopping(metric_value, epoch):
                    self.logger.info(f"Early Stoppingåœ¨epoch {epoch}è§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                    break
            
            # æ¯ä¸ªepochéƒ½ä¿å­˜latestç”¨äºæ–­ç‚¹ç»­è®­ï¼ˆä¸ä¼šå †ç§¯æ–‡ä»¶ï¼‰
            self.save_latest_checkpoint(epoch)
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆæ¯ä¸ªepochéƒ½æ›´æ–°ï¼‰
            try:
                self.visualizer.plot()
            except Exception as e:
                self.logger.warning(f"ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¤±è´¥: {e}")
        
        self.logger.info("âœ“ è®­ç»ƒå®Œæˆï¼")
        
        # æœ€åç»˜åˆ¶ä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒæ›²çº¿å¹¶å¯¼å‡ºCSV
        try:
            self.visualizer.plot()
            self.visualizer.export_to_csv()
            self.logger.info(f"âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {self.log_dir}/training_curves.png")
            self.logger.info(f"âœ“ è®­ç»ƒå†å²å·²å¯¼å‡ºåˆ°: {self.log_dir}/training_history.csv")
        except Exception as e:
            self.logger.error(f"ç»˜åˆ¶æœ€ç»ˆè®­ç»ƒæ›²çº¿å¤±è´¥: {e}")


def main() -> None:
    """ä¸»å‡½æ•°ã€‚"""
    parser = argparse.ArgumentParser(description='è‡ªé€‚åº”ä¸“å®¶RT-DETRè®­ç»ƒ')
    parser.add_argument('--config', type=str, default='A', 
                       help='ä¸“å®¶é…ç½® (A: 6ä¸“å®¶, B: 3ä¸“å®¶) æˆ–YAMLé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--backbone', type=str, default='presnet34', 
                       choices=['presnet18', 'presnet34', 'presnet50', 'presnet101',
                               'hgnetv2_l', 'hgnetv2_x', 'hgnetv2_h',
                               'cspresnet_s', 'cspresnet_m', 'cspresnet_l', 'cspresnet_x',
                               'cspdarknet', 'mresnet'],
                       help='Backboneç±»å‹')
    parser.add_argument('--data_root', type=str, default='datasets/DAIR-V2X', 
                       help='DAIR-V2Xæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å° (RTX 5090ä¼˜åŒ–)')
    parser.add_argument('--pretrained_lr', type=float, default=1e-5, help='é¢„è®­ç»ƒç»„ä»¶å­¦ä¹ ç‡')
    parser.add_argument('--new_lr', type=float, default=1e-4, help='æ–°ç»„ä»¶å­¦ä¹ ç‡')
    parser.add_argument('--top_k', type=int, default=3, help='è·¯ç”±å™¨Top-K')
    parser.add_argument('--pretrained_weights', type=str, default=None,
                       help='é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆRT-DETR COCOé¢„è®­ç»ƒæ¨¡å‹ï¼‰')
    parser.add_argument('--resume_from_checkpoint', type=str, default=None,
                       help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼‰')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­ï¼Œç”¨äºç¡®ä¿å®éªŒå¯é‡å¤æ€§ï¼ˆé»˜è®¤ï¼š42ï¼‰')
    parser.add_argument('--deterministic', action='store_true',
                       help='ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ï¼ˆä¼šé™ä½é€Ÿåº¦ä½†ä¿è¯å®Œå…¨å¯é‡å¤ï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰æ“ä½œä¹‹å‰ï¼‰
    print("\n" + "="*60)
    print("ğŸ”§ åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ")
    print("="*60)
    set_seed(args.seed, deterministic=args.deterministic)
    
    # åŠ è½½é…ç½®
    config_file_path = None
    if args.config and args.config.endswith('.yaml'):
        # ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
        config_file_path = args.config
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"ğŸ“„ ä»é…ç½®æ–‡ä»¶åŠ è½½: {args.config}")
        
        # ç¡®ä¿å­¦ä¹ ç‡ç›¸å…³å€¼æ˜¯æµ®ç‚¹æ•°ï¼ˆYAMLä¸­çš„ç§‘å­¦è®¡æ•°æ³•å¯èƒ½è¢«è§£æä¸ºå­—ç¬¦ä¸²ï¼‰
        if 'training' in config:
            if 'pretrained_lr' in config['training']:
                config['training']['pretrained_lr'] = float(config['training']['pretrained_lr'])
            if 'new_lr' in config['training']:
                config['training']['new_lr'] = float(config['training']['new_lr'])
            if 'eta_min' in config['training']:
                config['training']['eta_min'] = float(config['training']['eta_min'])
        
        # åªå…è®¸æ˜¾å¼ä¼ é€’çš„å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶ï¼ˆä¸ç­‰äºé»˜è®¤å€¼çš„æ‰è¦†ç›–ï¼‰
        if args.backbone != 'presnet34':
            config['model']['backbone'] = args.backbone
        if args.epochs != 100:
            config['training']['epochs'] = args.epochs
        if args.batch_size != 32:
            config['training']['batch_size'] = args.batch_size
        if args.pretrained_lr != 1e-5:
            config['training']['pretrained_lr'] = args.pretrained_lr
        if args.new_lr != 1e-4:
            config['training']['new_lr'] = args.new_lr
        if args.top_k != 3:
            config['model']['top_k'] = args.top_k
        if args.data_root != 'datasets/DAIR-V2X':
            config['data']['data_root'] = args.data_root
        if args.pretrained_weights:
            config['model']['pretrained_weights'] = args.pretrained_weights
        
        # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„ resume_from_checkpoint
        if args.resume_from_checkpoint:
            if 'checkpoint' not in config:
                config['checkpoint'] = {}
            config['checkpoint']['resume_from_checkpoint'] = args.resume_from_checkpoint
    else:
        # åˆ›å»ºé»˜è®¤é…ç½®
        config = {
            'model': {
                'config': args.config,
                'hidden_dim': 256,
                'num_queries': 300,
                'top_k': args.top_k,
                'backbone': args.backbone
            },
            'data': {
                'data_root': args.data_root
            },
            'training': {
                'epochs': args.epochs,
                'batch_size': args.batch_size,
                'pretrained_lr': args.pretrained_lr,
                'new_lr': args.new_lr,
                'use_mosaic': True, 
                'warmup_epochs': 3,
                'ema_decay': 0.9999
            }
        }
        
        if args.pretrained_weights:
            config['model']['pretrained_weights'] = args.pretrained_weights
        
        if args.resume_from_checkpoint:
            config['checkpoint'] = {'resume_from_checkpoint': args.resume_from_checkpoint}
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = AdaptiveExpertTrainer(config, config_file_path=config_file_path)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == '__main__':
    main()
