import sys
import os
import argparse
import yaml
import torch
import numpy as np
import re
from pathlib import Path
import logging
from typing import Optional, Dict, Union, List
from pycocotools.cocoeval import COCOeval
from pycocotools.coco import COCO

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.resolve()
# ç¡®ä¿å½“å‰å·¥ä½œç›®å½•åœ¨è·¯å¾„ä¸­ï¼ˆé‡è¦ï¼šå½“ä»ä¸åŒç›®å½•è¿è¡Œæ—¶ï¼‰
if str(os.getcwd()) not in sys.path:
    sys.path.insert(0, os.getcwd())
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root.parent))  # æ·»åŠ experimentsç›®å½•

# å¯¼å…¥éšæœºç§å­å·¥å…·
from seed_utils import set_seed, seed_worker

# å¯¼å…¥ç°æœ‰å·¥å…·
from src.misc.training_visualizer import TrainingVisualizer
from src.misc.early_stopping import EarlyStopping
from src.data import DataLoader
from src.optim.ema import ModelEMA
from src.optim.amp import GradScaler
from src.optim.warmup import WarmupLR
from src.data.dataset.dairv2x_detection import DAIRV2XDetection


def create_backbone(backbone_type: str, **kwargs):
    """åˆ›å»ºbackboneçš„å·¥å‚å‡½æ•°ã€‚
    
    Args:
        backbone_type: backboneç±»å‹ï¼ˆpresnet18/34/50/101, hgnetv2_lç­‰ï¼‰
        **kwargs: backboneç‰¹å®šå‚æ•°ï¼ˆä¼šè¦†ç›–é»˜è®¤é…ç½®ï¼‰
    
    Returns:
        nn.Module: backboneæ¨¡å‹å®ä¾‹
        
    Raises:
        ValueError: ä¸æ”¯æŒçš„backboneç±»å‹
    """
    from src.nn.backbone.presnet import PResNet
    from src.nn.backbone.hgnetv2 import HGNetv2
    from src.nn.backbone.csp_resnet import CSPResNet
    from src.nn.backbone.csp_darknet import CSPDarkNet
    
    # PResNeté…ç½®ï¼ˆé€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è§£ædepthï¼‰
    if backbone_type.startswith('presnet'):
        depth_match = re.search(r'(\d+)', backbone_type)
        if depth_match:
            depth = int(depth_match.group(1))
        else:
            raise ValueError(f"æ— æ³•ä»backboneç±»å‹ {backbone_type} è§£ædepth")
        
        default_params = {
            'depth': depth,
            'variant': 'd',
            'return_idx': [1, 2, 3],
            'freeze_at': 0,  # å†»ç»“ç¬¬ä¸€ä¸ªstage
            'freeze_norm': True,  # å†»ç»“BNå±‚
            'pretrained': False
        }
        default_params.update(kwargs)
        return PResNet(**default_params)
    
    # HGNetv2é…ç½®
    elif backbone_type.startswith('hgnetv2'):
        name_map = {'hgnetv2_l': 'L', 'hgnetv2_x': 'X', 'hgnetv2_h': 'H'}
        if backbone_type not in name_map:
            raise ValueError(f"ä¸æ”¯æŒçš„HGNetv2ç±»å‹: {backbone_type}")
        
        default_params = {
            'name': name_map[backbone_type],
            'return_idx': [1, 2, 3],
            'freeze_at': 0,
            'freeze_norm': True,
            'pretrained': False
        }
        default_params.update(kwargs)
        return HGNetv2(**default_params)
    
    # CSPResNeté…ç½®
    elif backbone_type.startswith('cspresnet'):
        name_map = {'cspresnet_s': 's', 'cspresnet_m': 'm', 'cspresnet_l': 'l', 'cspresnet_x': 'x'}
        if backbone_type not in name_map:
            raise ValueError(f"ä¸æ”¯æŒçš„CSPResNetç±»å‹: {backbone_type}")
        
        default_params = {
            'name': name_map[backbone_type],
            'return_idx': [1, 2, 3],
            'pretrained': False
        }
        default_params.update(kwargs)
        return CSPResNet(**default_params)
    
    # CSPDarkNeté…ç½®
    elif backbone_type == 'cspdarknet':
        default_params = {'return_idx': [2, 3, -1]}
        default_params.update(kwargs)
        return CSPDarkNet(**default_params)
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„backboneç±»å‹: {backbone_type}")


class RTDETRTrainer:
    
    def __init__(self, config: Union[str, dict], pretrained_weights: Optional[str] = None, 
                 data_root: Optional[str] = None, epochs: Optional[int] = None,
                 batch_size: Optional[int] = None, warmup_epochs: Optional[int] = None):
        """åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: é…ç½®æ–‡ä»¶è·¯å¾„æˆ–é…ç½®å­—å…¸
            pretrained_weights: é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰
            data_root: æ•°æ®é›†æ ¹ç›®å½•ï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰
            epochs: è®­ç»ƒè½®æ•°ï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰
            warmup_epochs: å­¦ä¹ ç‡é¢„çƒ­è½®æ•°ï¼ˆå¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰
        """
        self.pretrained_weights = pretrained_weights
        
        # åŠ è½½é…ç½®
        using_config_file = isinstance(config, str)
        if using_config_file:
            # ä»æ–‡ä»¶åŠ è½½é…ç½®
            self.config_path = config
            with open(config, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
        else:
            # ç›´æ¥ä½¿ç”¨é…ç½®å­—å…¸
            self.config_path = None
            self.config = config
        
        # å¦‚æœä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ŒéªŒè¯å¿…éœ€çš„é…ç½®é¡¹æ˜¯å¦å­˜åœ¨
        if using_config_file:
            self._validate_config_file()
        
        # ç¡®ä¿å­¦ä¹ ç‡ç›¸å…³å€¼æ˜¯æµ®ç‚¹æ•°ï¼ˆYAMLä¸­çš„ç§‘å­¦è®¡æ•°æ³•å¯èƒ½è¢«è§£æä¸ºå­—ç¬¦ä¸²ï¼‰
        # ç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å­—æ®µåï¼špretrained_lr, new_lr
        if 'training' in self.config:
            # ç±»å‹è½¬æ¢ç¡®ä¿æ˜¯æµ®ç‚¹æ•°
            if 'pretrained_lr' in self.config['training']:
                self.config['training']['pretrained_lr'] = float(self.config['training']['pretrained_lr'])
            if 'new_lr' in self.config['training']:
                self.config['training']['new_lr'] = float(self.config['training']['new_lr'])
            if 'eta_min' in self.config['training']:
                self.config['training']['eta_min'] = float(self.config['training']['eta_min'])
            if 'weight_decay' in self.config['training']:
                self.config['training']['weight_decay'] = float(self.config['training']['weight_decay'])
        
        # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶ï¼ˆåªæœ‰åœ¨æ˜¾å¼ä¼ é€’æ—¶æ‰è¦†ç›–ï¼‰
        if data_root is not None:
            self.config['data']['data_root'] = data_root
        
        if epochs is not None:
            self.config['training']['epochs'] = epochs
        
        if batch_size is not None:
            self.config['training']['batch_size'] = batch_size
        
        if warmup_epochs is not None:
            self.config['training']['warmup_epochs'] = warmup_epochs
        
        # è®¾ç½®åŸºæœ¬å±æ€§ï¼ˆdeviceåœ¨miscé…ç½®ä¸­ï¼‰
        if using_config_file:
            # å¦‚æœä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œdeviceå¿…é¡»å­˜åœ¨ï¼Œå¦åˆ™æŠ¥é”™
            if 'misc' not in self.config or 'device' not in self.config['misc']:
                raise ValueError(f"é…ç½®æ–‡ä»¶ {self.config_path} ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹: misc.device")
            device_str = self.config['misc']['device']
        else:
            device_str = self.config.get('misc', {}).get('device', 'cuda')
        self.device = torch.device(device_str)
        self.setup_logging()
        self._create_directories()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.criterion = None
        self.optimizer = None
        self.scheduler = None
        self.warmup_scheduler = None
        self.ema = None
        self.scaler = None
        self.visualizer = None
    
    def _validate_config_file(self):
        """éªŒè¯é…ç½®æ–‡ä»¶æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„é…ç½®é¡¹"""
        required_keys = {
            'model': ['backbone', 'num_decoder_layers', 'hidden_dim', 'num_queries'],
            'training': ['epochs', 'batch_size', 'pretrained_lr', 'new_lr'],
            'data': ['data_root'],
            'misc': ['device', 'num_workers']
        }
        
        missing_keys = []
        for section, keys in required_keys.items():
            if section not in self.config:
                missing_keys.append(f"ç¼ºå°‘é…ç½®èŠ‚: {section}")
                continue
            for key in keys:
                if key not in self.config[section]:
                    missing_keys.append(f"{section}.{key}")
        
        if missing_keys:
            error_msg = f"é…ç½®æ–‡ä»¶ {self.config_path} ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹:\n"
            error_msg += "\n".join(f"  - {key}" for key in missing_keys)
            raise ValueError(error_msg)
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # æ£€æŸ¥æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        resume_checkpoint = getattr(self, '_resume_checkpoint_path', None)
        
        if resume_checkpoint and Path(resume_checkpoint).exists():
            # æ¢å¤è®­ç»ƒï¼šä½¿ç”¨æ£€æŸ¥ç‚¹æ‰€åœ¨ç›®å½•
            self.log_dir = Path(resume_checkpoint).parent
            self.logger = logging.getLogger(__name__)
            self.logger.info(f"ğŸ“¦ æ¢å¤è®­ç»ƒï¼Œä½¿ç”¨ç°æœ‰æ—¥å¿—ç›®å½•: {self.log_dir}")
            # ä»ç›®å½•åä¸­æå–å®éªŒåç§°ï¼ˆå»æ‰æ—¶é—´æˆ³éƒ¨åˆ†ï¼‰
            dir_name = self.log_dir.name
            # å‡è®¾æ ¼å¼ä¸º rtdetr_r50_20240101_120000ï¼Œæå– rtdetr_r50
            parts = dir_name.rsplit('_', 2)  # åˆ†å‰²æœ€åä¸¤éƒ¨åˆ†ï¼ˆæ—¥æœŸå’Œæ—¶é—´ï¼‰
            if len(parts) >= 2:
                self.experiment_name = '_'.join(parts[:-2]) if len(parts) > 2 else parts[0]
            else:
                self.experiment_name = dir_name
        else:
            # æ–°è®­ç»ƒï¼šåˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ç›®å½•
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # ä»é…ç½®ä¸­è·å–backboneç±»å‹ï¼ŒåŠ å…¥åˆ°ç›®å½•åä¸­
            backbone = self.config['model']['backbone']
            # ç§»é™¤presnetå‰ç¼€ï¼Œåªä¿ç•™æ•°å­—éƒ¨åˆ†ï¼ˆå¦‚presnet18 -> r18, presnet34 -> r34ï¼‰
            backbone_short = backbone.replace('presnet', 'r').replace('pres', 'r') if 'presnet' in backbone or 'pres' in backbone else backbone
            # ç”Ÿæˆå®éªŒåç§°ï¼ˆä¸å¸¦æ—¶é—´æˆ³ï¼‰
            self.experiment_name = f"rtdetr_{backbone_short}"
            self.log_dir = Path(f"logs/{self.experiment_name}_{timestamp}")
            self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®æ—¥å¿—å¤„ç†å™¨
        handlers = [
            logging.FileHandler(self.log_dir / 'training.log', mode='a'),
            logging.StreamHandler()
        ]
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=handlers,
            force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
        )
        
        self.logger = logging.getLogger(__name__)
        
        # ä¿å­˜é…ç½®æ–‡ä»¶ï¼ˆä»…æ–°è®­ç»ƒæ—¶ï¼‰
        if not resume_checkpoint:
            config_save_path = self.log_dir / 'config.yaml'
            with open(config_save_path, 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
            self.logger.info(f"âœ“ é…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")
    
    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ï¼ˆå·²åœ¨setup_loggingä¸­åˆ›å»ºï¼‰"""
        # log_dir å·²åœ¨ setup_logging ä¸­åˆ›å»º
        # æ‰€æœ‰è¾“å‡ºéƒ½ä¿å­˜åœ¨ log_dir ä¸­ï¼Œæ— éœ€é¢å¤–åˆ›å»ºç›®å½•
        pass
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        # ä»é…ç½®æ–‡ä»¶è¯»å–backboneç±»å‹
        backbone_type = self.config['model']['backbone']
        
        # åŠ¨æ€åˆ›å»ºbackbone
        backbone = create_backbone(backbone_type)
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–encoderé…ç½®
        encoder_config = self.config['model']['encoder']
        in_channels = encoder_config['in_channels']
        expansion = encoder_config['expansion']
        
        self.logger.info(f"âœ“ Backbone: {backbone_type}")
        self.logger.info(f"âœ“ HybridEncoder: in_channels={in_channels}, expansion={expansion}")
        
        # åˆ›å»ºencoder
        from src.zoo.rtdetr.hybrid_encoder import HybridEncoder
        encoder = HybridEncoder(
            in_channels=in_channels,
            feat_strides=[8, 16, 32],
            hidden_dim=256,
            use_encoder_idx=[2],
            num_encoder_layers=1,
            expansion=expansion,
            nhead=8,
            dim_feedforward=1024,
            dropout=0.0,
            enc_act='gelu',
            act='silu',
            eval_spatial_size=[640, 640]
        )
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–æ¨¡å‹å‚æ•°
        num_decoder_layers = self.config['model']['num_decoder_layers']
        hidden_dim = self.config['model']['hidden_dim']
        num_queries = self.config['model']['num_queries']
        
        # åˆ›å»ºdecoderï¼ˆæ·»åŠ denoisingè®­ç»ƒï¼‰
        from src.zoo.rtdetr.rtdetrv2_decoder import RTDETRTransformerv2
        decoder = RTDETRTransformerv2(
            num_classes=6,
            hidden_dim=hidden_dim,
            num_queries=num_queries,
            num_layers=num_decoder_layers, 
            nhead=8,
            dim_feedforward=1024,
            dropout=0.1,
            activation='relu',
            feat_channels=[256, 256, 256],
            feat_strides=[8, 16, 32],
            num_levels=3,
            # æ·»åŠ denoisingè®­ç»ƒå‚æ•°
            num_denoising=100,
            label_noise_ratio=0.5,
            box_noise_scale=1.0,
            num_points=[4, 4, 4]
        )
        
        self.logger.info(f"âœ“ Decoderé…ç½®: {num_decoder_layers}å±‚, hidden_dim={hidden_dim}, queries={num_queries}")
        
        # åˆ›å»ºRT-DETRæ¨¡å‹
        from src.zoo.rtdetr.rtdetr import RTDETR
        model = RTDETR(backbone=backbone, encoder=encoder, decoder=decoder)
        
        self.logger.info("âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆï¼ˆå·²å¯ç”¨backboneé¢„è®­ç»ƒï¼‰")
        
        return model
    
    def load_pretrained_weights(self, model, pretrained_path: str):
        """åŠ è½½é¢„è®­ç»ƒæƒé‡
        
        æ”¯æŒå¤šç§checkpointæ ¼å¼ï¼š
        - EMAæ ¼å¼: {'ema': {'module': {...}}}
        - æ ‡å‡†æ ¼å¼: {'model': {...}} æˆ– {'model_state_dict': {...}}
        - ç›´æ¥æƒé‡: state_dict
        
        Args:
            model: RT-DETRæ¨¡å‹
            pretrained_path: é¢„è®­ç»ƒæƒé‡è·¯å¾„
        """
        try:
            pretrained_file = Path(pretrained_path)
            if not pretrained_file.exists():
                self.logger.warning(f"âš  é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {pretrained_path}")
                self.logger.info("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
                return
            
            self.logger.info(f"æ­£åœ¨ä»æœ¬åœ°æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
            checkpoint = torch.load(pretrained_file, map_location='cpu')
            
            # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
            if isinstance(checkpoint, dict):
                if 'ema' in checkpoint and 'module' in checkpoint['ema']:
                    # EMAæ ¼å¼: {'ema': {'module': {...}}}
                    state_dict = checkpoint['ema']['module']
                    self.logger.info("âœ“ æ£€æµ‹åˆ°EMA checkpointæ ¼å¼")
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                elif 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
        
            filtered_state_dict = {}
            skipped_class_params = 0
            
            for k, v in state_dict.items():
                # è·³è¿‡ç±»åˆ«ç›¸å…³çš„å‚æ•°ï¼ˆè¿™äº›å‚æ•°çš„å½¢çŠ¶ä¼šä¸åŒ¹é…ï¼‰
                if any(keyword in k for keyword in ['class_embed', 'score_head', 'denoising_class_embed']):
                    skipped_class_params += 1
                    continue
                filtered_state_dict[k] = v
            
            # åŠ è½½è¿‡æ»¤åçš„å‚æ•°
            missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)
            
            # ç»Ÿè®¡åŠ è½½ç»“æœï¼ˆä½¿ç”¨filtered_state_dictï¼‰
            total_params = len(filtered_state_dict)
            loaded_params = total_params - len(missing_keys)
            
            self.logger.info(f"âœ“ æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {loaded_params}/{total_params} ä¸ªå‚æ•°")
            
            # æŠ¥å‘Šè·³è¿‡çš„ç±»åˆ«å‚æ•°
            if skipped_class_params > 0:
                self.logger.info(f"  - è·³è¿‡ç±»åˆ«ç›¸å…³å‚æ•°: {skipped_class_params} ä¸ªï¼ˆCOCO 80ç±» â†’ DAIR-V2X 6ç±»ï¼‰")
            
            # ç»Ÿè®¡å„éƒ¨åˆ†çš„å‚æ•°
            backbone_loaded = sum(1 for k in filtered_state_dict.keys() if k not in missing_keys and 'backbone' in k)
            encoder_loaded = sum(1 for k in filtered_state_dict.keys() if k not in missing_keys and 'encoder' in k)
            decoder_loaded = sum(1 for k in filtered_state_dict.keys() if k not in missing_keys and 'decoder' in k)
            
            self.logger.info(f"  - Backbone: {backbone_loaded} ä¸ªå‚æ•°")
            self.logger.info(f"  - Encoder: {encoder_loaded} ä¸ªå‚æ•°")
            self.logger.info(f"  - Decoder: {decoder_loaded} ä¸ªå‚æ•°")
            
            if len(missing_keys) > 0:
                # missing_keysæ˜¯filtered_state_dictä¸­æœ‰ä½†å½“å‰æ¨¡å‹æ²¡æœ‰çš„å‚æ•°
                self.logger.info(f"  - é¢„è®­ç»ƒæ¨¡å‹ç¼ºå°‘å‚æ•°: {len(missing_keys)} ä¸ªï¼ˆå½“å‰æ¨¡å‹æ–°å¢ï¼‰")
                # æ˜¾ç¤ºå‰3ä¸ªç¤ºä¾‹
                if len(missing_keys) <= 5:
                    self.logger.info(f"    ç¤ºä¾‹: {list(missing_keys)}")
                else:
                    self.logger.info(f"    ç¤ºä¾‹: {list(missing_keys)[:3]} ...")
            
            if len(unexpected_keys) > 0:
                self.logger.info(f"  - æ¨¡å‹æ–°å¢å‚æ•°: {len(unexpected_keys)} ä¸ªï¼ˆå°†éšæœºåˆå§‹åŒ–ï¼‰")
            
        except Exception as e:
            self.logger.error(f"âœ— åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
            self.logger.info("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
    
    def create_criterion(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        from src.zoo.rtdetr.matcher import HungarianMatcher
        from src.zoo.rtdetr.rtdetrv2_criterion import RTDETRCriterionv2
        
        # åˆ›å»ºmatcher
        matcher = HungarianMatcher(
            weight_dict={'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2},
            use_focal_loss=False,
            alpha=0.25,
            gamma=2.0
        )
        
        # ä¸»æŸå¤±æƒé‡
        main_weight_dict = {
            'loss_vfl': 1.0,
            'loss_bbox': 5.0,
            'loss_giou': 2.0
        }
        
        # è¾…åŠ©æŸå¤±æƒé‡ï¼ˆdecoderçš„å‰N-1å±‚ï¼‰
        num_decoder_layers = self.config['model']['num_decoder_layers']
        aux_weight_dict = {}
        for i in range(num_decoder_layers - 1):  # å‰N-1å±‚
            aux_weight_dict[f'loss_vfl_aux_{i}'] = 1.0
            aux_weight_dict[f'loss_bbox_aux_{i}'] = 5.0
            aux_weight_dict[f'loss_giou_aux_{i}'] = 2.0
        
        # Encoderè¾…åŠ©æŸå¤±
        aux_weight_dict['loss_vfl_enc_0'] = 1.0
        aux_weight_dict['loss_bbox_enc_0'] = 5.0
        aux_weight_dict['loss_giou_enc_0'] = 2.0
        
        # Denoisingè¾…åŠ©æŸå¤±
        num_denoising_layers = num_decoder_layers  # å’Œdecoderå±‚æ•°ä¸€è‡´
        for i in range(num_denoising_layers):
            aux_weight_dict[f'loss_vfl_dn_{i}'] = 1.0
            aux_weight_dict[f'loss_bbox_dn_{i}'] = 5.0
            aux_weight_dict[f'loss_giou_dn_{i}'] = 2.0
        
        # åˆå¹¶æ‰€æœ‰æƒé‡
        weight_dict = {**main_weight_dict, **aux_weight_dict}
        
        # åˆ›å»ºcriterionï¼ˆä¸MoE RT-DETRä¿æŒä¸€è‡´ï¼‰
        criterion = RTDETRCriterionv2(
            matcher=matcher,
            weight_dict=weight_dict,
            losses=['vfl', 'boxes'],
            alpha=0.75,
            gamma=2.0,
            num_classes=6,
            boxes_weight_format=None,
            share_matched_indices=False
        )
        
        return criterion
    
    def create_datasets(self):
        """åˆ›å»ºæ•°æ®é›†"""
        from src.data.dataloader import BaseCollateFunction
        
        # åˆ›å»ºcollate_fnç±»
        class CustomCollateFunction(BaseCollateFunction):
            def __call__(self, batch):
                images, targets = zip(*batch)
                
                if isinstance(images[0], np.ndarray):
                    images = torch.stack([
                        torch.from_numpy(img).permute(2, 0, 1).float() / 255.0 
                        for img in images
                    ], dim=0)
                else:
                    images = torch.stack(images, 0)
                
                return images, list(targets)
        
        # ç›´æ¥ä½¿ç”¨DAIRV2XDetectionç±»ï¼ˆç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„data.data_rootï¼‰
        data_root = self.config['data']['data_root']
        use_mosaic = self.config.get('training', {}).get('use_mosaic', True)
        target_size = 640
        
        train_dataset = DAIRV2XDetection(
            data_root=data_root,
            split='train',
            transforms=None,
            use_mosaic=use_mosaic,
            target_size=target_size
        )
        
        val_dataset = DAIRV2XDetection(
            data_root=data_root,
            split='val',
            transforms=None,
            use_mosaic=False,  # éªŒè¯æ—¶ä¸ä½¿ç”¨Mosaic
            target_size=target_size
        )
        
        collate_fn = CustomCollateFunction()
        
        # num_workersåœ¨miscé…ç½®ä¸­
        num_workers = self.config.get('misc', {}).get('num_workers', 4)
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=True,
            num_workers=num_workers,
            collate_fn=collate_fn
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=False,
            num_workers=num_workers,
            collate_fn=collate_fn
        )
        
        return train_loader, val_loader
    
    def create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨åˆ†ç»„å­¦ä¹ ç‡ï¼‰"""
        # è·å–é…ç½®ä¸­çš„å­¦ä¹ ç‡ï¼Œç¡®ä¿æ˜¯æµ®ç‚¹æ•°ç±»å‹ï¼ˆç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶å­—æ®µåï¼‰
        new_lr = float(self.config['training']['new_lr'])
        pretrained_lr = float(self.config['training']['pretrained_lr'])
        weight_decay = float(self.config['training'].get('weight_decay', 0.0001))
        
        # åˆ†ç»„å‚æ•°
        param_groups = []
        
        # 1. Backboneå‚æ•°ï¼ˆä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ï¼Œæ’é™¤normå±‚ï¼‰
        backbone_params = []
        backbone_names = []
        for name, param in self.model.named_parameters():
            if 'backbone' in name and param.requires_grad:
                # æ’é™¤normå±‚
                if not any(norm in name for norm in ['norm', 'bn', 'gn', 'ln']):
                    backbone_params.append(param)
                    backbone_names.append(name)
        
        if backbone_params:
            param_groups.append({
                'params': backbone_params,
                'lr': pretrained_lr,
                'weight_decay': weight_decay
            })
            self.logger.info(f"âœ“ Backboneå‚æ•°ç»„: {len(backbone_params)} ä¸ªå‚æ•°, lr={pretrained_lr}")
        
        # 2. Normå±‚å‚æ•°ï¼ˆæ— weight decayï¼‰
        norm_params = []
        norm_names = []
        for name, param in self.model.named_parameters():
            if param.requires_grad and any(norm in name for norm in ['norm', 'bn', 'gn', 'ln']):
                norm_params.append(param)
                norm_names.append(name)
        
        if norm_params:
            param_groups.append({
                'params': norm_params,
                'lr': new_lr,
                'weight_decay': 0.0  # Normå±‚ä¸ä½¿ç”¨weight decay
            })
            self.logger.info(f"âœ“ Normå±‚å‚æ•°ç»„: {len(norm_params)} ä¸ªå‚æ•°, lr={new_lr}, wd=0")
        
        # 3. å…¶ä»–å‚æ•°ï¼ˆencoderã€decoderç­‰ï¼‰
        other_params = []
        other_names = []
        processed_params = set(id(p) for p in backbone_params + norm_params)
        
        for name, param in self.model.named_parameters():
            if param.requires_grad and id(param) not in processed_params:
                other_params.append(param)
                other_names.append(name)
        
        if other_params:
            param_groups.append({
                'params': other_params,
                'lr': new_lr,
                'weight_decay': weight_decay
            })
            self.logger.info(f"âœ“ å…¶ä»–å‚æ•°ç»„: {len(other_params)} ä¸ªå‚æ•°, lr={new_lr}")
        
        optimizer = torch.optim.AdamW(
            param_groups,
            betas=(0.9, 0.999)
        )
        
        return optimizer
    
    def create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_type = self.config['training'].get('scheduler', 'cosine')
        
        if scheduler_type == 'cosine':
            eta_min = float(self.config['training'].get('eta_min', 1e-7))
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['training']['epochs'],
                eta_min=eta_min
            )
            self.logger.info(f"âœ“ ä½¿ç”¨CosineAnnealingLRè°ƒåº¦å™¨ (eta_min={eta_min})")
        else:
            # MultiStepLR
            milestones = self.config['training'].get('milestones', [60, 80])
            gamma = float(self.config['training'].get('gamma', 0.1))
            scheduler = torch.optim.lr_scheduler.MultiStepLR(
                self.optimizer,
                milestones=milestones,
                gamma=gamma
            )
            self.logger.info(f"âœ“ ä½¿ç”¨MultiStepLRè°ƒåº¦å™¨ (milestones={milestones})")
        
        return scheduler
    
    def create_warmup_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨ï¼ˆä¸MoE RT-DETRä¿æŒä¸€è‡´ï¼‰"""
        warmup_epochs = self.config['training'].get('warmup_epochs', 3)
        
        # ç¡®ä¿warmup_end_lræ˜¯æµ®ç‚¹æ•°
        warmup_end_lr = float(self.config['training']['new_lr'])
        warmup_scheduler = WarmupLR(
            optimizer=self.optimizer,
            warmup_epochs=warmup_epochs,
            warmup_start_lr=1e-7,
            warmup_end_lr=warmup_end_lr
        )
        
        self.logger.info(f"âœ“ å­¦ä¹ ç‡é¢„çƒ­: {warmup_epochs} è½®")
        return warmup_scheduler
    
    def _save_latest_checkpoint(self):
        """ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ç”¨äºæ–­ç‚¹ç»­è®­ï¼ˆä¸moe-rtdeträ¸€è‡´ï¼‰"""
        try:
            checkpoint = {
                'epoch': self.last_epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'config': self.config,
                'best_loss': self.best_loss,
                'best_map': self.best_map,
                'best_metric': getattr(self, 'best_metric', 0.0),
                'global_step': getattr(self, 'global_step', 0)
            }
            
            # æ·»åŠ å¯é€‰ç»„ä»¶çŠ¶æ€
            if hasattr(self, 'warmup_scheduler') and self.warmup_scheduler:
                checkpoint['warmup_scheduler_state_dict'] = self.warmup_scheduler.state_dict()
            
            if hasattr(self, 'ema') and self.ema:
                checkpoint['ema_state_dict'] = self.ema.state_dict()
            
            if hasattr(self, 'scaler') and self.scaler:
                checkpoint['scaler_state_dict'] = self.scaler.state_dict()
            
            if hasattr(self, 'visualizer') and self.visualizer:
                checkpoint['visualizer_state_dict'] = self.visualizer.state_dict()
            
            if hasattr(self, 'early_stopping') and self.early_stopping:
                checkpoint['early_stopping_state'] = self.early_stopping.state_dict()
            
            # ä¿å­˜åˆ° log_dir
            latest_path = self.log_dir / 'latest_checkpoint.pth'
            torch.save(checkpoint, latest_path)
            self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹: {latest_path}")
            
        except Exception as e:
            self.logger.warning(f"ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def _save_best_checkpoint(self, epoch):
        """ä¿å­˜æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆåŸºäºmAPï¼‰"""
        try:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'config': self.config,
                'best_loss': self.best_loss,
                'best_map': self.best_map,
                'global_step': getattr(self, 'global_step', 0)
            }
            
            # æ·»åŠ å¯é€‰ç»„ä»¶çŠ¶æ€
            if hasattr(self, 'warmup_scheduler') and self.warmup_scheduler:
                checkpoint['warmup_scheduler_state_dict'] = self.warmup_scheduler.state_dict()
            
            if hasattr(self, 'ema') and self.ema:
                checkpoint['ema_state_dict'] = self.ema.state_dict()
            
            if hasattr(self, 'scaler') and self.scaler:
                checkpoint['scaler_state_dict'] = self.scaler.state_dict()
            
            if hasattr(self, 'visualizer') and self.visualizer:
                checkpoint['visualizer_state_dict'] = self.visualizer.state_dict()
            
            if hasattr(self, 'early_stopping') and self.early_stopping:
                checkpoint['early_stopping_state'] = self.early_stopping.state_dict()
            
            # ä¿å­˜åˆ° log_dir
            best_path = self.log_dir / 'best_model.pth'
            torch.save(checkpoint, best_path)
            self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
            
        except Exception as e:
            self.logger.warning(f"ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def start_training(self, resume_checkpoint=None):
        """å¼€å§‹è®­ç»ƒ"""
        # ä¿å­˜æ¢å¤æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆç”¨äºæ—¥å¿—è®¾ç½®ï¼‰
        self._resume_checkpoint_path = resume_checkpoint
        
        # é‡æ–°è®¾ç½®æ—¥å¿—ï¼ˆç°åœ¨å¯ä»¥æ­£ç¡®å¤„ç†æ¢å¤è®­ç»ƒçš„æƒ…å†µï¼‰
        self.setup_logging()
        
        self.logger.info("=" * 80)
        self.logger.info("ğŸš€ å¼€å§‹RT-DETRè®­ç»ƒ")
        self.logger.info("=" * 80)
        
        # æ˜¾ç¤ºå…³é”®é…ç½®ä¿¡æ¯
        self.logger.info("ğŸ“ è®­ç»ƒé…ç½®:")
        self.logger.info(f"  æ•°æ®é›†è·¯å¾„: {self.config['data']['data_root']}")
        self.logger.info(f"  è®­ç»ƒè½®æ•°: {self.config['training']['epochs']}")
        self.logger.info(f"  æ‰¹æ¬¡å¤§å°: {self.config['training']['batch_size']}")
        self.logger.info(f"  æ–°ç»„ä»¶å­¦ä¹ ç‡: {self.config['training']['new_lr']}")
        self.logger.info(f"  é¢„è®­ç»ƒç»„ä»¶å­¦ä¹ ç‡: {self.config['training']['pretrained_lr']}")
        self.logger.info(f"  è¾“å‡ºç›®å½•: {self.log_dir}")
        pretrained_weights_display = self.pretrained_weights or self.config.get('model', {}).get('pretrained_weights', None)
        if pretrained_weights_display:
            self.logger.info(f"  é¢„è®­ç»ƒæƒé‡: {pretrained_weights_display}")
        if resume_checkpoint:
            self.logger.info(f"  æ¢å¤æ£€æŸ¥ç‚¹: {resume_checkpoint}")
        self.logger.info("=" * 80)
        
        # 1. åˆ›å»ºæ¨¡å‹
        self.model = self.create_model()
        
        # 2. åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæä¾›ï¼‰
        pretrained_weights = self.pretrained_weights or self.config.get('model', {}).get('pretrained_weights', None)
        if pretrained_weights and not resume_checkpoint:
            self.logger.info(f"ğŸ”— åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_weights}")
            self.load_pretrained_weights(self.model, pretrained_weights)
        else:
            self.logger.info("â„¹ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
        
        # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        
        # 3. åˆ›å»ºå…¶ä»–ç»„ä»¶
        self.criterion = self.create_criterion()
        self.train_dataloader, self.val_dataloader = self.create_datasets()
        self.optimizer = self.create_optimizer()
        self.scheduler = self.create_scheduler()
        self.warmup_scheduler = self.create_warmup_scheduler()
        
        # 4. åˆ›å»ºEMAå’Œæ¢¯åº¦ç¼©æ”¾å™¨
        ema_decay = self.config['training'].get('ema_decay', 0.9999)
        self.ema = ModelEMA(self.model, decay=ema_decay)
        self.scaler = GradScaler()
        self.logger.info(f"âœ“ EMA decay={ema_decay}, æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        
        # 5. åˆ›å»ºå¯è§†åŒ–å™¨ï¼ˆä½¿ç”¨log_dirï¼‰
        self.visualizer = TrainingVisualizer(
            log_dir=self.log_dir,
            model_type='standard',
            experiment_name=self.experiment_name
        )
        
        # 6. è®¾ç½®è®­ç»ƒå±æ€§
        self.last_epoch = -1
        self.best_loss = float('inf')
        self.best_map = 0.0  # è®°å½•æœ€ä½³mAP
        
        # 6.5 åˆå§‹åŒ–Early Stopping
        self.early_stopping = self._create_early_stopping()
        
        # 7. è®¾ç½®æ¢¯åº¦è£å‰ªå‚æ•°
        self.clip_max_norm = self.config['training'].get('clip_max_norm', 10.0)
        self.logger.info(f"âœ“ æ¢¯åº¦è£å‰ª: max_norm={self.clip_max_norm}")
        
        # 8. æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæä¾›checkpointï¼‰
        if resume_checkpoint:
            self.logger.info(f"ğŸ“¦ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_checkpoint}")
            checkpoint = torch.load(resume_checkpoint, map_location=self.device)
            
            # æ¢å¤æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            # æ¢å¤warmupè°ƒåº¦å™¨
            if 'warmup_scheduler_state_dict' in checkpoint and self.warmup_scheduler:
                self.warmup_scheduler.load_state_dict(checkpoint['warmup_scheduler_state_dict'])
            
            # æ¢å¤EMA
            if 'ema_state_dict' in checkpoint and self.ema:
                self.ema.load_state_dict(checkpoint['ema_state_dict'])
            
            # æ¢å¤å¯è§†åŒ–å™¨å†å²è®°å½•
            if 'visualizer_state_dict' in checkpoint and self.visualizer:
                self.visualizer.load_state_dict(checkpoint['visualizer_state_dict'])
                self.logger.info(f"âœ“ å·²æ¢å¤è®­ç»ƒå†å²è®°å½•")
            
            # æ¢å¤early stoppingçŠ¶æ€
            if 'early_stopping_state' in checkpoint and self.early_stopping:
                self.early_stopping.load_state_dict(checkpoint['early_stopping_state'])
                self.logger.info(f"âœ“ å·²æ¢å¤Early StoppingçŠ¶æ€")
            
            # æ¢å¤epochè®¡æ•°å’Œæœ€ä½³æŒ‡æ ‡
            self.last_epoch = checkpoint.get('epoch', -1)
            self.best_loss = checkpoint.get('best_loss', float('inf'))
            self.best_map = checkpoint.get('best_map', 0.0)
            self.logger.info(f'âœ“ ä»epoch {self.last_epoch + 1}æ¢å¤è®­ç»ƒ')
            
            # æ˜¾ç¤ºæ¢å¤çš„è®­ç»ƒä¿¡æ¯
            if 'best_metric' in checkpoint:
                self.logger.info(f"âœ“ å†å²æœ€ä½³æŒ‡æ ‡: {checkpoint['best_metric']}")
            self.logger.info(f"âœ“ æœ€ä½³loss: {self.best_loss:.4f}, æœ€ä½³mAP: {self.best_map:.4f}")
            if 'train_loss' in checkpoint:
                self.logger.info(f"âœ“ ä¸Šæ¬¡è®­ç»ƒæŸå¤±: {checkpoint['train_loss']:.4f}")
            if 'val_loss' in checkpoint:
                self.logger.info(f"âœ“ ä¸Šæ¬¡éªŒè¯æŸå¤±: {checkpoint['val_loss']:.4f}")
        
        # 9. æ‰“å°è®­ç»ƒé…ç½®æ‘˜è¦
        self.logger.info("=" * 80)
        self.logger.info("è®­ç»ƒé…ç½®æ‘˜è¦:")
        self.logger.info(f"  - è®­ç»ƒè½®æ•°: {self.config['training']['epochs']}")
        self.logger.info(f"  - æ‰¹æ¬¡å¤§å°: {self.config['training']['batch_size']}")
        self.logger.info(f"  - æ–°ç»„ä»¶å­¦ä¹ ç‡: {self.config['training']['new_lr']}")
        self.logger.info(f"  - é¢„è®­ç»ƒç»„ä»¶å­¦ä¹ ç‡: {self.config['training']['pretrained_lr']}")
        self.logger.info(f"  - Weight decay: {self.config['training']['weight_decay']}")
        self.logger.info(f"  - Warmupè½®æ•°: {self.config['training'].get('warmup_epochs', 3)}")
        self.logger.info(f"  - æ¢¯åº¦è£å‰ª: {self.clip_max_norm}")
        self.logger.info(f"  - è®¾å¤‡: {self.device}")
        self.logger.info("=" * 80)
        
        self._custom_training_loop()
        
        # ä¿å­˜æœ€ç»ˆçš„ latest_checkpointï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
        self._save_latest_checkpoint()
    
    def _create_early_stopping(self) -> Optional[EarlyStopping]:
        """åˆ›å»ºEarly Stoppingã€‚"""
        training_config = self.config.get('training', {})
        patience = training_config.get('early_stopping_patience', None)
        
        if patience is None or patience <= 0:
            self.logger.info("â±ï¸  Early Stopping: æœªå¯ç”¨")
            return None
        
        metric_name = training_config.get('early_stopping_metric', 'mAP_0.5_0.95')
        mode = 'max' if 'mAP' in metric_name or 'AP' in metric_name else 'min'
        
        self.logger.info(f"â±ï¸  Early Stopping: å¯ç”¨ (patience={patience}, metric={metric_name}, mode={mode})")
        
        return EarlyStopping(
            patience=patience,
            mode=mode,
            min_delta=0.0001,
            metric_name=metric_name,
            logger=self.logger
        )
    
    def _custom_training_loop(self):
        """è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯"""
        epochs = self.config['training']['epochs']
        self.logger.info(f"å¼€å§‹è®­ç»ƒ {epochs} epochs")
        
        for epoch in range(self.last_epoch + 1, epochs):
            self.last_epoch = epoch
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self._train_epoch()
            
            # éªŒè¯
            val_metrics = self._validate_epoch()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if hasattr(self, 'warmup_scheduler') and self.warmup_scheduler and not self.warmup_scheduler.finished():
                self.warmup_scheduler.step()
            else:
                self.scheduler.step()
            
            # è¾“å‡ºæ—¥å¿—
            self.logger.info(f"Epoch {epoch}:")
            self.logger.info(f"  è®­ç»ƒæŸå¤±: {train_metrics.get('total_loss', 0.0):.2f} | éªŒè¯æŸå¤±: {val_metrics.get('total_loss', 0.0):.2f}")
            self.logger.info(f"  mAP@0.5: {val_metrics.get('mAP_0.5', 0.0):.4f} | mAP@0.75: {val_metrics.get('mAP_0.75', 0.0):.4f} | "
                           f"mAP@[0.5:0.95]: {val_metrics.get('mAP_0.5_0.95', 0.0):.4f}")
            self.logger.info(f"  é¢„æµ‹/ç›®æ ‡: {val_metrics['num_predictions']}/{val_metrics['num_targets']}")
            
            # è®°å½•åˆ°å¯è§†åŒ–å™¨
            current_lr = self.optimizer.param_groups[0]['lr']
            self.visualizer.record(
                epoch=epoch,
                train_loss=train_metrics.get('total_loss', 0.0),
                val_loss=val_metrics.get('total_loss', 0.0),
                mAP_0_5=val_metrics.get('mAP_0.5', 0.0),
                mAP_0_75=val_metrics.get('mAP_0.75', 0.0),
                mAP_0_5_0_95=val_metrics.get('mAP_0.5_0.95', 0.0),
                learning_rate=current_lr
            )
            
            # ä¿å­˜æ£€æŸ¥ç‚¹ - åŒæ—¶è€ƒè™‘losså’ŒmAP
            is_best_loss = val_metrics.get('total_loss', float('inf')) < self.best_loss
            is_best_map = val_metrics.get('mAP_0.5_0.95', 0.0) > self.best_map
            
            if is_best_loss:
                self.best_loss = val_metrics.get('total_loss', float('inf'))
                self.logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.2f}")
            
            if is_best_map:
                self.best_map = val_metrics.get('mAP_0.5_0.95', 0.0)
                self.logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³mAP: {self.best_map:.4f}")
                # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºmAPï¼‰
                self._save_best_checkpoint(epoch)
            
            # Early Stoppingæ£€æŸ¥
            if self.early_stopping:
                # è·å–è¦ç›‘æ§çš„æŒ‡æ ‡å€¼
                metric_name = self.early_stopping.metric_name
                if 'mAP_0.5_0.95' in metric_name or 'mAP_0.5:0.95' in metric_name:
                    metric_value = val_metrics.get('mAP_0.5_0.95', 0.0)
                elif 'mAP_0.5' in metric_name:
                    metric_value = val_metrics.get('mAP_0.5', 0.0)
                elif 'mAP_0.75' in metric_name:
                    metric_value = val_metrics.get('mAP_0.75', 0.0)
                elif 'loss' in metric_name.lower():
                    metric_value = val_metrics.get('total_loss', float('inf'))
                else:
                    metric_value = val_metrics.get('mAP_0.5_0.95', 0.0)  # é»˜è®¤
                
                if self.early_stopping(metric_value, epoch):
                    self.logger.info(f"Early Stoppingåœ¨epoch {epoch}è§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                    break
            
            # æ¯ä¸ªepochéƒ½ä¿å­˜latestç”¨äºæ–­ç‚¹ç»­è®­
            self._save_latest_checkpoint()
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆæ¯ä¸ªepochéƒ½æ›´æ–°ï¼‰
            try:
                self.visualizer.plot()
            except Exception as e:
                self.logger.warning(f"ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¤±è´¥: {e}")
        
        # è®­ç»ƒå®Œæˆåï¼Œç»˜åˆ¶æœ€ç»ˆçš„è®­ç»ƒæ›²çº¿å¹¶å¯¼å‡ºCSV
        self.logger.info("âœ“ è®­ç»ƒå®Œæˆï¼")
        try:
            self.visualizer.plot()
            self.visualizer.export_to_csv()
            self.logger.info(f"âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {self.log_dir}/training_curves.png")
            self.logger.info(f"âœ“ è®­ç»ƒå†å²å·²å¯¼å‡ºåˆ°: {self.log_dir}/training_history.csv")
            self.logger.info(f"âœ“ æ‰€æœ‰è¾“å‡ºå·²ä¿å­˜åˆ°: {self.log_dir}")
        except Exception as e:
            self.logger.warning(f"ç»˜åˆ¶æœ€ç»ˆè®­ç»ƒæ›²çº¿å¤±è´¥: {e}")
    
    def _train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        detection_loss = 0.0
        
        for batch_idx, (images, targets) in enumerate(self.train_dataloader):
            images = images.to(self.device)
            targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                       for k, v in t.items()} for t in targets]
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            with torch.cuda.amp.autocast():
                outputs = self.model(images, targets)
                # ä½¿ç”¨criterionè®¡ç®—æŸå¤±
                loss_dict = self.criterion(outputs, targets)
                loss = sum(loss_dict.values())
            
            # åå‘ä¼ æ’­
            self.scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ª
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.clip_max_norm)
            
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.ema.update(self.model)
            
            # ç»Ÿè®¡æŸå¤±
            total_loss += loss.item()
            
            # è®¡ç®—æ£€æµ‹æŸå¤±ï¼ˆä¸»è¦æŸå¤±é¡¹ï¼‰
            det_loss_val = 0.0
            if 'loss_vfl' in loss_dict:
                det_loss_val += loss_dict['loss_vfl'].item()
            if 'loss_bbox' in loss_dict:
                det_loss_val += loss_dict['loss_bbox'].item()
            if 'loss_giou' in loss_dict:
                det_loss_val += loss_dict['loss_giou'].item()
            
            detection_loss += det_loss_val
            
            # æ¯50ä¸ªbatchæ‰“å°ä¸€æ¬¡ï¼ˆå‚ç…§moe-rtdetræ ¼å¼ï¼‰
            if batch_idx % 50 == 0:
                self.logger.info(f'Epoch {self.last_epoch} | Batch {batch_idx} | '
                               f'Loss: {loss.item():.2f} (Det: {det_loss_val:.2f})')
        
        # è®¡ç®—å¹³å‡å€¼
        num_batches = len(self.train_dataloader)
        avg_loss = total_loss / num_batches
        avg_detection_loss = detection_loss / num_batches
        
        return {
            'total_loss': avg_loss,
            'detection_loss': avg_detection_loss
        }
    
    def _validate_epoch(self):
        """éªŒè¯æ¨¡å‹å¹¶è®¡ç®—mAP"""
        self.ema.module.eval()
        total_loss = 0.0
        all_predictions = []
        all_targets = []
        total_raw_predictions = 0  # åŸå§‹queryæ€»æ•°
        
        with torch.no_grad():
            for batch_idx, (images, targets) in enumerate(self.val_dataloader):
                images = images.to(self.device)
                targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                           for k, v in t.items()} for t in targets]
                
                outputs = self.ema.module(images, targets)
                
                # ä½¿ç”¨criterionè®¡ç®—æŸå¤±
                loss_dict = self.criterion(outputs, targets)
                loss = sum(loss_dict.values())
                total_loss += loss.item()
                
                # ç»Ÿè®¡åŸå§‹é¢„æµ‹æ•°ï¼ˆæ‰€æœ‰queriesï¼‰
                if 'pred_logits' in outputs:
                    total_raw_predictions += outputs['pred_logits'].shape[0] * outputs['pred_logits'].shape[1]
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                if 'pred_logits' in outputs and 'pred_boxes' in outputs:
                    self._collect_predictions(outputs, targets, batch_idx, all_predictions, all_targets)
        
        # è®¡ç®—mAP
        mAP_metrics = self._compute_map_metrics(all_predictions, all_targets)
        
        avg_loss = total_loss / len(self.val_dataloader)
        
        return {
            'total_loss': avg_loss,
            'mAP_0.5': mAP_metrics.get('mAP_0.5', 0.0),
            'mAP_0.75': mAP_metrics.get('mAP_0.75', 0.0),
            'mAP_0.5_0.95': mAP_metrics.get('mAP_0.5_0.95', 0.0),
            'num_predictions': len(all_predictions),
            'num_raw_predictions': len(all_predictions),  # ä¿®å¤ï¼šä½¿ç”¨å®é™…é¢„æµ‹æ•°
            'num_targets': len(all_targets)
        }
    
    def _collect_predictions(self, outputs: Dict, targets: List[Dict], batch_idx: int,
                            all_predictions: List, all_targets: List) -> None:
        """æ”¶é›†é¢„æµ‹ç»“æœç”¨äºmAPè®¡ç®—ã€‚ä¿ç•™æ‰€æœ‰æœ‰æ•ˆé¢„æµ‹æ¡†ï¼Œä¸åštop-ké™åˆ¶ã€‚"""
        pred_logits = outputs['pred_logits']  # [B, Q, C]
        pred_boxes = outputs['pred_boxes']    # [B, Q, 4]
        
        batch_size = pred_logits.shape[0]
        
        for i in range(batch_size):
            pred_scores = torch.softmax(pred_logits[i], dim=-1)  # [Q, C]
            max_scores, pred_classes = torch.max(pred_scores, dim=-1)  # [Q]
            
            # è¿‡æ»¤æ— æ•ˆæ¡†ï¼ˆpaddingæ¡†ï¼‰ï¼Œä¿ç•™æ‰€æœ‰æœ‰æ•ˆé¢„æµ‹æ¡†
            valid_boxes_mask = ~torch.all(pred_boxes[i] == 1.0, dim=1)
            valid_indices = torch.where(valid_boxes_mask)[0]
            if len(valid_indices) > 0:
                filtered_boxes = pred_boxes[i][valid_indices]
                filtered_classes = pred_classes[valid_indices]
                filtered_scores = max_scores[valid_indices]
                
                # è½¬æ¢ä¸ºCOCOæ ¼å¼
                if filtered_boxes.shape[0] > 0:
                    boxes_coco = torch.zeros_like(filtered_boxes)
                    if filtered_boxes.max() <= 1.0:
                        # å½’ä¸€åŒ–åæ ‡ -> åƒç´ åæ ‡
                        boxes_coco[:, 0] = (filtered_boxes[:, 0] - filtered_boxes[:, 2] / 2) * 640
                        boxes_coco[:, 1] = (filtered_boxes[:, 1] - filtered_boxes[:, 3] / 2) * 640
                        boxes_coco[:, 2] = filtered_boxes[:, 2] * 640
                        boxes_coco[:, 3] = filtered_boxes[:, 3] * 640
                    else:
                        boxes_coco = filtered_boxes.clone()
                    
                    # Clampåæ ‡
                    boxes_coco[:, 0] = torch.clamp(boxes_coco[:, 0], 0, 640)
                    boxes_coco[:, 1] = torch.clamp(boxes_coco[:, 1], 0, 640)
                    boxes_coco[:, 2] = torch.clamp(boxes_coco[:, 2], 1, 640)
                    boxes_coco[:, 3] = torch.clamp(boxes_coco[:, 3], 1, 640)
                    
                    for j in range(filtered_boxes.shape[0]):
                        all_predictions.append({
                            'image_id': batch_idx * self.config['training']['batch_size'] + i,
                            'category_id': int(filtered_classes[j].item()) + 1,
                            'bbox': boxes_coco[j].cpu().numpy().tolist(),
                            'score': float(filtered_scores[j].item())
                        })
            
            # å¤„ç†çœŸå®æ ‡ç­¾
            if i < len(targets) and 'labels' in targets[i] and 'boxes' in targets[i]:
                true_labels = targets[i]['labels']
                true_boxes = targets[i]['boxes']
                
                if len(true_labels) > 0:
                    img_size = 640
                    max_val = float(true_boxes.max().item()) if true_boxes.numel() > 0 else 0.0
                    scale = img_size if max_val <= 1.0 + 1e-6 else 1.0
                    
                    true_boxes_coco = torch.zeros_like(true_boxes)
                    true_boxes_coco[:, 0] = (true_boxes[:, 0] - true_boxes[:, 2] / 2) * scale
                    true_boxes_coco[:, 1] = (true_boxes[:, 1] - true_boxes[:, 3] / 2) * scale
                    true_boxes_coco[:, 2] = true_boxes[:, 2] * scale
                    true_boxes_coco[:, 3] = true_boxes[:, 3] * scale
                    
                    true_boxes_coco[:, 0] = torch.clamp(true_boxes_coco[:, 0], 0, img_size)
                    true_boxes_coco[:, 1] = torch.clamp(true_boxes_coco[:, 1], 0, img_size)
                    true_boxes_coco[:, 2] = torch.clamp(true_boxes_coco[:, 2], 1, img_size)
                    true_boxes_coco[:, 3] = torch.clamp(true_boxes_coco[:, 3], 1, img_size)
                    
                    for j in range(len(true_labels)):
                        all_targets.append({
                            'image_id': batch_idx * self.config['training']['batch_size'] + i,
                            'category_id': int(true_labels[j].item()) + 1,
                            'bbox': true_boxes_coco[j].cpu().numpy().tolist(),
                            'area': float((true_boxes_coco[j, 2] * true_boxes_coco[j, 3]).item()),
                            'iscrowd': 0
                        })
    
    def _compute_map_metrics(self, predictions: List[Dict], targets: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—mAPæŒ‡æ ‡ã€‚"""
        try:
            if len(predictions) == 0:
                return {
                    'mAP_0.5': 0.0,
                    'mAP_0.75': 0.0,
                    'mAP_0.5_0.95': 0.0
                }
            
            # è·å–ç±»åˆ«ä¿¡æ¯
            if hasattr(self, 'val_dataloader') and hasattr(self.val_dataloader.dataset, 'get_categories'):
                categories = self.val_dataloader.dataset.get_categories()
            else:
                categories = [
                    {'id': 1, 'name': 'car'},
                    {'id': 2, 'name': 'truck'},
                    {'id': 3, 'name': 'bus'},
                    {'id': 4, 'name': 'person'},
                    {'id': 5, 'name': 'bicycle'},
                    {'id': 6, 'name': 'motorcycle'}
                ]
            
            # åˆ›å»ºCOCOæ ¼å¼æ•°æ®
            coco_gt = {
                'images': [],
                'annotations': [],
                'categories': categories,
                'info': {
                    'description': 'DAIR-V2X Dataset',
                    'version': '1.0',
                    'year': 2024
                }
            }
            
            # æ·»åŠ å›¾åƒä¿¡æ¯
            image_ids = set(target['image_id'] for target in targets)
            for img_id in image_ids:
                coco_gt['images'].append({
                    'id': img_id, 
                    'width': 640, 
                    'height': 640
                })
            
            # æ·»åŠ æ ‡æ³¨
            for i, target in enumerate(targets):
                target['id'] = i + 1
                coco_gt['annotations'].append(target)
            
            # ä½¿ç”¨pycocotoolsè¯„ä¼°
            coco_gt_obj = COCO()
            coco_gt_obj.dataset = coco_gt
            coco_gt_obj.createIndex()
            
            coco_dt = coco_gt_obj.loadRes(predictions)
            
            coco_eval = COCOeval(coco_gt_obj, coco_dt, 'bbox')
            coco_eval.evaluate()
            coco_eval.accumulate()
            coco_eval.summarize()
            
            return {
                'mAP_0.5': coco_eval.stats[1],
                'mAP_0.75': coco_eval.stats[2],
                'mAP_0.5_0.95': coco_eval.stats[0]
            }
            
        except Exception as e:
            self.logger.warning(f"mAPè®¡ç®—å¤±è´¥: {e}")
            return {
                'mAP_0.5': 0.0,
                'mAP_0.75': 0.0,
                'mAP_0.5_0.95': 0.0
            }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='RT-DETRè®­ç»ƒè„šæœ¬')
    parser.add_argument('--backbone', type=str, default='presnet50', 
                       choices=['presnet18', 'presnet34', 'presnet50', 'presnet101',
                               'hgnetv2_l', 'hgnetv2_x', 'hgnetv2_h',
                               'cspresnet_s', 'cspresnet_m', 'cspresnet_l', 'cspresnet_x',
                               'cspdarknet', 'mresnet'],
                       help='Backboneç±»å‹')
    parser.add_argument('--data_root', type=str, default='datasets/DAIR-V2X', 
                       help='DAIR-V2Xæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--pretrained_lr', type=float, default=1e-5, help='é¢„è®­ç»ƒç»„ä»¶å­¦ä¹ ç‡')
    parser.add_argument('--new_lr', type=float, default=1e-4, help='æ–°ç»„ä»¶å­¦ä¹ ç‡')
    parser.add_argument('--warmup_epochs', type=int, default=3, 
                       help='å­¦ä¹ ç‡é¢„çƒ­è½®æ•°')
    parser.add_argument('--pretrained_weights', type=str, default=None,
                       help='é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆRT-DETR COCOé¢„è®­ç»ƒæ¨¡å‹ï¼‰')
    parser.add_argument('--resume_from_checkpoint', type=str, default=None,
                       help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼‰')
    parser.add_argument('--config', type=str, default=None,
                       help='YAMLé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­ï¼Œç”¨äºç¡®ä¿å®éªŒå¯é‡å¤æ€§ï¼ˆé»˜è®¤ï¼š42ï¼‰')
    parser.add_argument('--deterministic', action='store_true',
                       help='ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ï¼ˆä¼šé™ä½é€Ÿåº¦ä½†ä¿è¯å®Œå…¨å¯é‡å¤ï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰æ“ä½œä¹‹å‰ï¼‰
    print("\n" + "="*60)
    print("ğŸ”§ åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ")
    print("="*60)
    set_seed(args.seed, deterministic=args.deterministic)
    
    # åŠ è½½é…ç½®
    if args.config and args.config.endswith('.yaml'):
        # ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"ğŸ“„ ä»é…ç½®æ–‡ä»¶åŠ è½½: {args.config}")
        
        # ç¡®ä¿å­¦ä¹ ç‡ç›¸å…³å€¼æ˜¯æµ®ç‚¹æ•°ï¼ˆYAMLä¸­çš„ç§‘å­¦è®¡æ•°æ³•å¯èƒ½è¢«è§£æä¸ºå­—ç¬¦ä¸²ï¼‰
        # ç›´æ¥ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å­—æ®µåï¼špretrained_lr, new_lr
        if 'training' in config:
            # ç±»å‹è½¬æ¢ç¡®ä¿æ˜¯æµ®ç‚¹æ•°
            if 'pretrained_lr' in config['training']:
                config['training']['pretrained_lr'] = float(config['training']['pretrained_lr'])
            if 'new_lr' in config['training']:
                config['training']['new_lr'] = float(config['training']['new_lr'])
            if 'eta_min' in config['training']:
                config['training']['eta_min'] = float(config['training']['eta_min'])
            if 'weight_decay' in config['training']:
                config['training']['weight_decay'] = float(config['training']['weight_decay'])
        
        # å…è®¸å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
        if args.backbone != 'presnet50':
            config['model']['backbone'] = args.backbone
        if args.epochs != 100:
            config['training']['epochs'] = args.epochs
        if args.batch_size != 32:
            config['training']['batch_size'] = args.batch_size
        if args.pretrained_lr != 1e-5:
            config['training']['pretrained_lr'] = args.pretrained_lr
        if args.new_lr != 1e-4:
            config['training']['new_lr'] = args.new_lr
        if args.warmup_epochs != 3:
            config['training']['warmup_epochs'] = args.warmup_epochs
        if args.data_root != 'datasets/DAIR-V2X':
            config['data']['data_root'] = args.data_root
        if args.pretrained_weights:
            config['model']['pretrained_weights'] = args.pretrained_weights
    else:
        # åˆ›å»ºé»˜è®¤é…ç½®
        config = {
        'model': {
            'hidden_dim': 256,
            'num_queries': 100,
            'backbone': args.backbone
        },
        'data': {
            'data_root': args.data_root
        },
        'train_dataloader': {
            'dataset': 'DAIRV2XDetection',
            'batch_size': args.batch_size,
            'shuffle': True,
            'num_workers': 4,
            'collate_fn': None
        },
        'val_dataloader': {
            'dataset': 'DAIRV2XDetection',
            'batch_size': args.batch_size,
            'shuffle': False,
            'num_workers': 4,
            'collate_fn': None
        },
        'training': {
            'device': 'cuda',
            'epochs': args.epochs,
            'batch_size': args.batch_size,
            'new_lr': args.new_lr,
            'pretrained_lr': args.pretrained_lr,
            'weight_decay': 0.0001,
            'num_workers': 4,
            'save_interval': 10,
            'print_freq': 50,
            'log_dir': 'logs',
            'save_dir': 'checkpoints',
            'output_dir': 'checkpoints',
            'ema_decay': 0.9999,
            'scheduler': 'cosine',
            'eta_min': 0.0000001,
            'warmup_epochs': args.warmup_epochs,
            'clip_max_norm': 10.0
        },
        'validation': {
            'interval': 5,
            'metrics': ['mAP', 'mAP_50', 'mAP_75']
        },
        'augmentation': {
            'mixup': {'enabled': False, 'alpha': 0.2},
            'cutmix': {'enabled': False, 'alpha': 1.0},
            'mosaic': {'enabled': True, 'prob': 0.5}
        }
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    # å¦‚æœä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œåªä¼ é€’æ˜¾å¼ä¼ é€’çš„å‚æ•°ï¼ˆä¸ç­‰äºé»˜è®¤å€¼çš„ï¼‰ï¼Œå…¶ä»–ä¼ é€’Noneè®©é…ç½®æ–‡ä»¶çš„å€¼ç”Ÿæ•ˆ
    if args.config and args.config.endswith('.yaml'):
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼šåªä¼ é€’æ˜¾å¼ä¼ é€’çš„å‚æ•°ï¼Œé»˜è®¤å€¼å‚æ•°ä¼ é€’None
        data_root_arg = None if args.data_root == 'datasets/DAIR-V2X' else args.data_root
        epochs_arg = None if args.epochs == 100 else args.epochs
        batch_size_arg = None if args.batch_size == 32 else args.batch_size
        warmup_epochs_arg = None if args.warmup_epochs == 3 else args.warmup_epochs
    else:
        # ä¸ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼šä¼ é€’æ‰€æœ‰å‚æ•°ï¼ˆåŒ…æ‹¬é»˜è®¤å€¼ï¼‰
        data_root_arg = args.data_root
        epochs_arg = args.epochs
        batch_size_arg = args.batch_size
        warmup_epochs_arg = args.warmup_epochs
    
    trainer = RTDETRTrainer(
        config=config,
        pretrained_weights=args.pretrained_weights,
        data_root=data_root_arg,
        epochs=epochs_arg,
        batch_size=batch_size_arg,
        warmup_epochs=warmup_epochs_arg
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.start_training(resume_checkpoint=args.resume_from_checkpoint)


if __name__ == '__main__':
    main()
