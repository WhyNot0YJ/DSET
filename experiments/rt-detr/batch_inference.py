#!/usr/bin/env python3
"""RT-DETR æ‰¹é‡æ¨ç†è„šæœ¬ - å¤„ç†æ•´ä¸ªå›¾åƒç›®å½•"""

import sys
import argparse
import yaml
import torch
import cv2
import numpy as np
from pathlib import Path

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    class SimpleProgress:
        def __init__(self, iterable, desc=""):
            self.iterable = iterable
            self.desc = desc
            self.total = len(iterable)
            self.current = 0
            print(f"{desc}: å¼€å§‹å¤„ç† {self.total} ä¸ªæ–‡ä»¶...")
        
        def __iter__(self):
            for item in self.iterable:
                self.current += 1
                if self.current % 10 == 0 or self.current == self.total:
                    print(f"  è¿›åº¦: {self.current}/{self.total} ({100*self.current/self.total:.1f}%)")
                yield item
    
    def tqdm(iterable, desc=""):
        return SimpleProgress(iterable, desc) if not HAS_TQDM else iterable

# æ·»åŠ é¡¹ç›®è·¯å¾„ï¼ˆä»…åœ¨ç›´æ¥è¿è¡Œæ—¶æ‰§è¡Œï¼Œå¯¼å…¥æ—¶ä¸æ‰§è¡Œï¼‰
def _setup_paths():
    """è®¾ç½®é¡¹ç›®è·¯å¾„ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯å¯¼å…¥ï¼‰"""
    project_root = Path(__file__).parent.resolve()
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    if str(project_root.parent) not in sys.path:
        sys.path.insert(0, str(project_root.parent))

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨å¯¼å…¥æ—¶å°±æ‰§è¡Œè·¯å¾„è®¾ç½®
def _import_modules():
    """å»¶è¿Ÿå¯¼å…¥æ¨¡å—"""
    _setup_paths()
    from train import RTDETRTrainer, create_backbone
    from src.nn.postprocessor.detr_postprocessor import DetDETRPostProcessor
    from src.nn.postprocessor.box_revert import box_revert, BoxProcessFormat
    return RTDETRTrainer, create_backbone, DetDETRPostProcessor, BoxProcessFormat

# ç±»åˆ«åç§°ï¼ˆ8ç±»ï¼‰
CLASS_NAMES = [
    "Car", "Truck", "Van", "Bus", "Pedestrian", 
    "Cyclist", "Motorcyclist", "Trafficcone"
]
COLORS = [
    (255, 0, 0),      # Car - çº¢è‰²
    (0, 255, 0),      # Truck - ç»¿è‰²
    (255, 128, 0),    # Van - æ©™è‰²
    (0, 0, 255),      # Bus - è“è‰²
    (255, 255, 0),    # Pedestrian - é»„è‰²
    (255, 0, 255),    # Cyclist - å“çº¢
    (0, 255, 255),    # Motorcyclist - é’è‰²
    (128, 128, 128),  # Trafficcone - ç°è‰²
]


def load_model(config_path: str, checkpoint_path: str, device: str = "cuda"):
    """åŠ è½½æ¨¡å‹å’Œæƒé‡"""
    RTDETRTrainer, _, DetDETRPostProcessor, BoxProcessFormat = _import_modules()
    
    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºè®­ç»ƒå™¨ä»¥æ„å»ºæ¨¡å‹
    trainer = RTDETRTrainer(config)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„loggerï¼ˆæ¨ç†æ—¶ä¸éœ€è¦æ—¥å¿—ï¼Œåªéœ€è¦æ¨¡å‹èƒ½åˆ›å»ºï¼‰
    if trainer.logger is None:
        class SimpleLogger:
            def info(self, msg): pass  # ä»€ä¹ˆéƒ½ä¸åš
        trainer.logger = SimpleLogger()
    
    model = trainer.create_model()
    
    # åŠ è½½checkpoint
    # å…¼å®¹ PyTorch 2.6+ (weights_only=True by default)
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    except TypeError:
        # æ—§ç‰ˆæœ¬ PyTorch ä¸æ”¯æŒ weights_only å‚æ•°
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # éªŒè¯æ—¶ä½¿ç”¨ self.ema.moduleï¼Œæ‰€ä»¥æ¨ç†æ—¶ä¹Ÿåº”è¯¥ä½¿ç”¨EMAæƒé‡
    if 'ema' in checkpoint and 'module' in checkpoint['ema']:
        print("  ä½¿ç”¨EMAæ¨¡å‹æƒé‡ï¼ˆä¸éªŒè¯æ—¶ä¸€è‡´ï¼‰")
        state_dict = checkpoint['ema']['module']
    elif 'model_state_dict' in checkpoint:
        print("  ä½¿ç”¨æ™®é€šæ¨¡å‹æƒé‡ï¼ˆæœªæ‰¾åˆ°EMAæƒé‡ï¼‰")
        state_dict = checkpoint['model_state_dict']
    elif 'model' in checkpoint:
        state_dict = checkpoint['model']
    else:
        state_dict = checkpoint
    
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    model = model.to(device)
    
    # ä»é…ç½®æ–‡ä»¶è¯»å– num_queriesï¼ˆæ¨¡å‹å®é™…ç”Ÿæˆçš„æŸ¥è¯¢æ•°é‡ï¼‰
    num_queries = config.get('model', {}).get('num_queries', 300)
    
    # åˆ›å»ºåå¤„ç†å™¨ï¼ˆä½¿ç”¨RESIZEæ¨¡å¼ï¼Œç„¶åæ‰‹åŠ¨å¤„ç†paddingå’Œç¼©æ”¾ï¼‰
    # num_top_queries åº”è¯¥ä½¿ç”¨é…ç½®ä¸­çš„ num_queriesï¼Œå› ä¸ºæ¨¡å‹åªç”Ÿæˆäº†è¿™ä¹ˆå¤šæŸ¥è¯¢
    postprocessor = DetDETRPostProcessor(
        num_classes=8,
        use_focal_loss=True,
        num_top_queries=num_queries,  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ num_queries
        box_process_format=BoxProcessFormat.RESIZE
    )
    
    return model, postprocessor


def preprocess_image(image_path: str, target_size: int = 1280):
    """
    é¢„å¤„ç†å›¾åƒ - é€‚é… Phase 2 é«˜æ¸…çŸ©å½¢æ¨ç†
    target_size: è¿™é‡ŒæŒ‡ max_size (é•¿è¾¹é™åˆ¶)ï¼Œå»ºè®®è®¾ä¸º 1280
    """
    # 1. è¯»å–å›¾åƒ
    image_bgr = cv2.imread(str(image_path))
    if image_bgr is None:
        raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
    
    # BGR -> RGB
    image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    orig_h, orig_w = image.shape[:2]
    
    # 2. æ™ºèƒ½ç¼©æ”¾ (Rectangular Resize: short=720, max=1280)
    # é€»è¾‘ï¼šè®¡ç®—ç¼©æ”¾æ¯”ä¾‹
    # å°è¯•å°†çŸ­è¾¹ç¼©æ”¾åˆ° 720
    scale = 720 / min(orig_h, orig_w)
    # å¦‚æœé•¿è¾¹è¶…è¿‡ 1280ï¼Œåˆ™æŒ‰é•¿è¾¹ç¼©æ”¾åˆ° 1280
    if max(orig_h, orig_w) * scale > 1280:
        scale = 1280 / max(orig_h, orig_w)
    
    new_w = int(orig_w * scale)
    new_h = int(orig_h * scale)
    
    # ä½¿ç”¨ Bilinear æ’å€¼ç¼©æ”¾
    image_tensor = torch.from_numpy(image).float().permute(2, 0, 1) # HWC->CHW
    image_tensor = torch.nn.functional.interpolate(
        image_tensor.unsqueeze(0), 
        size=(new_h, new_w), 
        mode='bilinear', align_corners=False
    ).squeeze(0)
    
    # 3. å½’ä¸€åŒ– (Normalize) - å…³é”®ä¿®æ­£ï¼
    image_tensor = image_tensor / 255.0  # [0, 255] -> [0, 1]
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
    image_tensor = (image_tensor - mean) / std
    
    # 4. 32å€æ•°å¯¹é½ + å·¦ä¸Šè§’å¡«å…… (Top-Left Padding)
    stride = 32
    # è®¡ç®— padding åçš„å°ºå¯¸
    padded_h = (new_h + stride - 1) // stride * stride
    padded_w = (new_w + stride - 1) // stride * stride
    
    pad_h = padded_h - new_h
    pad_w = padded_w - new_w
    
    # åˆ›å»ºç”»å¸ƒ (å¡«å…… 0)
    padded_image = torch.zeros(3, padded_h, padded_w, dtype=torch.float32)
    padded_image[:, :new_h, :new_w] = image_tensor  # ğŸ‘ˆ å·¦ä¸Šè§’å¯¹é½ï¼
    
    # æ·»åŠ  Batch ç»´åº¦
    img_input = padded_image.unsqueeze(0) # [1, 3, H, W]
    
    # æ„å»º Meta ä¿¡æ¯ (ç”¨äºè¿˜åŸåæ ‡)
    meta = {
        'orig_size': torch.tensor([[orig_h, orig_w]]),
        'new_h': new_h,
        'new_w': new_w,
        'pad_h': 0,       # å·¦ä¸Šè§’å¯¹é½ï¼ŒTop padding = 0
        'pad_w': 0,       # å·¦ä¸Šè§’å¯¹é½ï¼ŒLeft padding = 0
        'scale_h': new_h / orig_h,
        'scale_w': new_w / orig_w,
        'padded_h': padded_h, # è®°å½•ç½‘ç»œå®é™…è¾“å…¥å°ºå¯¸
        'padded_w': padded_w
    }
    
    return img_input, image_bgr, meta


def postprocess_outputs(outputs, postprocessor, meta, conf_threshold=0.3, target_size=None, device='cuda', verbose=False):
    """åå¤„ç†æ¨¡å‹è¾“å‡º"""
    # è·å–æ¨¡å‹è¾“å‡ºçš„è®¾å¤‡
    if isinstance(outputs, dict) and 'pred_logits' in outputs:
        output_device = outputs['pred_logits'].device
    else:
        output_device = torch.device(device)
    
    # 1. è®© PostProcessor åœ¨ "Padding åçš„ç”»å¸ƒ" ä¸Šè¾“å‡ºç»å¯¹åæ ‡
    # æˆ‘ä»¬ä¼ å…¥å®é™…è¾“å…¥ç½‘ç»œçš„å°ºå¯¸ (padded_w, padded_h)
    target_sizes = torch.tensor([[meta['padded_h'], meta['padded_w']]], device=output_device)
    
    # DetDETRPostProcessor é»˜è®¤ä½¿ç”¨ orig_target_sizes å°† 0-1 æ˜ å°„å›åƒç´ 
    # è¿™é‡Œæˆ‘ä»¬è¦å®ƒæ˜ å°„å› "padded_image" çš„åƒç´ åæ ‡
    # æ³¨æ„ï¼šå¿…é¡»ä½¿ç”¨å…³é”®å­—å‚æ•° orig_target_sizesï¼Œå› ä¸º DetDETRPostProcessor.forward åªæ¥å— outputs ä½œä¸ºä½ç½®å‚æ•°
    results = postprocessor(outputs, orig_target_sizes=target_sizes) 
    result = results[0]
    
    # 2. æå–ç»“æœ
    labels = result['labels'].cpu().numpy()
    boxes = result['boxes'].cpu().numpy() # [x1, y1, x2, y2] åœ¨ padded å›¾ä¸Šçš„åæ ‡
    scores = result['scores'].cpu().numpy()
    
    # 3. åæ ‡æ˜ å°„å›åŸå›¾ (Map back to original image)
    # å› ä¸ºæ˜¯å·¦ä¸Šè§’å¯¹é½ï¼Œæ‰€ä»¥ x_orig = x_padded / scale
    scale_w = meta['scale_w']
    scale_h = meta['scale_h']
    
    boxes[:, 0] /= scale_w
    boxes[:, 2] /= scale_w
    boxes[:, 1] /= scale_h
    boxes[:, 3] /= scale_h
    
    # 4. è£å‰ªè¾¹ç•Œ (é˜²æ­¢è¶…å‡ºåŸå›¾)
    orig_h, orig_w = meta['orig_size'][0].tolist()
    boxes[:, 0] = np.clip(boxes[:, 0], 0, orig_w)
    boxes[:, 2] = np.clip(boxes[:, 2], 0, orig_w)
    boxes[:, 1] = np.clip(boxes[:, 1], 0, orig_h)
    boxes[:, 3] = np.clip(boxes[:, 3], 0, orig_h)
    
    # 5. è¿‡æ»¤ä½ç½®ä¿¡åº¦
    mask = scores >= conf_threshold
    labels = labels[mask]
    boxes = boxes[mask]
    scores = scores[mask]
    
    # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥åæ ‡èŒƒå›´å’Œç½®ä¿¡åº¦ï¼ˆä»…åœ¨verboseæ¨¡å¼ä¸‹æ‰“å°ï¼‰
    if verbose and len(boxes) > 0:
        print(f"  æ£€æµ‹åˆ° {len(boxes)} ä¸ªå€™é€‰æ¡†")
        print(f"  ç½®ä¿¡åº¦èŒƒå›´: [{scores.min():.4f}, {scores.max():.4f}], é˜ˆå€¼: {conf_threshold:.4f}")
    
    return labels, boxes, scores


def draw_boxes(image, labels, boxes, scores, class_names=None, colors=None):
    """åœ¨å›¾åƒä¸Šç»˜åˆ¶é¢„æµ‹æ¡†
    
    Args:
        image: BGRæ ¼å¼çš„å›¾åƒï¼ˆnumpyæ•°ç»„ï¼‰
        labels: ç±»åˆ«æ ‡ç­¾æ•°ç»„
        boxes: è¾¹ç•Œæ¡†æ•°ç»„ [N, 4] (x1, y1, x2, y2)
        scores: ç½®ä¿¡åº¦æ•°ç»„
        class_names: ç±»åˆ«åç§°åˆ—è¡¨ï¼ˆé»˜è®¤ä½¿ç”¨å…¨å±€CLASS_NAMESï¼‰
        colors: é¢œè‰²åˆ—è¡¨ï¼ˆé»˜è®¤ä½¿ç”¨å…¨å±€COLORSï¼‰
    
    Returns:
        ç»˜åˆ¶äº†æ£€æµ‹æ¡†çš„å›¾åƒ
    """
    if len(labels) == 0:
        return image
    
    if class_names is None:
        class_names = CLASS_NAMES
    if colors is None:
        colors = COLORS
    
    for label, box, score in zip(labels, boxes, scores):
        x1, y1, x2, y2 = box.astype(int)
        
        # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
        x1 = max(0, min(x1, image.shape[1] - 1))
        y1 = max(0, min(y1, image.shape[0] - 1))
        x2 = max(0, min(x2, image.shape[1] - 1))
        y2 = max(0, min(y2, image.shape[0] - 1))
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        color = colors[label]
        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
        
        # ç»˜åˆ¶æ ‡ç­¾å’Œç½®ä¿¡åº¦
        class_name = class_names[label]
        label_text = f"{class_name}: {score:.2f}"
        
        # è®¡ç®—æ–‡æœ¬ä½ç½®
        (text_w, text_h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        text_y = max(text_h + 4, y1)
        cv2.rectangle(image, (x1, text_y - text_h - 4), (x1 + text_w, text_y), color, -1)
        cv2.putText(image, label_text, (x1, text_y - 2), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    return image


def inference_from_preprocessed_image(image_tensor, model, postprocessor, orig_image_path, 
                                     conf_threshold=0.3, target_size=640, device='cuda', 
                                     class_names=None, colors=None, verbose=False):
    """ä»å·²é¢„å¤„ç†çš„å›¾åƒtensorè¿›è¡Œæ¨ç†ï¼ˆç”¨äºè®­ç»ƒæ—¶ï¼‰
    
    Args:
        image_tensor: å·²é¢„å¤„ç†çš„å›¾åƒtensor [1, 3, H, W]
        model: æ¨¡å‹
        postprocessor: åå¤„ç†å™¨
        orig_image_path: åŸå§‹å›¾åƒè·¯å¾„ï¼ˆç”¨äºè¯»å–åŸå§‹å›¾åƒå’Œæ„å»ºmetaï¼‰
        conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        target_size: ç›®æ ‡å°ºå¯¸
        device: è®¾å¤‡
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        colors: é¢œè‰²åˆ—è¡¨
        verbose: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯
    
    Returns:
        result_image: ç»˜åˆ¶äº†æ£€æµ‹æ¡†çš„å›¾åƒï¼ˆBGRæ ¼å¼ï¼‰ï¼Œå¦‚æœæ²¡æœ‰æ£€æµ‹ç»“æœåˆ™è¿”å›None
    """
    # è¯»å–åŸå§‹å›¾åƒ
    orig_image_bgr = cv2.imread(str(orig_image_path))
    if orig_image_bgr is None:
        return None
    
    orig_h, orig_w = orig_image_bgr.shape[:2]
    
    # æ„å»ºmetaå­—å…¸ï¼ˆä¸preprocess_imageè¿”å›æ ¼å¼ä¸€è‡´ï¼‰
    scale = min(target_size / orig_h, target_size / orig_w)
    new_h = int(orig_h * scale)
    new_w = int(orig_w * scale)
    new_h = ((new_h + 31) // 32) * 32
    new_w = ((new_w + 31) // 32) * 32
    new_h = min(new_h, target_size)
    new_w = min(new_w, target_size)
    pad_h = (target_size - new_h) // 2
    pad_w = (target_size - new_w) // 2
    
    # âš ï¸ å…³é”®ä¿®å¤ï¼šè®¡ç®—å®é™…ç¼©æ”¾æ¯”ä¾‹ï¼ˆè€ƒè™‘32å€æ•°è°ƒæ•´ï¼‰
    actual_scale_h = new_h / orig_h
    actual_scale_w = new_w / orig_w
    
    meta = {
        'orig_size': torch.tensor([[orig_h, orig_w]]),  # [1, 2] format: [h, w]
        'pad_h': pad_h,
        'pad_w': pad_w,
        'scale': scale,  # ä¿ç•™åŸå§‹scaleç”¨äºå…¼å®¹æ€§
        'scale_h': actual_scale_h,  # å®é™…é«˜åº¦ç¼©æ”¾æ¯”ä¾‹
        'scale_w': actual_scale_w,  # å®é™…å®½åº¦ç¼©æ”¾æ¯”ä¾‹
        'new_h': new_h,
        'new_w': new_w
    }
    
    # æ¨ç†
    with torch.no_grad():
        outputs = model(image_tensor)
    
    # åå¤„ç†
    labels, boxes, scores = postprocess_outputs(
        outputs, postprocessor, meta, conf_threshold, target_size, device, verbose=verbose
    )
    
    if len(labels) == 0:
        return None
    
    # ç»˜åˆ¶ç»“æœ
    result_image = draw_boxes(orig_image_bgr.copy(), labels, boxes, scores, class_names, colors)
    return result_image


def process_single_image(image_path: Path, model, postprocessor, output_dir: Path, 
                        conf_threshold: float, device: str, target_size: int = 1280):
    """å¤„ç†å•å¼ å›¾åƒ"""
    try:
        # é¢„å¤„ç†å›¾åƒ
        img_tensor, orig_image, meta = preprocess_image(str(image_path), target_size)
        img_tensor = img_tensor.to(device)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model(img_tensor)
        
        # åå¤„ç†ï¼ˆverbose=True ç”¨äºæ‰¹é‡æ¨ç†æ—¶æ˜¾ç¤ºä¿¡æ¯ï¼‰
        labels, boxes, scores = postprocess_outputs(
            outputs, postprocessor, meta, conf_threshold, target_size, device, verbose=True
        )
        
        # ç»˜åˆ¶ç»“æœ
        result_image = draw_boxes(orig_image.copy(), labels, boxes, scores)
        
        # ä¿å­˜ç»“æœ
        output_path = output_dir / image_path.name
        cv2.imwrite(str(output_path), result_image)
        
        return len(labels), True, None
    except Exception as e:
        return 0, False, str(e)


def batch_inference(image_dir: str, config_path: str, checkpoint_path: str, 
                   output_dir: str = None, conf_threshold: float = 0.3, 
                   device: str = "cuda", max_images: int = None,
                   target_size: int = 1280,
                   image_extensions: tuple = ('.jpg', '.jpeg', '.png', '.bmp')):
    """æ‰¹é‡æ¨ç†"""
    # åŠ è½½æ¨¡å‹
    print(f"åŠ è½½æ¨¡å‹: {checkpoint_path}")
    model, postprocessor = load_model(config_path, checkpoint_path, device)
    print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # è®¾ç½®è¾“å…¥å’Œè¾“å‡ºç›®å½•
    image_dir = Path(image_dir)
    if not image_dir.exists():
        raise ValueError(f"å›¾åƒç›®å½•ä¸å­˜åœ¨: {image_dir}")
    
    if output_dir is None:
        output_dir = image_dir.parent / f"{image_dir.name}_results"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æ¨ç†å°ºå¯¸ (Max Size): {target_size}")
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    image_files = []
    for ext in image_extensions:
        image_files.extend(list(image_dir.glob(f"*{ext}")))
        image_files.extend(list(image_dir.glob(f"*{ext.upper()}")))
    
    if len(image_files) == 0:
        print(f"è­¦å‘Š: åœ¨ {image_dir} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
        return
    
    # é™åˆ¶å¤„ç†æ•°é‡
    total_images = len(image_files)
    if max_images is not None and max_images > 0:
        image_files = image_files[:max_images]
        print(f"æ‰¾åˆ° {total_images} å¼ å›¾åƒï¼Œå°†å¤„ç†å‰ {len(image_files)} å¼ ")
    else:
        print(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
    
    # æ‰¹é‡å¤„ç†
    total_detections = 0
    success_count = 0
    failed_images = []
    
    for image_path in tqdm(image_files, desc="å¤„ç†å›¾åƒ"):
        num_detections, success, error = process_single_image(
            image_path, model, postprocessor, output_dir, 
            conf_threshold, device, target_size
        )
        
        if success:
            total_detections += num_detections
            success_count += 1
        else:
            failed_images.append((image_path.name, error))
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*50)
    print("å¤„ç†å®Œæˆ!")
    print(f"æˆåŠŸå¤„ç†: {success_count}/{len(image_files)} å¼ å›¾åƒ")
    print(f"æ€»æ£€æµ‹æ•°: {total_detections} ä¸ªç›®æ ‡")
    if failed_images:
        print(f"å¤±è´¥: {len(failed_images)} å¼ å›¾åƒ")
        for img_name, error in failed_images[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
            print(f"  - {img_name}: {error}")
        if len(failed_images) > 5:
            print(f"  ... è¿˜æœ‰ {len(failed_images) - 5} ä¸ªé”™è¯¯")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("="*50)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="RT-DETR æ‰¹é‡æ¨ç†è„šæœ¬")
    parser.add_argument("--image_dir", type=str, required=True, 
                       help="è¾“å…¥å›¾åƒç›®å½•è·¯å¾„")
    parser.add_argument("--config", type=str, required=True, 
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", type=str, required=True, 
                       help="æ¨¡å‹checkpointè·¯å¾„")
    parser.add_argument("--output_dir", type=str, default=None, 
                       help="è¾“å‡ºå›¾åƒç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ï¼šè¾“å…¥ç›®å½•_resultsï¼‰")
    parser.add_argument("--conf", type=float, default=0.3, 
                       help="ç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--device", type=str, default="cuda", 
                       help="è®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--max_images", type=int, default=None,
                       help="æœ€å¤§å¤„ç†å›¾åƒæ•°é‡ï¼ˆé»˜è®¤ï¼šå¤„ç†æ‰€æœ‰å›¾åƒï¼‰")
    parser.add_argument("--target_size", type=int, default=1280,
                       help="æ¨ç†å›¾åƒå°ºå¯¸ï¼ˆé•¿è¾¹é™åˆ¶ï¼Œé»˜è®¤1280ï¼‰")
    
    args = parser.parse_args()
    
    batch_inference(
        args.image_dir,
        args.config,
        args.checkpoint,
        args.output_dir,
        args.conf,
        args.device,
        args.max_images,
        args.target_size
    )

