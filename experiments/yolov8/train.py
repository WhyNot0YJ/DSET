#!/usr/bin/env python3
"""
YOLOv8è®­ç»ƒè„šæœ¬ - æ”¯æŒDAIR-V2Xæ•°æ®é›†
"""

import sys
import os
import argparse
import yaml
import torch
import logging
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.resolve()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
if str(project_root.parent) not in sys.path:
    sys.path.insert(0, str(project_root.parent))

# å¯¼å…¥ultralyticsï¼ˆæœ¬åœ°å‰¯æœ¬ï¼‰
from ultralytics import YOLO

# DAIR-V2Xç±»åˆ«å®šä¹‰ï¼ˆ10ç±»ï¼‰
CLASS_NAMES = [
    "Car", "Truck", "Van", "Bus", "Pedestrian", 
    "Cyclist", "Tricyclist", "Motorcyclist", "Barrowlist", "Trafficcone"
]


class YOLOv8Trainer:
    """YOLOv8è®­ç»ƒå™¨ - é€‚é…DAIR-V2Xæ•°æ®é›†"""
    
    def __init__(self, config: Dict, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config: é…ç½®å­—å…¸
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºä¿å­˜ï¼‰
        """
        self.config = config
        self.config_path = config_path
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # éªŒè¯é…ç½®
        self._validate_config()
        
        # è·å–é…ç½®å‚æ•°
        self.model_config = config.get('model', {})
        self.training_config = config.get('training', {})
        self.data_config = config.get('data', {})
        self.checkpoint_config = config.get('checkpoint', {})
        self.misc_config = config.get('misc', {})
        
        # ç±»åˆ«ä¿¡æ¯
        self.class_names = CLASS_NAMES
        self.num_classes = len(CLASS_NAMES)
        
        self.logger.info(f"âœ“ åˆå§‹åŒ–YOLOv8è®­ç»ƒå™¨")
        self.logger.info(f"  ç±»åˆ«æ•°é‡: {self.num_classes}")
        self.logger.info(f"  ç±»åˆ«: {', '.join(self.class_names)}")
    
    def _validate_config(self):
        """éªŒè¯é…ç½®æ–‡ä»¶"""
        required_keys = {
            'model': ['model_name'],
            'training': ['epochs', 'batch_size'],
            'data': ['data_yaml']
        }
        
        missing_keys = []
        for section, keys in required_keys.items():
            if section not in self.config:
                missing_keys.append(f"ç¼ºå°‘é…ç½®èŠ‚: {section}")
                continue
            for key in keys:
                if key not in self.config[section]:
                    missing_keys.append(f"{section}.{key}")
        
        if missing_keys:
            error_msg = f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹:\n"
            error_msg += "\n".join(f"  - {key}" for key in missing_keys)
            raise ValueError(error_msg)
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # æ£€æŸ¥æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        resume_checkpoint = getattr(self, '_resume_checkpoint_path', None)
        
        if resume_checkpoint and Path(resume_checkpoint).exists():
            # æ¢å¤è®­ç»ƒï¼šä½¿ç”¨æ£€æŸ¥ç‚¹æ‰€åœ¨ç›®å½•
            self.log_dir = Path(resume_checkpoint).parent
            self.experiment_name = self.log_dir.name
        else:
            # æ–°è®­ç»ƒï¼šåˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ç›®å½•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = self.config.get('model', {}).get('model_name', 'yolov8n')
            self.experiment_name = f"yolo_{model_name.replace('yolov8', 'v8').replace('yolo11', 'v11')}"
            log_base = self.checkpoint_config.get('log_dir', 'logs')
            self.log_dir = Path(f"{log_base}/{self.experiment_name}_{timestamp}")
            self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®æ—¥å¿—å¤„ç†å™¨
        handlers = [
            logging.FileHandler(self.log_dir / 'training.log', mode='a'),
            logging.StreamHandler()
        ]
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=handlers,
            force=True
        )
        
        self.logger = logging.getLogger(__name__)
        
        # ä¿å­˜é…ç½®æ–‡ä»¶ï¼ˆä»…æ–°è®­ç»ƒæ—¶ï¼‰
        if not resume_checkpoint:
            config_save_path = self.log_dir / 'config.yaml'
            with open(config_save_path, 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
            self.logger.info(f"âœ“ é…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")
    
    def create_model(self):
        """åˆ›å»ºYOLOæ¨¡å‹"""
        model_name = self.model_config.get('model_name', 'yolov8n.pt')
        pretrained_weights = self.model_config.get('pretrained_weights', None)
        
        # å¦‚æœæŒ‡å®šäº†é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨æ¨¡å‹åç§°
        if pretrained_weights and Path(pretrained_weights).exists():
            self.logger.info(f"âœ“ åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_weights}")
            model = YOLO(pretrained_weights)
        else:
            self.logger.info(f"âœ“ åˆ›å»ºæ¨¡å‹: {model_name}")
            model = YOLO(model_name)
        
        # YOLOæ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨ä»data.yamlè¯»å–ç±»åˆ«æ•°å¹¶è°ƒæ•´æ¨¡å‹
        # è¿™é‡Œæˆ‘ä»¬åªéœ€è¦ç¡®ä¿data.yamlä¸­çš„ç±»åˆ«æ•°æ­£ç¡®å³å¯
        # YOLOçš„train()æ–¹æ³•ä¼šè‡ªåŠ¨å¤„ç†ç±»åˆ«æ•°çš„ä¿®æ”¹
        self.logger.info(f"  æ¨¡å‹å°†åœ¨è®­ç»ƒæ—¶è‡ªåŠ¨é€‚é… {self.num_classes} ç±»ï¼ˆä»data.yamlè¯»å–ï¼‰")
        
        return model
    
    def start_training(self, resume_checkpoint: Optional[str] = None):
        """å¼€å§‹è®­ç»ƒ"""
        self._resume_checkpoint_path = resume_checkpoint
        
        # è®¾ç½®æ—¥å¿—ï¼ˆéœ€è¦åœ¨è®¾ç½®resume_checkpointä¹‹åï¼‰
        self.setup_logging()
        
        self.logger.info("="*60)
        self.logger.info("ğŸš€ å¼€å§‹YOLOv8è®­ç»ƒ")
        self.logger.info("="*60)
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model()
        
        # è·å–è®­ç»ƒå‚æ•°
        epochs = self.training_config.get('epochs', 100)
        batch_size = self.training_config.get('batch_size', 16)
        imgsz = self.training_config.get('imgsz', 640)
        device = self.misc_config.get('device', 'cuda')
        workers = self.misc_config.get('num_workers', 8)
        
        # æ•°æ®é…ç½®
        data_yaml = self.data_config.get('data_yaml')
        if not Path(data_yaml).exists():
            raise FileNotFoundError(f"æ•°æ®é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {data_yaml}")
        
        # è®­ç»ƒå‚æ•°
        train_kwargs = {
            'data': str(data_yaml),
            'epochs': epochs,
            'batch': batch_size,
            'imgsz': imgsz,
            'device': device,
            'workers': workers,
            'project': str(self.log_dir.parent),
            'name': self.experiment_name,
            'exist_ok': True,
            'plots': True,
            'save': True,
            'save_period': self.training_config.get('save_period', 10),
            'val': True,
        }
        
        # å­¦ä¹ ç‡é…ç½®
        if 'lr0' in self.training_config:
            train_kwargs['lr0'] = self.training_config['lr0']
        if 'lrf' in self.training_config:
            train_kwargs['lrf'] = self.training_config['lrf']
        if 'momentum' in self.training_config:
            train_kwargs['momentum'] = self.training_config['momentum']
        if 'weight_decay' in self.training_config:
            train_kwargs['weight_decay'] = self.training_config['weight_decay']
        if 'warmup_epochs' in self.training_config:
            train_kwargs['warmup_epochs'] = self.training_config['warmup_epochs']
        if 'warmup_momentum' in self.training_config:
            train_kwargs['warmup_momentum'] = self.training_config['warmup_momentum']
        if 'warmup_bias_lr' in self.training_config:
            train_kwargs['warmup_bias_lr'] = self.training_config['warmup_bias_lr']
        
        # æ•°æ®å¢å¼ºé…ç½®
        if 'hsv_h' in self.training_config:
            train_kwargs['hsv_h'] = self.training_config['hsv_h']
        if 'hsv_s' in self.training_config:
            train_kwargs['hsv_s'] = self.training_config['hsv_s']
        if 'hsv_v' in self.training_config:
            train_kwargs['hsv_v'] = self.training_config['hsv_v']
        if 'degrees' in self.training_config:
            train_kwargs['degrees'] = self.training_config['degrees']
        if 'translate' in self.training_config:
            train_kwargs['translate'] = self.training_config['translate']
        if 'scale' in self.training_config:
            train_kwargs['scale'] = self.training_config['scale']
        if 'flipud' in self.training_config:
            train_kwargs['flipud'] = self.training_config['flipud']
        if 'fliplr' in self.training_config:
            train_kwargs['fliplr'] = self.training_config['fliplr']
        if 'mosaic' in self.training_config:
            train_kwargs['mosaic'] = self.training_config['mosaic']
        if 'mixup' in self.training_config:
            train_kwargs['mixup'] = self.training_config['mixup']
        
        # æ¢å¤è®­ç»ƒ
        if resume_checkpoint and Path(resume_checkpoint).exists():
            self.logger.info(f"ğŸ“¦ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_checkpoint}")
            train_kwargs['resume'] = True
            # YOLOçš„resumeå‚æ•°å¯ä»¥æ˜¯Trueæˆ–æ£€æŸ¥ç‚¹è·¯å¾„
            if Path(resume_checkpoint).is_file():
                train_kwargs['resume'] = str(resume_checkpoint)
        
        self.logger.info(f"è®­ç»ƒå‚æ•°:")
        self.logger.info(f"  æ•°æ®é…ç½®: {data_yaml}")
        self.logger.info(f"  è®­ç»ƒè½®æ•°: {epochs}")
        self.logger.info(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        self.logger.info(f"  å›¾åƒå°ºå¯¸: {imgsz}")
        self.logger.info(f"  è®¾å¤‡: {device}")
        self.logger.info(f"  å·¥ä½œè¿›ç¨‹: {workers}")
        self.logger.info(f"  æ—¥å¿—ç›®å½•: {self.log_dir}")
        
        # å¼€å§‹è®­ç»ƒ
        try:
            results = model.train(**train_kwargs)
            self.logger.info("="*60)
            self.logger.info("âœ… è®­ç»ƒå®Œæˆï¼")
            self.logger.info("="*60)
            
            # æ‰“å°æœ€ä½³æ¨¡å‹è·¯å¾„
            best_model_path = self.log_dir / "weights" / "best.pt"
            if best_model_path.exists():
                self.logger.info(f"æœ€ä½³æ¨¡å‹: {best_model_path}")
            
            return results
        except Exception as e:
            self.logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='YOLOv8è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, required=True,
                       help='YAMLé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume_from_checkpoint', type=str, default=None,
                       help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼‰')
    parser.add_argument('--resume', action='store_true',
                       help='è‡ªåŠ¨ä»æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    if not Path(args.config).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
    
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å¦‚æœå¯ç”¨è‡ªåŠ¨æ¢å¤ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
    if args.resume and not args.resume_from_checkpoint:
        log_base = config.get('checkpoint', {}).get('log_dir', 'logs')
        log_dir = Path(log_base)
        if log_dir.exists():
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«weights/best.ptçš„ç›®å½•
            checkpoints = list(log_dir.glob("*/weights/best.pt"))
            if checkpoints:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
                latest_checkpoint = max(checkpoints, key=lambda p: p.stat().st_mtime)
                args.resume_from_checkpoint = str(latest_checkpoint)
                print(f"ğŸ“¦ æ‰¾åˆ°æœ€æ–°æ£€æŸ¥ç‚¹: {args.resume_from_checkpoint}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = YOLOv8Trainer(config, config_path=args.config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.start_training(resume_checkpoint=args.resume_from_checkpoint)


if __name__ == '__main__':
    main()

