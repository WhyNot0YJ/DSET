import os
import sys
import subprocess
import argparse
import torch

# Fix PyTorch 2.6+ weights_only issue for MMEngine checkpoint loading
# MMEngine checkpoints contain custom classes and numpy arrays that need to be whitelisted
def patch_torch_load_for_mmengine():
    """Patch torch.load to use weights_only=False for MMEngine compatibility"""
    import torch
    _original_torch_load = torch.load
    def _patched_torch_load(*args, **kwargs):
        if 'weights_only' not in kwargs:
            kwargs['weights_only'] = False
        return _original_torch_load(*args, **kwargs)
    torch.load = _patched_torch_load
    return True

try:
    # Method 1: Add safe globals for known classes
    from mmengine.logging.history_buffer import HistoryBuffer
    import numpy as np
    torch.serialization.add_safe_globals([HistoryBuffer])
    # Try to add numpy reconstruct function
    try:
        import numpy._core.multiarray as numpy_multiarray
        torch.serialization.add_safe_globals([numpy_multiarray._reconstruct])
    except (ImportError, AttributeError):
        pass
    try:
        torch.serialization.add_safe_globals([np.ndarray, np.dtype])
    except (TypeError, AttributeError):
        pass
except (ImportError, AttributeError):
    pass

# Method 2: Patch torch.load as fallback (more reliable)
patch_torch_load_for_mmengine()
print("âœ“ å·²ä¿®è¡¥ torch.load ä»¥å…¼å®¹ MMEngine checkpoint (weights_only=False)")

# Auto-Installation Block: Checks and installs openmim, mmengine, mmcv, mmdet if not present
def check_and_install_dependencies():
    try:
        import mmengine
        import mmdet
        print("Dependencies found.")
    except ImportError:
        print("Dependencies missing. Installing...")
        try:
            import mim
        except ImportError:
            print("Installing openmim...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-U", "openmim"])
            import mim

        # Set CUDA arch to 9.0 for RTX 5090 (CUDA 12.1 compatible, prevents compute_100 error)
        os.environ["TORCH_CUDA_ARCH_LIST"] = "9.0"
        print("âœ“ Set TORCH_CUDA_ARCH_LIST=9.0 for RTX 5090 (CUDA 12.1 compatible)")

        print("Installing mmengine, mmcv, mmdet via mim (Force Arch 9.0)...")
        subprocess.check_call(["mim", "install", "mmengine", "mmcv>=2.0.0", "mmdet>=3.0.0", "-v"])

# Check dependencies before main imports if running in a fresh environment
check_and_install_dependencies()


from mmengine.config import Config
from mmengine.runner import Runner

def setup_gpu_optimizations():
    """Configure GPU optimizations for RTX 5090 with CUDA 12.8"""
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        print(f"âœ“ GPU ä¼˜åŒ–å·²å¯ç”¨:")
        print(f"  - Device: {torch.cuda.get_device_name(0)}")
        print(f"  - CUDA Version: {torch.version.cuda}")
        print(f"  - PyTorch Version: {torch.__version__}")
        print(f"  - cudnn.benchmark: {torch.backends.cudnn.benchmark}")
        print(f"  - TF32 (matmul): {torch.backends.cuda.matmul.allow_tf32}")
        print(f"  - TF32 (cudnn): {torch.backends.cudnn.allow_tf32}")
    else:
        print("âš  GPU ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")

def main():
    # Parse Arguments
    parser = argparse.ArgumentParser(description='Deformable DETR R18 Training')
    parser.add_argument('--epochs', type=int, default=None,
                        help='Override max_epochs (for test mode, use --epochs 2)')
    parser.add_argument('--data_root', type=str, default=None,
                        help='Override data root path (default: /root/autodl-tmp/datasets/DAIR-V2X/)')
    parser.add_argument('--work_dir', type=str, default=None,
                        help='Override work directory')
    parser.add_argument('--resume', type=str, default=None,
                        help='Resume training from checkpoint (path to checkpoint file, or "auto" to resume from latest)')
    parser.add_argument('--disable_early_stop', action='store_true',
                        help='Disable early stopping (useful when resuming training)')
    parser.add_argument('--early_stop_patience', type=int, default=None,
                        help='Override early stopping patience (default: 20)')
    args = parser.parse_args()
    setup_gpu_optimizations()
    
    # Load Base Config
    try:
        config_path = '/root/miniconda3/lib/python3.10/site-packages/mmdet/.mim/configs/deformable_detr/deformable-detr_r50_16xb2-50e_coco.py'
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Config file not found at: {config_path}")
        cfg = Config.fromfile(config_path)
    except Exception as e:
        print(f"Error loading config: {e}")
        return

    # Model Modifications: R50 -> R18
    cfg.model.backbone.depth = 18
    cfg.model.backbone.init_cfg = dict(type='Pretrained', checkpoint='torchvision://resnet18')
    # ResNet-18 outputs [64, 128, 256, 512], use last 3 stages -> [128, 256, 512]
    cfg.model.neck.in_channels = [128, 256, 512]
    cfg.model.bbox_head.num_classes = 8
    # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šå¼ºåˆ¶ä½¿ç”¨ 100 Queriesï¼ˆä¸ RT-DETR å¯¹é½ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼‰
    cfg.model.num_queries = 100

    # Dataset Configuration
    if args.data_root:
        data_root = args.data_root
    else:
        possible_paths = [
            '/root/autodl-tmp/datasets/DAIR-V2X/',
            '/home/yujie/proj/task-selective-det/data/DAIR-V2X/',
            os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data/DAIR-V2X/'),
        ]
        data_root = None
        for path in possible_paths:
            if os.path.exists(path) and os.path.exists(os.path.join(path, 'annotations')):
                data_root = path
                break
        if data_root is None:
            data_root = '/root/autodl-tmp/datasets/DAIR-V2X/'
            print(f"âš  Warning: Using default data root: {data_root}")
            print(f"   If data is elsewhere, use --data_root to specify")
    
    cfg.data_root = data_root
    print(f"âœ“ Data root: {data_root}")

    class_names = ('Car', 'Truck', 'Van', 'Bus', 'Pedestrian', 'Cyclist', 'Motorcyclist', 'Trafficcone')
    metainfo = dict(classes=class_names)

    # Data Augmentation Pipeline (aligned with DSET)
    train_pipeline = [
        dict(type='LoadImageFromFile', backend_args=None),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(type='PhotoMetricDistortion'),  
        dict(type='MinIoURandomCrop', min_crop_size=0.1),  
        dict(type='RandomFlip', prob=0.5),   
        dict(
            type='RandomChoiceResize',
            scales=[(1280, x) for x in range(480, 801, 32)],
            keep_ratio=True),  
        dict(type='PackDetInputs')
    ]

    test_pipeline = [
        dict(type='LoadImageFromFile', backend_args=None),
        dict(type='Resize', scale=(1280, 720), keep_ratio=True),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(
            type='PackDetInputs',
            meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                       'scale_factor'))
    ]

    # Train Dataloader
    cfg.train_dataloader.dataset.data_root = data_root
    cfg.train_dataloader.dataset.ann_file = 'annotations/instances_train.json'
    cfg.train_dataloader.dataset.data_prefix = dict(img='')
    cfg.train_dataloader.dataset.metainfo = metainfo
    cfg.train_dataloader.dataset.pipeline = train_pipeline
    cfg.train_dataloader.batch_size = 8
    cfg.train_dataloader.num_workers = 16

    # Val Dataloader
    cfg.val_dataloader.dataset.data_root = data_root
    cfg.val_dataloader.dataset.ann_file = 'annotations/instances_val.json'
    cfg.val_dataloader.dataset.data_prefix = dict(img='')
    cfg.val_dataloader.dataset.metainfo = metainfo
    cfg.val_dataloader.dataset.pipeline = test_pipeline
    cfg.val_dataloader.batch_size = 16
    cfg.val_dataloader.num_workers = 16

    # Test Dataloader
    cfg.test_dataloader.dataset.data_root = data_root
    cfg.test_dataloader.dataset.ann_file = 'annotations/instances_val.json'
    cfg.test_dataloader.dataset.data_prefix = dict(img='')
    cfg.test_dataloader.dataset.metainfo = metainfo
    cfg.test_dataloader.dataset.pipeline = test_pipeline
    cfg.test_dataloader.batch_size = 16
    cfg.test_dataloader.num_workers = 16

    # Evaluators
    cfg.val_evaluator.ann_file = os.path.join(data_root, 'annotations/instances_val.json')
    cfg.test_evaluator.ann_file = os.path.join(data_root, 'annotations/instances_val.json')
    # ğŸ”¥ å…³é”®ç‚¹ï¼šproposal_nums å†³å®šäº†"è®¡ç®—"å“ªäº› AR
    cfg.val_evaluator.proposal_nums = (1, 10, 100)
    cfg.test_evaluator.proposal_nums = (1, 10, 100)
    # ğŸ”¥ å…³é”®ç‚¹ï¼šmetric_items å†³å®šäº†"åœ¨è¿›åº¦æ¡æœ€åæ‰“å°"å“ªäº› Key
    # è¿™é‡Œä¸èƒ½å†™ ARï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚åˆ æ‰ AR åï¼Œå®Œæ•´çš„ AR æ•°æ®ä¾ç„¶ä¼šåœ¨è¯¦ç»†æ—¥å¿—ä¸­æ‰“å°å‡ºæ¥ã€‚
    cfg.val_evaluator.metric_items = ['mAP', 'mAP_50', 'mAP_75', 'mAP_s', 'mAP_m', 'mAP_l']
    cfg.test_evaluator.metric_items = ['mAP', 'mAP_50', 'mAP_75', 'mAP_s', 'mAP_m', 'mAP_l']

    # Training Schedule
    if args.epochs is not None:
        max_epochs = args.epochs
        print(f"âœ“ Overriding max_epochs to {max_epochs} (from --epochs argument)")
    else:
        max_epochs = 200
    cfg.train_cfg.max_epochs = max_epochs
    
    # LR Scheduler: Linear Warmup (500 iters) + MultiStepLR (decay at epoch 160)
    cfg.param_scheduler = [
        dict(
            type='LinearLR',
            start_factor=0.001,
            by_epoch=False,
            begin=0,
            end=500),
        dict(
            type='MultiStepLR',
            begin=0,
            end=200,
            by_epoch=True,
            milestones=[160],
            gamma=0.1)
    ]
    
    cfg.auto_scale_lr = dict(enable=True, base_batch_size=16)
    
    # Work Directory
    if args.work_dir:
        cfg.work_dir = args.work_dir
    else:
        cfg.work_dir = 'work_dirs/r18_baseline'
    
    # ç¡®ä¿ work_dir æ˜¯ç»å¯¹è·¯å¾„ï¼ˆä¾¿äº checkpoint æŸ¥æ‰¾ï¼‰
    if not os.path.isabs(cfg.work_dir):
        # å¦‚æœ work_dir æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•ä»å½“å‰å·¥ä½œç›®å½•æˆ–è„šæœ¬æ‰€åœ¨ç›®å½•è§£æ
        # é»˜è®¤å‡è®¾åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
        cfg.work_dir = os.path.abspath(cfg.work_dir)
    
    print(f"âœ“ Work directory: {cfg.work_dir}")

    # Enable AMP (Mixed Precision Training)
    cfg.optim_wrapper.type = 'AmpOptimWrapper'
    cfg.optim_wrapper.loss_scale = 'dynamic'
    
    # ==================================================================
    # Early Stopping & Best Model Saving Configuration
    # ==================================================================
    
    # 1. é…ç½® CheckpointHook ä¿å­˜ã€æœ€ä½³æ¨¡å‹ã€‘
    # (å¦‚æœä¸åŠ è¿™ä¸€æ­¥ï¼Œæ—©åœåä½ æ‹¿åˆ°çš„æ˜¯æœ€åä¸€æ¬¡è¿­ä»£çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯åˆ†æ•°æœ€é«˜çš„æ¨¡å‹)
    if 'default_hooks' not in cfg:
        cfg.default_hooks = {}
    
    # ç¡®ä¿ checkpoint hook å­˜åœ¨å¹¶è®¾ç½® save_best
    if hasattr(cfg.default_hooks, 'checkpoint'):
        cfg.default_hooks.checkpoint.save_best = 'coco/bbox_mAP'
        cfg.default_hooks.checkpoint.rule = 'greater'
        cfg.default_hooks.checkpoint.interval = 1  # éªŒè¯é—´éš”
        cfg.default_hooks.checkpoint.max_keep_ckpts = 1  # ğŸ”¥ åªä¿ç•™æœ€æ–° 1 ä¸ªï¼ŒèŠ‚çœç¡¬ç›˜ç©ºé—´
    else:
        cfg.default_hooks.checkpoint = dict(
            type='CheckpointHook', 
            interval=1, 
            save_best='coco/bbox_mAP',
            rule='greater',
            max_keep_ckpts=1  # ğŸ”¥ åªä¿ç•™æœ€æ–° 1 ä¸ªï¼ŒèŠ‚çœç¡¬ç›˜ç©ºé—´
        )
    
    # 2. é…ç½® EarlyStoppingHook (æ”¾åœ¨ custom_hooks ä¸­ï¼Œç¬¦åˆ MMEngine è§„èŒƒ)
    if 'custom_hooks' not in cfg:
        cfg.custom_hooks = []
    
    # Early Stopping é…ç½®ï¼ˆå¯é€šè¿‡å‚æ•°ç¦ç”¨æˆ–è°ƒæ•´ï¼‰
    if not args.disable_early_stop:
        early_stop_patience = args.early_stop_patience if args.early_stop_patience is not None else 20
        cfg.custom_hooks.append(dict(
            type='EarlyStoppingHook',
            monitor='coco/bbox_mAP',  # ç›‘æ§çš„æŒ‡æ ‡
            patience=early_stop_patience,  # å®¹å¿å¤šå°‘ä¸ªepochæ²¡æœ‰æ”¹å–„
            min_delta=0.0001,  # æœ€å°æ”¹å–„é˜ˆå€¼
            rule='greater'  # 'greater'è¡¨ç¤ºè¶Šå¤§è¶Šå¥½ï¼Œ'less'è¡¨ç¤ºè¶Šå°è¶Šå¥½
        ))
        print(f"âœ“ Early Stopping & Save Best: å·²å¯ç”¨ (patience={early_stop_patience}, monitor=coco/bbox_mAP)")
    else:
        print(f"âš  Early Stopping: å·²ç¦ç”¨ (--disable_early_stop)")
    
    # Resume training configuration
    resume_from = None
    if args.resume:
        if args.resume.lower() == 'auto':
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ checkpointï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            import glob
            work_dir = cfg.work_dir
            
            # ä¼˜å…ˆçº§1: æŸ¥æ‰¾ latest.pthï¼ˆMMEngine é€šå¸¸ä¿å­˜è¿™ä¸ªï¼‰
            latest_checkpoint = os.path.join(work_dir, 'latest.pth')
            if os.path.exists(latest_checkpoint):
                resume_from = latest_checkpoint
                # è¯»å– checkpoint æŸ¥çœ‹ epoch ä¿¡æ¯
                try:
                    import torch
                    ckpt = torch.load(latest_checkpoint, map_location='cpu', weights_only=False)
                    epoch = ckpt.get('meta', {}).get('epoch', ckpt.get('epoch', 'unknown'))
                    print(f"ğŸ“¦ è‡ªåŠ¨æ‰¾åˆ° latest checkpoint: {resume_from} (Epoch: {epoch})")
                except:
                    print(f"ğŸ“¦ è‡ªåŠ¨æ‰¾åˆ° latest checkpoint: {resume_from}")
            else:
                # ä¼˜å…ˆçº§2: æŸ¥æ‰¾ epoch_*.pth
                checkpoint_pattern = os.path.join(work_dir, 'epoch_*.pth')
                checkpoints = glob.glob(checkpoint_pattern)
                if checkpoints:
                    # æŒ‰æ–‡ä»¶åä¸­çš„ epoch æ•°å­—æ’åº
                    checkpoints.sort(key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0]))
                    resume_from = checkpoints[-1]
                    try:
                        import torch
                        ckpt = torch.load(resume_from, map_location='cpu', weights_only=False)
                        epoch = ckpt.get('meta', {}).get('epoch', ckpt.get('epoch', 'unknown'))
                        print(f"ğŸ“¦ è‡ªåŠ¨æ‰¾åˆ°æœ€æ–° epoch checkpoint: {resume_from} (Epoch: {epoch})")
                    except:
                        print(f"ğŸ“¦ è‡ªåŠ¨æ‰¾åˆ°æœ€æ–° epoch checkpoint: {resume_from}")
                else:
                    # ä¼˜å…ˆçº§3: æŸ¥æ‰¾ best_*.pth
                    best_pattern = os.path.join(work_dir, 'best_*.pth')
                    best_checkpoints = glob.glob(best_pattern)
                    if best_checkpoints:
                        resume_from = best_checkpoints[0]  # é€šå¸¸åªæœ‰ä¸€ä¸ª best
                        try:
                            import torch
                            ckpt = torch.load(resume_from, map_location='cpu', weights_only=False)
                            epoch = ckpt.get('meta', {}).get('epoch', ckpt.get('epoch', 'unknown'))
                            print(f"ğŸ“¦ è‡ªåŠ¨æ‰¾åˆ° best checkpoint: {resume_from} (Epoch: {epoch})")
                        except:
                            print(f"ğŸ“¦ è‡ªåŠ¨æ‰¾åˆ° best checkpoint: {resume_from}")
                    else:
                        print(f"âš  æœªæ‰¾åˆ° checkpointï¼Œå°†ä» epoch 0 å¼€å§‹è®­ç»ƒ")
                        print(f"   æ£€æŸ¥ç›®å½•: {work_dir}")
                        print(f"   å°è¯•æŸ¥æ‰¾çš„æ–‡ä»¶: latest.pth, epoch_*.pth, best_*.pth")
        else:
            # ä½¿ç”¨æŒ‡å®šçš„ checkpoint è·¯å¾„
            # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„ä½ç½®ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            original_path = args.resume
            possible_paths = []
            
            # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
            if os.path.isabs(original_path):
                possible_paths.append(original_path)
            else:
                # 1. ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
                possible_paths.append(os.path.abspath(original_path))
                # 2. ç›¸å¯¹äº work_dir
                possible_paths.append(os.path.join(cfg.work_dir, original_path))
                # 3. work_dir ä¸‹çš„æ–‡ä»¶åï¼ˆå»æ‰å‰é¢çš„ç›®å½•éƒ¨åˆ†ï¼‰
                possible_paths.append(os.path.join(cfg.work_dir, os.path.basename(original_path)))
                # 4. åŸå§‹è·¯å¾„ï¼ˆä¿æŒåŸæ ·ï¼Œè®©åç»­å¤„ç†ï¼‰
                possible_paths.append(original_path)
            
            resume_from = None
            for path in possible_paths:
                if os.path.exists(path):
                    resume_from = os.path.abspath(path)  # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                    print(f"ğŸ“¦ æ‰¾åˆ° checkpoint: {resume_from}")
                    break
            
            if resume_from:
                # è¯»å– checkpoint æŸ¥çœ‹ epoch ä¿¡æ¯
                try:
                    import torch
                    ckpt = torch.load(resume_from, map_location='cpu', weights_only=False)
                    epoch = ckpt.get('meta', {}).get('epoch', ckpt.get('epoch', 'unknown'))
                    print(f"ğŸ“¦ ä½¿ç”¨æŒ‡å®šçš„ checkpoint: {resume_from} (Epoch: {epoch})")
                except Exception as e:
                    print(f"ğŸ“¦ ä½¿ç”¨æŒ‡å®šçš„ checkpoint: {resume_from} (æ— æ³•è¯»å– epoch ä¿¡æ¯: {e})")
            else:
                print(f"âš  Checkpoint ä¸å­˜åœ¨ï¼Œå°è¯•çš„è·¯å¾„:")
                for path in possible_paths:
                    exists = "âœ“" if os.path.exists(path) else "âœ—"
                    print(f"   {exists} {path}")
                print(f"   å°†ä» epoch 0 å¼€å§‹è®­ç»ƒ")
    
    print(f"\n{'='*60}")
    print(f"Starting Deformable DETR R18 Training")
    print(f"{'='*60}")
    print(f"Config path: {config_path}")
    print(f"Work dir: {cfg.work_dir}")
    print(f"Max Epochs: {cfg.train_cfg.max_epochs}")
    print(f"Batch Size: {cfg.train_dataloader.batch_size}")
    print(f"Data Root: {data_root}")
    if resume_from:
        print(f"Resume from: {resume_from}")
    print(f"{'='*60}\n")
    
    # é…ç½® resumeï¼ˆMMEngine çš„ resume æœºåˆ¶ï¼‰
    resume_checkpoint_path = None
    if resume_from:
        # ç¡®ä¿ resume_from æ˜¯ç»å¯¹è·¯å¾„
        if not os.path.isabs(resume_from):
            # å†æ¬¡å°è¯•è§£æè·¯å¾„ï¼ˆä»¥é˜²å‰é¢çš„è§£æå¤±è´¥ï¼‰
            possible_paths = [
                os.path.abspath(resume_from),  # ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
                os.path.join(cfg.work_dir, resume_from),  # ç›¸å¯¹äº work_dir
                os.path.join(cfg.work_dir, os.path.basename(resume_from)),  # work_dir ä¸‹çš„æ–‡ä»¶å
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    resume_from = os.path.abspath(path)
                    break
        
        # æœ€ç»ˆéªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨
        if os.path.exists(resume_from):
            resume_checkpoint_path = os.path.abspath(resume_from)  # ä¿å­˜è·¯å¾„
            
            print(f"âœ“ å·²æ‰¾åˆ° checkpoint: {resume_checkpoint_path}")
            
            # è¯»å–å¹¶æ˜¾ç¤º checkpoint ä¸­çš„ epoch ä¿¡æ¯
            # MMEngine checkpoint æ–‡ä»¶ç»“æ„ï¼š
            # {
            #     'epoch': 130,                    # â† è¿™é‡Œè®°å½•äº†æ˜¯ç¬¬å‡ ä¸ª epoch
            #     'meta': {'epoch': 130, ...},    # â† æˆ–è€…åœ¨è¿™é‡Œ
            #     'optimizer_state_dict': {...},  # ä¼˜åŒ–å™¨çŠ¶æ€
            #     'message_hub': {...},           # è®­ç»ƒå†å²çŠ¶æ€
            #     'state_dict': {...},            # æ¨¡å‹æƒé‡
            #     ...
            # }
            # å½“æˆ‘ä»¬å¤åˆ¶æ–‡ä»¶æ—¶ï¼Œæ–‡ä»¶å†…éƒ¨çš„æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬ epoch ç¼–å·ï¼‰éƒ½ä¼šè¢«å®Œæ•´å¤åˆ¶
            try:
                import torch
                ckpt = torch.load(resume_checkpoint_path, map_location='cpu', weights_only=False)
                # MMEngine checkpoint æ ¼å¼ï¼šmeta.epoch æˆ–ç›´æ¥æ˜¯ epoch
                epoch_info = ckpt.get('meta', {}).get('epoch', ckpt.get('epoch', 'unknown'))
                print(f"   Checkpoint æ–‡ä»¶å†…éƒ¨ä¿¡æ¯:")
                print(f"     - epoch: {epoch_info}")
                print(f"     - åŒ…å«çš„é”®: {list(ckpt.keys())[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªé”®
                if isinstance(epoch_info, int):
                    print(f"     âœ“ å°†ä» epoch {epoch_info + 1} ç»§ç»­è®­ç»ƒ")
                    print(f"     (MMEngine ä¼šè¯»å–æ–‡ä»¶ä¸­çš„ epoch={epoch_info}ï¼Œç„¶åä» {epoch_info + 1} ç»§ç»­)")
                else:
                    print(f"   âš  æ— æ³•ç¡®å®š epoch ä¿¡æ¯ï¼Œä½†ä¼šå°è¯•æ¢å¤è®­ç»ƒçŠ¶æ€")
            except Exception as e:
                print(f"   âš  æ— æ³•è¯»å– checkpoint ä¿¡æ¯: {e}")
                print(f"   ä½†ä»ä¼šå°è¯•æ¢å¤è®­ç»ƒ")
            
            # MMEngine çš„ resume æœºåˆ¶ï¼šéœ€è¦åŒæ—¶è®¾ç½® load_from å’Œ resume
            # æ–¹æ³•1ï¼šå…ˆå°è¯•å¤åˆ¶ä¸º latest.pthï¼Œç„¶åè®¾ç½® resume = True
            latest_pth = os.path.join(cfg.work_dir, 'latest.pth')
            try:
                import shutil
                shutil.copy2(resume_checkpoint_path, latest_pth)
                print(f"âœ“ å·²å¤åˆ¶ checkpoint ä¸º latest.pth: {latest_pth}")
                print(f"   (æ–‡ä»¶å†…éƒ¨çš„ epoch ä¿¡æ¯å·²å®Œæ•´ä¿ç•™)")
                
                # MMEngine çš„æ­£ç¡® resume æ–¹å¼ï¼šåŒæ—¶è®¾ç½® load_from å’Œ resume = True
                cfg.load_from = latest_pth  # æŒ‡å®š checkpoint è·¯å¾„
                cfg.resume = True  # å¯ç”¨æ¢å¤è®­ç»ƒçŠ¶æ€
                print(f"âœ“ å·²è®¾ç½® cfg.load_from = {latest_pth}")
                print(f"âœ“ å·²è®¾ç½® cfg.resume = True")
            except Exception as e:
                print(f"âš  æ— æ³•å¤åˆ¶ checkpoint: {e}")
                print(f"   å°†å°è¯•ç›´æ¥ä½¿ç”¨ checkpoint è·¯å¾„")
                # å›é€€ï¼šç›´æ¥ä½¿ç”¨ checkpoint è·¯å¾„
                cfg.load_from = resume_checkpoint_path
                cfg.resume = True
        else:
            print(f"âš  é”™è¯¯: Checkpoint æ–‡ä»¶ä¸å­˜åœ¨: {resume_from}")
            print(f"   å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            print(f"   Work dir: {cfg.work_dir}")
            print(f"   å°è¯•çš„è·¯å¾„:")
            if 'possible_paths' in locals():
                for path in possible_paths:
                    exists = "âœ“" if os.path.exists(path) else "âœ—"
                    print(f"     {exists} {path}")
            print(f"   å°†ä» epoch 0 å¼€å§‹è®­ç»ƒï¼ˆä¸ä½¿ç”¨ resumeï¼‰")
            cfg.resume = False
    
    # åœ¨åˆ›å»º Runner ä¹‹å‰ï¼ŒéªŒè¯ resume é…ç½®
    resume_checkpoint_to_use = None
    if resume_checkpoint_path:
        latest_pth = os.path.join(cfg.work_dir, 'latest.pth')
        if cfg.resume and cfg.load_from:
            if os.path.exists(cfg.load_from):
                resume_checkpoint_to_use = cfg.load_from
                print(f"âœ“ Resume é…ç½®å·²è®¾ç½®:")
                print(f"   - cfg.load_from: {cfg.load_from}")
                print(f"   - cfg.resume: {cfg.resume}")
            elif os.path.exists(latest_pth):
                resume_checkpoint_to_use = latest_pth
                cfg.load_from = latest_pth
                print(f"âš  cfg.load_from æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ latest.pth: {latest_pth}")
    
    runner = Runner.from_cfg(cfg)
    
    # æ³¨æ„ï¼šä¸å†éœ€è¦æ‰‹åŠ¨æ¢å¤ï¼Œå› ä¸ºï¼š
    # 1. æˆ‘ä»¬å·²ç»è®¾ç½®äº† cfg.load_from å’Œ cfg.resume = True
    # 2. å·²ç»åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ äº† HistoryBuffer åˆ° safe globals
    # 3. MMEngine ä¼šåœ¨ runner.train() æ—¶è‡ªåŠ¨è°ƒç”¨ resume()
    # å¦‚æœ MMEngine çš„è‡ªåŠ¨ resume å¤±è´¥ï¼Œå®ƒä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œæˆ‘ä»¬è®©å¼‚å¸¸ä¼ æ’­
    
    runner.train()

if __name__ == '__main__':
    main()
