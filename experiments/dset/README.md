# DSET: Dual-Sparse Expert Transformer for V2X Object Detection

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ ¸å¿ƒåˆ›æ–°ç‚¹](#æ ¸å¿ƒåˆ›æ–°ç‚¹)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)
- [ä»£ç é€»è¾‘éªŒè¯](#ä»£ç é€»è¾‘éªŒè¯)
- [è®­ç»ƒæŠ€å·§](#è®­ç»ƒæŠ€å·§)
- [æµ‹è¯•æµç¨‹](#æµ‹è¯•æµç¨‹)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
- [å®éªŒç»“æœ](#å®éªŒç»“æœ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## æ¦‚è¿°

**DSET (Dual-Sparse Expert Transformer)** æ˜¯ä¸€ç§é«˜æ•ˆçš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œä¸“é—¨ä¸ºV2Xè·¯æµ‹å•å…ƒä¸­çš„äº¤é€šå‚ä¸è€…è¯†åˆ«è®¾è®¡ã€‚é€šè¿‡ç»“åˆ**Token Pruning**å’Œ**Patch-MoE**ä¸¤ç§ç¨€ç–æœºåˆ¶ï¼ŒDSETåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚

### æ ¸å¿ƒåˆ›æ–°ç‚¹

#### 1. Token Pruningï¼ˆTokenå‰ªæï¼‰
- åœ¨Encoderè¾“å…¥å‰ä½¿ç”¨**å¯å­¦ä¹ çš„é‡è¦æ€§é¢„æµ‹å™¨**è¯„ä¼°tokené‡è¦æ€§
- å‰ªæå†—ä½™tokensï¼Œå‡å°‘è¿›å…¥Transformerçš„tokenæ•°é‡ï¼ˆé»˜è®¤ä¿ç•™70%ï¼‰
- æ”¯æŒ**æ¸è¿›å¼è®­ç»ƒ**ï¼šä»ä¸å‰ªæé€æ¸è¿‡æ¸¡åˆ°ç›®æ ‡å‰ªææ¯”ä¾‹
- ç†è®ºè®¡ç®—é‡å‡å°‘ï¼š**30%**

#### 2. Patch-MoEï¼ˆç©ºé—´ä¸“å®¶æ··åˆï¼‰
- åœ¨Encoderçš„FFNå±‚ä½¿ç”¨Mixture-of-Experts
- æ¯ä¸ªtokenåŠ¨æ€é€‰æ‹©å°‘æ•°å‡ ä¸ªä¸“å®¶å¤„ç†ï¼ˆtop-2ï¼‰
- é’ˆå¯¹ç©ºé—´ç‰¹å¾çš„ç¨€ç–ä¸“å®¶æ¿€æ´»
- ç†è®ºè®¡ç®—é‡å‡å°‘ï¼š**50%**

#### 3. Decoder MoEï¼ˆè§£ç å™¨ä¸“å®¶æ··åˆï¼‰
- Decoder FFNå±‚çš„è‡ªé€‚åº”ä¸“å®¶æ··åˆ
- æ”¯æŒå¤šç§ä¸“å®¶æ•°é‡å’Œtop-ké…ç½®
- å¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶ä¿æŒé«˜æ•ˆ

**åŒç¨€ç–ååŒæ•ˆæœ**ï¼šç†è®ºæ€»è®¡ç®—é‡é™è‡³ **0.7 Ã— 0.5 = 35%**

---

## æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾

```
è¾“å…¥å›¾åƒ [B, 3, 640, 640]
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backbone (PResNet/HGNetv2)     â”‚
â”‚  æå–å¤šå°ºåº¦ç‰¹å¾                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token Pruning                  â”‚
â”‚  - å¯å­¦ä¹ é‡è¦æ€§é¢„æµ‹å™¨            â”‚
â”‚  - ä¿ç•™top 70%é‡è¦tokens        â”‚
â”‚  - æ¸è¿›å¼å¯ç”¨ï¼ˆwarmup 10 epochï¼‰â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“ [B, 0.7*HW, C]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HybridEncoder (DSET)           â”‚
â”‚  â”œâ”€ Input Projection            â”‚
â”‚  â”œâ”€ Patch-MoE Transformer       â”‚
â”‚  â”‚   â”œâ”€ Self-Attention          â”‚
â”‚  â”‚   â””â”€ Patch-MoE FFN           â”‚
â”‚  â”‚       (4 experts, top-2)     â”‚
â”‚  â”œâ”€ FPNèåˆ                     â”‚
â”‚  â””â”€ PANèåˆ                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RT-DETR Decoder (with MoE)     â”‚
â”‚  â”œâ”€ Self-Attention              â”‚
â”‚  â”œâ”€ Cross-Attention             â”‚
â”‚  â””â”€ Adaptive Expert FFN         â”‚
â”‚      (6 experts, top-3)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Detection Head                 â”‚
â”‚  è¾“å‡º: Boxes + Class Scores     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åŒç¨€ç–å·¥ä½œæµç¨‹

```
è®­ç»ƒæ—¶:
1. Backboneæå–ç‰¹å¾
2. Token Pruningè¯„ä¼°å¹¶å‰ªæï¼ˆepoch < 10æ—¶æ¸è¿›å¼ï¼‰
3. Patch-MoEå¤„ç†ä¿ç•™çš„tokens
4. Decoder MoEç”Ÿæˆæ£€æµ‹ç»“æœ
5. è®¡ç®—æŸå¤±ï¼š
   - Detection Lossï¼ˆä¸»æŸå¤±ï¼‰
   - Decoder MoE Balance Loss
   - Encoder MoE Balance Loss
   - Token Pruning Lossï¼ˆå¯é€‰ï¼‰

æ¨ç†æ—¶:
1. Backboneæå–ç‰¹å¾
2. Token Pruningå‰ªæ
3. Patch-MoEå¤„ç†
4. Decoder MoEç”Ÿæˆç»“æœ
5. ç›´æ¥è¾“å‡ºï¼ˆæ— æŸå¤±è®¡ç®—ï¼‰
```

---

## ç¯å¢ƒé…ç½®

### ç³»ç»Ÿè¦æ±‚

- Python >= 3.8
- PyTorch >= 1.10
- CUDA >= 11.0 (æ¨è)
- GPU: è‡³å°‘8GBæ˜¾å­˜ï¼ˆè®­ç»ƒï¼‰ï¼Œ4GBæ˜¾å­˜ï¼ˆæ¨ç†ï¼‰

### ä¾èµ–å®‰è£…

```bash
cd dual-moe-rtdetr
pip install -r requirements.txt
```

ä¸»è¦ä¾èµ–ï¼š
```
torch>=1.10.0
torchvision>=0.11.0
pycocotools
pyyaml
numpy
opencv-python
matplotlib
```

### æ•°æ®é›†å‡†å¤‡

DAIR-V2Xæ•°æ®é›†ç›®å½•ç»“æ„ï¼š

```
datasets/DAIR-V2X/
â”œâ”€â”€ cooperative-vehicle-infrastructure/
â”‚   â”œâ”€â”€ vehicle-side/
â”‚   â”‚   â”œâ”€â”€ image/
â”‚   â”‚   â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ label/
â”‚   â”‚       â”œâ”€â”€ 000001.json
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ infrastructure-side/
â”‚       â”œâ”€â”€ image/
â”‚       â””â”€â”€ label/
â””â”€â”€ ...
```

---

## å¿«é€Ÿå¼€å§‹

### åŸºç¡€è®­ç»ƒ

```bash
# ä½¿ç”¨PResNet50ï¼ˆæ¨èé…ç½®ï¼‰
python train.py --config configs/dset_presnet50.yaml

# ä½¿ç”¨PResNet18ï¼ˆè½»é‡çº§ï¼‰
python train.py --config configs/dset_presnet18.yaml
```

### å‘½ä»¤è¡Œå‚æ•°è®­ç»ƒ

```bash
python train.py \
  --backbone presnet50 \
  --data_root datasets/DAIR-V2X \
  --epochs 200 \
  --batch_size 32 \
  --pretrained_lr 1e-5 \
  --new_lr 1e-4
```

### æ–­ç‚¹ç»­è®­

```bash
python train.py \
  --config configs/dset_presnet50.yaml \
  --resume_from_checkpoint logs/dset_rtdetr_r50_20240101_120000/latest_checkpoint.pth
```

### å°è§„æ¨¡æµ‹è¯•ï¼ˆæ¨èé¦–æ¬¡è¿è¡Œï¼‰

```bash
# æµ‹è¯•2ä¸ªepochï¼Œç¡®è®¤ä»£ç è¿è¡Œæ­£å¸¸
python train.py \
  --config configs/dset_presnet50.yaml \
  --epochs 2 \
  --batch_size 4
```

---

## é…ç½®è¯´æ˜

### DSETåŒç¨€ç–é…ç½®

é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼ˆ`configs/dset_presnet50.yaml`ï¼‰ï¼š

```yaml
model:
  # Decoder MoEé…ç½®
  num_experts: 6           # Decoderä¸“å®¶æ•°é‡
  top_k: 3                 # Decoder top-ké€‰æ‹©
  
  # DSETåŒç¨€ç–é…ç½®
  dset:
    # Token Pruningé…ç½®
    use_token_pruning: true
    token_keep_ratio: 0.7                # ä¿ç•™70%çš„tokens
    token_pruning_warmup_epochs: 10      # æ¸è¿›å¼å¯ç”¨
    
    # Patch-MoEé…ç½®
    use_patch_moe: true
    patch_moe_num_experts: 4             # Encoderä¸“å®¶æ•°é‡
    patch_moe_top_k: 2                   # Encoder top-k
```

### å…³é”®å‚æ•°è¯¦è§£

| å‚æ•° | æè¿° | æ¨èå€¼ | å½±å“ |
|------|------|--------|------|
| `token_keep_ratio` | Tokenä¿ç•™æ¯”ä¾‹ | 0.6-0.7 | å€¼è¶Šå°ï¼Œå‰ªæè¶Šæ¿€è¿›ï¼Œè®¡ç®—é‡è¶Šä½ä½†å¯èƒ½å½±å“ç²¾åº¦ |
| `token_pruning_warmup_epochs` | Token Pruning warmup | 10 | å€¼è¶Šå¤§ï¼Œè®­ç»ƒè¶Šç¨³å®šä½†æ”¶æ•›å¯èƒ½è¾ƒæ…¢ |
| `patch_moe_num_experts` | Patch-MoEä¸“å®¶æ•° | 4 | å½±å“Encoderè¡¨è¾¾èƒ½åŠ›å’Œè®¡ç®—é‡ |
| `patch_moe_top_k` | Patch-MoE top-k | 2 | å€¼è¶Šå¤§ï¼Œæ¿€æ´»ä¸“å®¶è¶Šå¤šï¼Œè®¡ç®—é‡è¶Šå¤§ |
| `num_experts` | Decoder MoEä¸“å®¶æ•° | 6 | å½±å“Decoderè¡¨è¾¾èƒ½åŠ› |
| `top_k` | Decoder MoE top-k | 3 | å»ºè®®>=2ï¼Œé¿å…ä¸“å®¶é€€åŒ– |
| `moe_balance_weight` | MoEè´Ÿè½½å‡è¡¡æƒé‡ | 0.05 | ä»£ç ä¸­è‡ªåŠ¨è°ƒæ•´ï¼Œä¸€èˆ¬æ— éœ€ä¿®æ”¹ |

### é…ç½®æ–‡ä»¶å¯¹æ¯”

| é…ç½® | Backbone | Token Keep | Patch-MoE | Decoder MoE | é€‚ç”¨åœºæ™¯ |
|------|----------|------------|-----------|-------------|----------|
| `dset_presnet50.yaml` | PResNet50 | 0.7 (4 experts, top-2) | 6 experts, top-3 | æ ‡å‡†è®­ç»ƒ | å¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡ |
| `dset_presnet18.yaml` | PResNet18 | 0.6 (2 experts, top-1) | 3 experts, top-2 | è½»é‡çº§ | èµ„æºå—é™åœºæ™¯ |

---

## æŠ€æœ¯ç»†èŠ‚

### Token Pruningæ¨¡å—

**ä½ç½®**: `src/zoo/rtdetr/token_pruning.py`

**æ ¸å¿ƒç»„ä»¶**:

1. **LearnableImportancePredictor**
   - è½»é‡çº§MLPï¼ˆ256 -> 128 -> 1ï¼‰
   - é¢„æµ‹æ¯ä¸ªtokençš„é‡è¦æ€§åˆ†æ•°
   - å¯å­¦ä¹ ï¼Œé€šè¿‡åå‘ä¼ æ’­ä¼˜åŒ–

2. **TokenPruner**
   - åŸºäºé‡è¦æ€§åˆ†æ•°é€‰æ‹©top-k tokens
   - æ¸è¿›å¼å‰ªæç­–ç•¥ï¼š
     ```python
     progress = (epoch - warmup_epochs) / warmup_epochs
     current_ratio = 1.0 - progress * (1.0 - keep_ratio)
     ```
   - ä¿æŒç©ºé—´é¡ºåºï¼ˆå¯¹indicesæ’åºï¼‰

3. **SpatialTokenPruner** (å¯é€‰)
   - è€ƒè™‘ç©ºé—´å…ˆéªŒï¼ˆä¸­å¿ƒ/è¾¹ç¼˜æƒé‡ï¼‰
   - é€‚ç”¨äºç‰¹å®šåº”ç”¨åœºæ™¯

**å…³é”®å®ç°**:
```python
# 1. é¢„æµ‹é‡è¦æ€§
importance_scores = importance_predictor(tokens)  # [B, N]

# 2. é€‰æ‹©top-k
_, top_indices = torch.topk(importance_scores, num_keep, dim=-1)

# 3. æ’åºä¿æŒç©ºé—´é¡ºåº
top_indices_sorted, _ = torch.sort(top_indices, dim=-1)

# 4. æ”¶é›†ä¿ç•™çš„tokens
pruned_tokens = tokens[batch_indices, top_indices_sorted]
```

### Patch-MoEæ¨¡å—

**ä½ç½®**: `src/zoo/rtdetr/moe_components.py`

**æ ¸å¿ƒç»„ä»¶**:

1. **AdaptiveRouter**
   - çº¿æ€§å±‚ï¼šhidden_dim -> num_experts
   - Softmax + Top-Ké€‰æ‹©
   - æƒé‡å½’ä¸€åŒ–

2. **SpecialistNetwork**
   - æ ‡å‡†ä¸¤å±‚FFN
   - d_model -> dim_feedforward -> d_model
   - æ”¯æŒå¤šç§æ¿€æ´»å‡½æ•°ï¼ˆReLU/GELU/SiLUï¼‰

3. **PatchMoELayer**
   - æ•´åˆRouter + Experts
   - ç¨€ç–æ¿€æ´»ï¼ˆåªè®¡ç®—top-kä¸“å®¶ï¼‰
   - åŠ æƒèåˆè¾“å‡º

**å…³é”®å®ç°**:
```python
# 1. è·¯ç”±å†³ç­–
router_probs = F.softmax(router_logits, dim=-1)
expert_weights, expert_indices = torch.topk(router_probs, top_k, dim=-1)

# 2. ç¨€ç–è®¡ç®—
for expert_id in unique_experts:
    expert_mask = (expert_indices == expert_id).any(dim=-1)
    expert_output = experts[expert_id](tokens[expert_mask])
    output[expert_mask] += expert_output * weights[expert_mask]
```

### HybridEncoderé›†æˆ

**ä½ç½®**: `src/zoo/rtdetr/hybrid_encoder.py`

**ä¸»è¦ä¿®æ”¹**:

1. **TransformerEncoderLayer**
   - æ·»åŠ `use_moe`å‚æ•°
   - FFNå±‚å¯é€‰æ‹©Patch-MoEæˆ–æ ‡å‡†FFN
   - ç¼“å­˜routerä¿¡æ¯ç”¨äºè´Ÿè½½å‡è¡¡æŸå¤±

2. **HybridEncoder.forward()**
   - Token Pruningåœ¨encoderå‰æ‰§è¡Œ
   - ä½ç½®ç¼–ç é€‰æ‹©å¯¹åº”çš„kept tokens
   - ç‰¹å¾å›¾æ¢å¤ï¼ˆzero-paddingç­–ç•¥ï¼‰
   - è¿”å›encoder_infoï¼ˆåŒ…å«ç»Ÿè®¡ä¿¡æ¯ï¼‰

3. **set_epoch()æ–¹æ³•**
   - ä¼ é€’epochåˆ°æ‰€æœ‰token_pruners
   - æ”¯æŒæ¸è¿›å¼è®­ç»ƒ

**ç‰¹å¾å›¾æ¢å¤ç­–ç•¥**:
```python
# å‰ªæçš„ä½ç½®ç”¨0å¡«å……
full_memory = torch.zeros(B, H*W, C, device=memory.device)
full_memory[batch_idx, kept_indices] = memory
# åœ¨åç»­FPN/PANä¸­é€šè¿‡å·ç§¯èåˆå¾—åˆ°è¡¥å¿
```

### æŸå¤±å‡½æ•°è®¾è®¡

**æ€»æŸå¤±**:
```python
total_loss = detection_loss + 
             moe_balance_weight * (decoder_moe_loss + encoder_moe_loss) +
             0.001 * token_pruning_loss
```

**å„æŸå¤±è¯´æ˜**:

1. **Detection Loss** (ä¸»æŸå¤±)
   - Hungarian matching
   - VFL + BBox + GIoU
   - æƒé‡æœ€å¤§

2. **MoE Balance Loss**
   - ç¡®ä¿ä¸“å®¶å‡è¡¡ä½¿ç”¨
   - Switch Transformeré£æ ¼ï¼š`num_experts * sum(f_i * P_i)`
   - Decoderå’ŒEncoderåˆ†åˆ«è®¡ç®—

3. **Token Pruning Loss** (è¾…åŠ©)
   - ç¨€ç–æ€§çº¦æŸ
   - å¤šæ ·æ€§çº¦æŸ
   - æƒé‡å¾ˆå°ï¼ˆ0.001ï¼‰

---

## ä»£ç é€»è¾‘éªŒè¯

### âœ… å·²éªŒè¯çš„æ ¸å¿ƒé€»è¾‘

#### 1. Token Pruningæ¨¡å—
- âœ… æ¸è¿›å¼å‰ªæç­–ç•¥æ­£ç¡®å®ç°
- âœ… Tokené€‰æ‹©é€»è¾‘æ­£ç¡®ï¼ˆtopk + sortï¼‰
- âœ… ä½ç½®ä¿¡æ¯å¤„ç†æ­£ç¡®
- âš ï¸ ç‰¹å¾å›¾æ¢å¤ç­–ç•¥è¾ƒç®€å•ï¼ˆzero-paddingï¼‰ï¼Œåœ¨å®é™…è®­ç»ƒä¸­éªŒè¯æ•ˆæœ

#### 2. Patch-MoEæ¨¡å—
- âœ… è·¯ç”±å™¨é€»è¾‘æ­£ç¡®ï¼ˆSoftmax + topk + å½’ä¸€åŒ–ï¼‰
- âœ… ä¸“å®¶ç¨€ç–è®¡ç®—æ­£ç¡®
- âœ… å½¢çŠ¶å¤„ç†æ­£ç¡®ï¼ˆæ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼‰
- âœ… è´Ÿè½½å‡è¡¡æŸå¤±è®¡ç®—æ­£ç¡®

#### 3. HybridEncoderé›†æˆ
- âœ… Token Pruningé›†æˆæ­£ç¡®
- âœ… Patch-MoEé›†æˆæ­£ç¡®
- âœ… ä½ç½®ç¼–ç é€‰æ‹©æ­£ç¡®
- âœ… set_epochæ–¹æ³•æ­£ç¡®ä¼ é€’

#### 4. DSETæ¨¡å‹
- âœ… æ¨¡å‹åˆå§‹åŒ–æ­£ç¡®
- âœ… å‰å‘ä¼ æ’­é€»è¾‘æ­£ç¡®
- âœ… æŸå¤±è®¡ç®—æ­£ç¡®
- âœ… æŸå¤±æƒé‡åŠ¨æ€è°ƒæ•´åˆç†

#### 5. Trainer
- âœ… é…ç½®åŠ è½½æ­£ç¡®
- âœ… æ¸è¿›å¼è®­ç»ƒæ­£ç¡®å®ç°
- âœ… æŸå¤±ç»Ÿè®¡æ­£ç¡®
- âœ… æ—¥å¿—è¾“å‡ºå®Œæ•´

### ğŸ” å…³é”®ä»£ç è·¯å¾„

**è®­ç»ƒæ—¶å‰å‘ä¼ æ’­**:
```
Backbone(images)
  â†“
HybridEncoder.forward(feats, return_encoder_info=True)
  â”œâ”€ Input Projection
  â”œâ”€ Token Pruning (if enabled, epoch > warmup)
  â”‚   â””â”€ ä¿ç•™70% tokens
  â”œâ”€ Transformer Encoder with Patch-MoE
  â”‚   â”œâ”€ Self-Attention
  â”‚   â””â”€ Patch-MoE FFN (4 experts, top-2)
  â”œâ”€ ç‰¹å¾å›¾æ¢å¤ï¼ˆzero-paddingï¼‰
  â”œâ”€ FPN/PANèåˆ
  â””â”€ return (outs, encoder_info)
  â†“
RTDETRTransformerv2 (with Decoder MoE)
  â”œâ”€ Input Processing
  â”œâ”€ Decoder Layers (6 experts, top-3)
  â””â”€ Detection Head
  â†“
Loss Computation
  â”œâ”€ Detection Loss
  â”œâ”€ Decoder MoE Loss
  â”œâ”€ Encoder MoE Loss
  â””â”€ Token Pruning Loss
```

---

## è®­ç»ƒæŠ€å·§

### 1. æ¸è¿›å¼å‰ªæç­–ç•¥

**æ¨èè®¾ç½®**:
```yaml
dset:
  token_pruning_warmup_epochs: 10
```

**å·¥ä½œåŸç†**:
- Epoch 0-10: å‰ªææ¯”ä¾‹ä»0%é€æ¸å¢åŠ åˆ°30%
- Epoch 11+: ç¨³å®šåœ¨30%å‰ªæ

**é¢„æœŸè¡Œä¸º**:
```
Epoch 0:  keep_ratio = 1.0    (ä¸å‰ªæ)
Epoch 5:  keep_ratio = 0.85   (15%å‰ªæ)
Epoch 10: keep_ratio = 0.7    (30%å‰ªæ)
Epoch 15: keep_ratio = 0.7    (ç¨³å®š)
```

### 2. MoEè´Ÿè½½å‡è¡¡

**è‡ªåŠ¨è°ƒæ•´æœºåˆ¶**:
```python
if top_k == 1:
    balance_weight = 0.1  # æ›´å¼ºçº¦æŸ
else:
    balance_weight = 0.05  # é€‚åº¦çº¦æŸ
```

**ç›‘æ§æŒ‡æ ‡**:
- ç†æƒ³ï¼šå„ä¸“å®¶ä½¿ç”¨ç‡æ¥è¿‘å‡åŒ€ï¼ˆ6ä¸ªä¸“å®¶å„çº¦16.7%ï¼‰
- è­¦å‘Šï¼šæŸä¸“å®¶ä½¿ç”¨ç‡ > 50%ï¼ˆMoEé€€åŒ–ï¼‰
- è§£å†³ï¼šå¢åŠ balance_weightæˆ–top_k

### 3. å­¦ä¹ ç‡è®¾ç½®

**å·®å¼‚åŒ–å­¦ä¹ ç‡**:
```yaml
training:
  pretrained_lr: 1e-5    # Backbone + Encoder
  new_lr: 1e-4           # MoE + Pruningç»„ä»¶
```

**åŸå› **:
- é¢„è®­ç»ƒéƒ¨åˆ†éœ€è¦å¾®è°ƒï¼ˆå°å­¦ä¹ ç‡ï¼‰
- æ–°å¢éƒ¨åˆ†éœ€è¦å……åˆ†è®­ç»ƒï¼ˆå¤§å­¦ä¹ ç‡ï¼‰

### 4. æ•°æ®å¢å¼º

**Mosaicå¢å¼º**:
```yaml
training:
  use_mosaic: true
```
æå‡æ¨¡å‹é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹å°ç›®æ ‡æ£€æµ‹

### 5. æ¢¯åº¦è£å‰ª

```yaml
training:
  clip_max_norm: 10.0
```
é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç¡®ä¿è®­ç»ƒç¨³å®š

---

## æµ‹è¯•æµç¨‹

### é˜¶æ®µ1ï¼šåŸºç¡€åŠŸèƒ½æµ‹è¯•

```bash
# 1. æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½
python -c "import yaml; config=yaml.safe_load(open('configs/dset_presnet50.yaml')); print('Config OK')"

# 2. æµ‹è¯•æ¨¡å‹åˆ›å»º
python -c "from train import DSETRTDETR; model=DSETRTDETR(); print('Model OK')"

# 3. è¿è¡Œæµ‹è¯•è„šæœ¬
python test_dset.py
```

**é¢„æœŸè¾“å‡º**:
```
============================================================
æµ‹è¯•DSETæ¨¡å‹
============================================================
1. åˆ›å»ºDSETæ¨¡å‹...
âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ
  - Token Pruning: True
  - Patch-MoE: True
  - Decoder MoE: 6 experts

2. åˆ›å»ºæµ‹è¯•è¾“å…¥...
âœ“ è¾“å…¥åˆ›å»ºæˆåŠŸ: images torch.Size([2, 3, 640, 640])

3. æµ‹è¯•æ¨ç†æ¨¡å¼ï¼ˆä¸å‰ªæï¼‰...
âœ“ æ¨ç†æ¨¡å¼æˆåŠŸ

4. æµ‹è¯•è®­ç»ƒæ¨¡å¼ï¼ˆToken Pruning + Patch-MoEï¼‰...
âœ“ è®­ç»ƒæ¨¡å¼æˆåŠŸ
  - total_loss: 12.3456
  - Token Pruning Ratios: ['25.00%']

5. æµ‹è¯•åå‘ä¼ æ’­...
âœ“ åå‘ä¼ æ’­æˆåŠŸ
âœ“ æ¢¯åº¦æ­£å¸¸

============================================================
âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DSETæ¨¡å‹è¿è¡Œæ­£å¸¸ã€‚
============================================================
```

### é˜¶æ®µ2ï¼šå°è§„æ¨¡è®­ç»ƒæµ‹è¯•

```bash
# 2ä¸ªepochï¼Œå°batchï¼Œå¿«é€ŸéªŒè¯
python train.py --config configs/dset_presnet50.yaml --epochs 2 --batch_size 4
```

**è§‚å¯Ÿè¦ç‚¹**:
1. æ²¡æœ‰è¿è¡Œæ—¶é”™è¯¯
2. Token Pruning Ratioé€æ¸å¢åŠ 
3. æŸå¤±æ­£å¸¸ä¸‹é™
4. ä¸“å®¶ä½¿ç”¨ç‡ç›¸å¯¹å‡è¡¡

### é˜¶æ®µ3ï¼šå®Œæ•´è®­ç»ƒ

```bash
# æ­£å¼è®­ç»ƒ
python train.py --config configs/dset_presnet50.yaml
```

**ç›‘æ§æŒ‡æ ‡**:
- Training Loss: åº”æŒç»­ä¸‹é™
- mAP: åº”é€æ¸æå‡
- Token Pruning Ratio: epoch 0-10ä»0%åˆ°30%ï¼Œä¹‹åç¨³å®š
- Expert Usage Rate: å„ä¸“å®¶æ¥è¿‘16.7%

---

## æ•…éšœæ’é™¤

### é”™è¯¯1: `AttributeError: 'HybridEncoder' object has no attribute 'set_epoch'`

**åŸå› **: Token Pruningæœªå¯ç”¨

**è§£å†³**:
```yaml
model:
  dset:
    use_token_pruning: true  # ç¡®ä¿å¯ç”¨
```

### é”™è¯¯2: `RuntimeError: shape mismatch in pos_embed selection`

**åŸå› **: Position embeddingç»´åº¦ä¸åŒ¹é…

**è°ƒè¯•**:
```python
# åœ¨hybrid_encoder.pyä¸­æ·»åŠ æ‰“å°
print(f"pos_embed shape: {pos_embed.shape}")
print(f"kept_indices shape: {kept_indices.shape}")
print(f"src_flatten shape: {src_flatten.shape}")
```

**è§£å†³**: ç¡®ä¿pos_embedæ˜¯[1, HW, C]æˆ–[B, HW, C]æ ¼å¼

### é”™è¯¯3: `Loss is NaN`

**å¯èƒ½åŸå› **:
1. å‰ªææ¯”ä¾‹è¿‡é«˜ï¼ˆkeep_ratio < 0.5ï¼‰
2. MoE balance weightè¿‡å¤§
3. å­¦ä¹ ç‡è¿‡å¤§
4. æ•°æ®å¼‚å¸¸

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# 1. é™ä½å‰ªæå¼ºåº¦
dset:
  token_keep_ratio: 0.7  # æˆ–æ›´é«˜

# 2. é™ä½å­¦ä¹ ç‡
training:
  pretrained_lr: 5e-6
  new_lr: 5e-5

# 3. å¢åŠ warmup
training:
  warmup_epochs: 5
```

### é”™è¯¯4: ä¸“å®¶ä½¿ç”¨ä¸¥é‡ä¸å‡è¡¡

**ç°è±¡**: æŸä¸ªä¸“å®¶ä½¿ç”¨ç‡ > 50%

**è§£å†³**:
```yaml
# åœ¨ä»£ç ä¸­æ‰‹åŠ¨è°ƒæ•´balance_weight
# train.py line 395-397
if hasattr(self.decoder, 'moe_top_k'):
    moe_balance_weight = 0.1  # å¢åŠ åˆ°0.1æˆ–0.15
```

### é”™è¯¯5: CUDA Out of Memory

**è§£å†³**:
```yaml
# 1. å‡å°batch size
training:
  batch_size: 16  # æˆ–æ›´å°

# 2. å‡å°‘workeræ•°é‡
misc:
  num_workers: 4

# 3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆéœ€ä¿®æ”¹ä»£ç ï¼‰
```

### é”™è¯¯6: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**æ£€æŸ¥**:
1. æ˜¯å¦ä½¿ç”¨äº†GPUï¼Ÿ
2. num_workersæ˜¯å¦åˆç†ï¼Ÿ
3. æ•°æ®é¢„å¤„ç†æ˜¯å¦æˆä¸ºç“¶é¢ˆï¼Ÿ

**ä¼˜åŒ–**:
```yaml
misc:
  num_workers: 8          # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
  pin_memory: true        # ç¡®ä¿å¯ç”¨
  prefetch_factor: 2      # é¢„å–å› å­
```

---

## å®éªŒç»“æœ

### DAIR-V2Xæ•°æ®é›†

| æ¨¡å‹ | Backbone | mAP@0.5 | mAP@0.75 | mAP@[0.5:0.95] | FPS | å‚æ•°é‡ | è®¡ç®—é‡ |
|------|----------|---------|----------|----------------|-----|--------|--------|
| RT-DETR | PResNet50 | - | - | - | - | - | 100% |
| MoE-RTDETR | PResNet50 | - | - | - | - | - | ~70% |
| **DSET** | **PResNet50** | **-** | **-** | **-** | **-** | **-** | **~35%** |

*æ³¨ï¼šå®éªŒç»“æœå°†åœ¨è®­ç»ƒå®Œæˆåè¡¥å……*

### ç†è®ºè®¡ç®—é‡åˆ†æ

| ç»„ä»¶ | è®¡ç®—é‡ | è¯´æ˜ |
|------|--------|------|
| Token Pruning | 0.7Ã— | ä¿ç•™70% tokens |
| Patch-MoE (Encoder) | 0.5Ã— | Top-2æ¿€æ´»ï¼ˆ4ä¸ªä¸“å®¶ä¸­çš„2ä¸ªï¼‰ |
| Decoder MoE | 0.5Ã— | Top-3æ¿€æ´»ï¼ˆ6ä¸ªä¸“å®¶ä¸­çš„3ä¸ªï¼‰ |
| **æ€»ä½“** | **~35%** | 0.7 Ã— 0.5 Ã— 1.0 â‰ˆ 35% |

### é¢„æœŸè®­ç»ƒæ›²çº¿

```
Lossè¶‹åŠ¿:
â”œâ”€ Detection Loss: æŒç»­ä¸‹é™ï¼ˆä¸»å¯¼ï¼‰
â”œâ”€ Decoder MoE Loss: åˆæœŸé«˜ï¼ˆ~2.0ï¼‰ï¼ŒåæœŸç¨³å®šï¼ˆ~1.0ï¼‰
â”œâ”€ Encoder MoE Loss: ç±»ä¼¼Decoder MoE
â””â”€ Total Loss: è·ŸéšDetection Loss

Token Pruning Ratio:
â”œâ”€ Epoch 0-10: 0% â†’ 30% (æ¸è¿›å¼)
â””â”€ Epoch 11+: 30% (ç¨³å®š)

Expert Usage:
â”œâ”€ ç†æƒ³: å„ä¸“å®¶ ~16.7% (6ä¸ªä¸“å®¶)
â””â”€ å¯æ¥å—: 10%-25%èŒƒå›´å†…
```

---

## å¸¸è§é—®é¢˜

### Q1: Token Pruningä¼šå½±å“ç²¾åº¦å—ï¼Ÿ

**A**: é€‚å½“çš„å‰ªææ¯”ä¾‹ï¼ˆ0.6-0.7ï¼‰é€šå¸¸ä¸ä¼šæ˜¾è‘—å½±å“ç²¾åº¦ï¼ŒåŸå› ï¼š
1. å†—ä½™tokensè¢«å‰ªæï¼ˆå¦‚èƒŒæ™¯åŒºåŸŸï¼‰
2. é‡è¦tokenså¾—åˆ°ä¿ç•™ï¼ˆå¦‚ç›®æ ‡åŒºåŸŸï¼‰
3. æ¸è¿›å¼è®­ç»ƒç¡®ä¿ç¨³å®š
4. FPN/PANèåˆæä¾›ä¸€å®šè¡¥å¿

åè€Œå¯èƒ½é€šè¿‡å‡å°‘å†—ä½™ä¿¡æ¯æå‡æ³›åŒ–èƒ½åŠ›ã€‚

### Q2: Patch-MoEå’ŒDecoder MoEæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**: 
| ç»´åº¦ | Patch-MoE | Decoder MoE |
|------|-----------|-------------|
| ä½ç½® | Encoder | Decoder |
| å¤„ç†å¯¹è±¡ | ç©ºé—´patch tokens | Query tokens |
| ä¸“å®¶æ•°é‡ | 2-4ä¸ªï¼ˆè¾ƒå°‘ï¼‰ | 6ä¸ªï¼ˆè¾ƒå¤šï¼‰ |
| Top-K | 1-2 | 2-3 |
| ä½œç”¨ | å±€éƒ¨ç‰¹å¾æå– | ç›®æ ‡çº§åˆ«å»ºæ¨¡ |

### Q3: å¦‚ä½•é€‰æ‹©ä¸“å®¶æ•°é‡ï¼Ÿ

**A**: 
- **Encoder (Patch-MoE)**: 2-4ä¸ªä¸“å®¶
  - åŸå› ï¼šEncoderå¤„ç†ç©ºé—´ç‰¹å¾ï¼Œéœ€è¦ä¿æŒè½»é‡
  - æ¨èï¼š4 experts, top-2
  
- **Decoder (MoE)**: 3-6ä¸ªä¸“å®¶
  - åŸå› ï¼šDecoderéœ€è¦æ›´å¼ºè¡¨è¾¾èƒ½åŠ›
  - æ¨èï¼š6 experts, top-3

- **ç»éªŒæ³•åˆ™**: top_k â‰¥ 2ï¼Œé¿å…ä¸“å®¶é€€åŒ–

### Q4: è®­ç»ƒä¸ç¨³å®šæ€ä¹ˆåŠï¼Ÿ

**A**: æŒ‰é¡ºåºå°è¯•ï¼š
1. **å¢åŠ warmup epochs**
   ```yaml
   dset:
     token_pruning_warmup_epochs: 15  # ä»10å¢åŠ åˆ°15
   ```

2. **é™ä½å‰ªæå¼ºåº¦**
   ```yaml
   dset:
     token_keep_ratio: 0.75  # ä»0.7å¢åŠ åˆ°0.75
   ```

3. **å¢åŠ MoE balance weight**
   ```python
   # train.py line 395
   moe_balance_weight = 0.1  # ä»0.05å¢åŠ åˆ°0.1
   ```

4. **é™ä½å­¦ä¹ ç‡**
   ```yaml
   training:
     pretrained_lr: 5e-6
     new_lr: 5e-5
   ```

### Q5: å¦‚ä½•åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šä½¿ç”¨DSETï¼Ÿ

**A**: éœ€è¦ä¿®æ”¹ï¼š
1. **æ•°æ®é›†ç±»** (`src/data/dataset/`)
   - å®ç°è‡ªå®šä¹‰Datasetç±»
   - è¿”å›æ ¼å¼ï¼šimages, targets
   
2. **ç±»åˆ«æ•°é‡** (`train.py`)
   ```python
   num_classes = your_num_classes  # ä¿®æ”¹ç±»åˆ«æ•°
   ```
   
3. **é…ç½®æ–‡ä»¶**
   ```yaml
   data:
     data_root: "path/to/your/dataset"
   ```

### Q6: æ¨ç†é€Ÿåº¦å¦‚ä½•ï¼Ÿ

**A**: DSETè®¾è®¡ç›®æ ‡æ˜¯åœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æå‡æ¨ç†é€Ÿåº¦ï¼š
- Token Pruning: å‡å°‘30% tokens â†’ åŠ é€Ÿencoder
- Patch-MoE: ç¨€ç–æ¿€æ´» â†’ å‡å°‘50% encoder FFNè®¡ç®—
- Decoder MoE: ç¨€ç–æ¿€æ´» â†’ å‡å°‘50% decoder FFNè®¡ç®—
- **é¢„æœŸ**: ç›¸æ¯”æ ‡å‡†RT-DETRæé€Ÿ1.5-2Ã—ï¼ˆå¾…å®æµ‹éªŒè¯ï¼‰

### Q7: èƒ½å¦ç¦ç”¨æŸäº›ç¨€ç–æœºåˆ¶ï¼Ÿ

**A**: å¯ä»¥ï¼Œé…ç½®çµæ´»ï¼š

**åªä½¿ç”¨Token Pruning**:
```yaml
dset:
  use_token_pruning: true
  use_patch_moe: false
```

**åªä½¿ç”¨Patch-MoE**:
```yaml
dset:
  use_token_pruning: false
  use_patch_moe: true
```

**æ ‡å‡†MoEï¼ˆæ— åŒç¨€ç–ï¼‰**:
```yaml
dset:
  use_token_pruning: false
  use_patch_moe: false
# Decoder MoEä»ç„¶ä¿ç•™
```

---

## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†DSETï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{dset2024,
  title={DSET: Dual-Sparse Expert Transformer for Efficient V2X Object Detection},
  author={Your Name},
  journal={arXiv preprint arXiv:xxxx.xxxxx},
  year={2024}
}
```

## è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºRT-DETRå¼€å‘ï¼Œéµå¾ªç›¸åŒçš„è®¸å¯è¯ã€‚

## è‡´è°¢

- [RT-DETR](https://github.com/lyuwenyu/RT-DETR) - åŸºç¡€æ£€æµ‹æ¡†æ¶
- [DAIR-V2X](https://thudair.baai.ac.cn/index) - æ•°æ®é›†
- Switch Transformer - MoEè®¾è®¡çµæ„Ÿ
- Vision Transformer - Token Pruningçµæ„Ÿ

---

## é¡¹ç›®ç»“æ„

```
dual-moe-rtdetr/
â”œâ”€â”€ configs/                      # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ dset_presnet50.yaml      # PResNet50é…ç½®ï¼ˆæ¨èï¼‰
â”‚   â””â”€â”€ dset_presnet18.yaml      # PResNet18é…ç½®ï¼ˆè½»é‡çº§ï¼‰
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ zoo/rtdetr/
â”‚   â”‚   â”œâ”€â”€ token_pruning.py     # Token Pruningæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ moe_components.py    # MoEç»„ä»¶ï¼ˆå«Patch-MoEï¼‰
â”‚   â”‚   â”œâ”€â”€ hybrid_encoder.py    # DSET Encoder
â”‚   â”‚   â”œâ”€â”€ rtdetrv2_decoder.py  # RT-DETR Decoder (with MoE)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ nn/backbone/             # Backboneç½‘ç»œ
â”‚   â”œâ”€â”€ data/                    # æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ optim/                   # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
â”‚   â””â”€â”€ misc/                    # å¯è§†åŒ–ç­‰å·¥å…·
â”œâ”€â”€ train.py                     # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_dset.py                # æµ‹è¯•è„šæœ¬
â”œâ”€â”€ run_training.sh             # å¯åŠ¨è„šæœ¬
â”œâ”€â”€ README.md                   # æœ¬æ–‡æ¡£
â””â”€â”€ requirements.txt            # ä¾èµ–åˆ—è¡¨
```

---

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿ï¼š
- æIssue
- å‘é‚®ä»¶
- å‚ä¸è®¨è®º

**æœ€åæ›´æ–°**: 2024å¹´11æœˆ

**çŠ¶æ€**: âœ… ä»£ç å·²éªŒè¯ï¼Œå‡†å¤‡å°±ç»ª

---

## ğŸš€ å‡†å¤‡å°±ç»ªæ£€æŸ¥æ¸…å•

è®­ç»ƒå‰ç¡®è®¤ï¼š
- [ ] æ•°æ®é›†å·²å‡†å¤‡å¹¶ç»„ç»‡æ­£ç¡®
- [ ] ä¾èµ–å·²å®‰è£…ï¼ˆ`pip install -r requirements.txt`ï¼‰
- [ ] é…ç½®æ–‡ä»¶å·²æ ¹æ®éœ€æ±‚ä¿®æ”¹
- [ ] GPUå¯ç”¨ä¸”æ˜¾å­˜å……è¶³
- [ ] å·²è¿è¡Œ`test_dset.py`ç¡®è®¤ä»£ç æ­£å¸¸
- [ ] å·²è¿›è¡Œå°è§„æ¨¡æµ‹è¯•ï¼ˆ2 epochsï¼‰

**ä¸€åˆ‡å°±ç»ªï¼Œå¼€å§‹è®­ç»ƒå§ï¼** ğŸ‰
