# DSET-2 with PResNet18 - 轻量级配置
# Decoder: 2 experts, top-1 | Encoder: 2 experts, top-1 | Token Keep: 60%

model:
  config_name: "C"
  num_experts: 2
  top_k: 1
  backbone: "presnet18"
  encoder:
    in_channels: [128, 256, 512]
    expansion: 0.5
    num_encoder_layers: 1  # Encoder Transformer层数（支持共享MoE，可尝试2-3层不增加参数量）
  pretrained_weights: "pretrained/rtdetrv2_r18vd_120e_coco_rerun_48.1.pth"
  num_decoder_layers: 3
  hidden_dim: 256
  num_queries: 100
  
  # DSET双稀疏配置
  dset:
    # DSET核心特性：Patch-MoE 和 Patch-level Pruning 必然启用（无需配置 use_patch_moe 和 use_token_pruning）
    token_keep_ratio: 0.6
    token_pruning_warmup_epochs: 10
    patch_moe_num_experts: 2
    patch_moe_top_k: 1
    patch_moe_patch_size: 8  # Patch-MoE patch大小（8x8）
    use_token_pruning_loss: true  # Token Pruning辅助损失（已启用）
    token_pruning_loss_weight: 0.002  # Token Pruning损失权重（推荐0.001-0.005）

training:
  epochs: 200
  batch_size: 64
  warmup_epochs: 5
  pretrained_lr: 1e-5
  new_lr: 1e-4
  eta_min: 1e-6
  weight_decay: 0.0001
  use_mosaic: true
  ema_decay: 0.9999
  clip_max_norm: 10.0
  scheduler: "cosine"
  early_stopping_patience: 35
  early_stopping_metric: "mAP_0.5_0.95"
  seed: 42
  deterministic: false
  decoder_moe_balance_weight: 0.1  # Decoder MoE负载均衡损失权重 (top_k=1, 需要更强约束)
  encoder_moe_balance_weight: 0.1  # Encoder MoE负载均衡损失权重 (top_k=1, 需要更强约束)

data:
  data_root: "/root/autodl-fs/datasets/DAIR-V2X"

checkpoint:
  resume_mode: "auto"
  log_dir: "logs"

misc:
  device: "cuda"
  num_workers: 16
  pin_memory: true

