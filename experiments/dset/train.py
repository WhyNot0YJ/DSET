#!/usr/bin/env python3
"""DSET Training Script - Dual-Sparse Expert Transformer (Token Pruning + Patch-MoE + Decoder MoE)"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import re
import gc
from torch.utils.data import DataLoader
from pathlib import Path
import logging
from datetime import datetime
import json
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from pycocotools.cocoeval import COCOeval
from pycocotools.coco import COCO

project_root = Path(__file__).parent.resolve()
if str(os.getcwd()) not in sys.path:
    sys.path.insert(0, os.getcwd())
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root.parent))

from seed_utils import set_seed, seed_worker
from src.misc.training_visualizer import TrainingVisualizer
from src.misc.early_stopping import EarlyStopping
from src.zoo.rtdetr import HybridEncoder, RTDETRTransformerv2, RTDETRCriterionv2, HungarianMatcher, ASBGate
from src.nn.backbone.presnet import PResNet
from src.nn.backbone.hgnetv2 import HGNetv2
from src.nn.backbone.csp_resnet import CSPResNet
from src.nn.backbone.csp_darknet import CSPDarkNet
from src.nn.backbone.test_resnet import MResNet
from src.optim.ema import ModelEMA
from src.optim.amp import GradScaler
from src.optim.warmup import WarmupLR
from src.data.dataset.dairv2x_detection import DAIRV2XDetection
from src.nn.postprocessor.detr_postprocessor import DetDETRPostProcessor
from src.nn.postprocessor.box_revert import BoxProcessFormat
import cv2
import torchvision.transforms as T

try:
    from batch_inference import postprocess_outputs, draw_boxes, inference_from_preprocessed_image
    USE_BATCH_INFERENCE_LOGIC = True
except ImportError:
    USE_BATCH_INFERENCE_LOGIC = False


def create_backbone(backbone_type: str, **kwargs) -> nn.Module:
    """Factory function to create backbone"""
    if backbone_type.startswith('presnet'):
        depth_match = re.search(r'(\d+)', backbone_type)
        if depth_match:
            depth = int(depth_match.group(1))
        else:
            raise ValueError(f"Cannot parse depth from backbone type {backbone_type}")
        
        default_params = {
            'depth': depth,
            'variant': 'd',
            'return_idx': [1, 2, 3],
            'freeze_at': 0,
            'freeze_norm': True,
            'pretrained': False
        }
        default_params.update(kwargs)
        return PResNet(**default_params)
    
    elif backbone_type.startswith('hgnetv2'):
        name_map = {'hgnetv2_l': 'L', 'hgnetv2_x': 'X', 'hgnetv2_h': 'H'}
        if backbone_type not in name_map:
            raise ValueError(f"Unsupported HGNetv2 type: {backbone_type}")
        
        default_params = {
            'name': name_map[backbone_type],
            'return_idx': [1, 2, 3],
            'freeze_at': 0,
            'freeze_norm': True,
            'pretrained': False
        }
        default_params.update(kwargs)
        return HGNetv2(**default_params)
    
    # CSPResNet config
    elif backbone_type.startswith('cspresnet'):
        name_map = {'cspresnet_s': 's', 'cspresnet_m': 'm', 'cspresnet_l': 'l', 'cspresnet_x': 'x'}
        if backbone_type not in name_map:
            raise ValueError(f"Unsupported CSPResNet type: {backbone_type}")
        
        default_params = {
            'name': name_map[backbone_type],
            'return_idx': [1, 2, 3],
            'pretrained': False
        }
        default_params.update(kwargs)
        return CSPResNet(**default_params)
    
    # CSPDarkNet config
    elif backbone_type == 'cspdarknet':
        default_params = {'return_idx': [2, 3, -1]}
        default_params.update(kwargs)
        return CSPDarkNet(**default_params)
    
    # Modified ResNet
    elif backbone_type == 'mresnet':
        default_params = {'num_blocks': [2, 2, 2, 2]}
        default_params.update(kwargs)
        return MResNet(**default_params)
    
    else:
        raise ValueError(f"Unsupported backbone type: {backbone_type}")






class DSETRTDETR(nn.Module):
    """DSET (Dual-Sparse Expert Transformer) RT-DETR model.
    
    Architecture Design:
    1. Shared Backbone: Extracts multi-scale features
    2. DSET Encoder (Dual-Sparse):
       - Token Pruning: Prunes redundant tokens
       - Patch-MoE: Sparse expert processing for spatial features
    3. MoE Decoder: MoELayer in FFN
    4. Unified Output: Directly outputs detection results
    """
    
    def __init__(self, hidden_dim: int = 256,
                 decoder_hidden_dim: Optional[int] = None,
                 num_queries: int = 300, top_k: int = 2, backbone_type: str = "presnet34",
                 num_decoder_layers: int = 3, encoder_in_channels: list = None, 
                 encoder_expansion: float = 1.0, num_experts: int = 6,
                 num_encoder_layers: int = 1,
                 use_encoder_idx: list = None,
                 token_keep_ratio: Union[float, Dict[int, float]] = None,
                 token_pruning_warmup_epochs: int = 10,
                 encoder_moe_num_experts: int = 4,
                 encoder_moe_top_k: int = 2,
                 # MoE weight config
                 decoder_moe_balance_weight: float = None,
                 encoder_moe_balance_weight: float = None,
                 moe_balance_warmup_epochs: int = 0,
                # CASS (Context-Aware Soft Supervision) config
                use_cass: bool = False,
                cass_loss_weight: float = 0.2,
                cass_expansion_ratio: float = 0.3,
                cass_min_size: float = 1.0,
                cass_warmup_epochs: int = 3,
                # CASS Loss config
                cass_loss_type: str = 'vfl',  # 'focal' or 'vfl'
                cass_focal_alpha: float = 0.75,
                cass_focal_beta: float = 2.0,
                # MoE noise_std config
                moe_noise_std: float = 0.1):
        """Initialize DSET RT-DETR model.
        
        Args:
            hidden_dim: Encoder hidden dimension
            decoder_hidden_dim: Decoder hidden dimension (after ASB-Gate)
            num_queries: Number of queries
            top_k: Top-K experts for router
            backbone_type: Backbone type
            num_decoder_layers: Number of decoder layers
            encoder_in_channels: Encoder input channels
            encoder_expansion: Encoder expansion parameter
            num_experts: Number of decoder experts (required)
            num_encoder_layers: Number of encoder transformer layers
            use_encoder_idx: Which feature pyramid levels (P3, P4, P5) to process with Transformer Encoder
            token_keep_ratio: Patch retention ratio, can be float (uniform) or dict mapping layer index to ratio (e.g., {2: 0.9})
            token_pruning_warmup_epochs: Pruning warmup epochs
            encoder_moe_num_experts: Number of Encoder-MoE experts
            encoder_moe_top_k: Encoder-MoE top-k
            decoder_moe_balance_weight: Decoder MoE balance loss weight
            encoder_moe_balance_weight: Encoder MoE balance loss weight
            moe_balance_warmup_epochs: Number of epochs before applying MOE balance loss (default: 0)
            use_cass: Whether to use Context-Aware Soft Supervision for token pruning
            cass_loss_weight: CASS loss weight
            cass_expansion_ratio: Context band expansion ratio (0.2-0.8)
            cass_min_size: Minimum box size on feature map (protects small objects)
            cass_loss_type: Loss type ('focal' for Focal Loss, 'vfl' for Varifocal Loss)
            cass_focal_alpha: Focal/VFL alpha parameter (positive sample weight)
            cass_focal_beta: Focal/VFL beta/gamma parameter (hard example mining strength)
            
        Note:
            - Encoder-MoE and Token-level Pruning are always enabled (DSET core features)
            - No need to configure use_encoder_moe and use_token_pruning
        """
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.decoder_hidden_dim = decoder_hidden_dim or hidden_dim
        self.num_queries = num_queries
        self.top_k = top_k
        self.backbone_type = backbone_type
        self.num_decoder_layers = num_decoder_layers
        
        # Encoderé…ç½®
        self.encoder_in_channels = encoder_in_channels
        self.encoder_expansion = encoder_expansion
        self.num_encoder_layers = num_encoder_layers
        self.use_encoder_idx = use_encoder_idx
        
        # DSETåŒç¨€ç–é…ç½®ï¼ˆPatch-MoE å¿…ç„¶å¯ç”¨ï¼Œæ— éœ€å­˜å‚¨ï¼‰
        self.token_keep_ratio = token_keep_ratio
        self.token_pruning_warmup_epochs = token_pruning_warmup_epochs
        self.encoder_moe_num_experts = encoder_moe_num_experts
        self.encoder_moe_top_k = encoder_moe_top_k
        
        # CASS configuration
        self.use_cass = use_cass
        self.cass_loss_weight = cass_loss_weight
        self.cass_expansion_ratio = cass_expansion_ratio
        self.cass_min_size = cass_min_size
        self.cass_warmup_epochs = cass_warmup_epochs
        # CASS Loss configuration
        self.cass_loss_type = cass_loss_type
        self.cass_focal_alpha = cass_focal_alpha
        self.cass_focal_beta = cass_focal_beta
        
        # MoE noise_std configuration
        self.moe_noise_std = moe_noise_std
        
        # MoEå’ŒToken Pruningæƒé‡é…ç½®
        if decoder_moe_balance_weight is not None:
            self.decoder_moe_balance_weight = decoder_moe_balance_weight
        if encoder_moe_balance_weight is not None:
            self.encoder_moe_balance_weight = encoder_moe_balance_weight
        
        # MOE Balance Warmupé…ç½®ï¼šåœ¨å‰Nä¸ªepochå†…ä¸åº”ç”¨MOEå¹³è¡¡æŸå¤±ï¼Œè®©ä¸“å®¶è‡ªç„¶åˆ†åŒ–
        self.moe_balance_warmup_epochs = moe_balance_warmup_epochs
        
        # è®¾ç½®ä¸“å®¶æ•°é‡
        self.num_experts = num_experts
        
        # Current epoch for warmup control (Token Pruning Loss and CASS Loss)
        self.current_epoch = 0
        
        # ========== Shared Components ==========
        self.backbone = self._build_backbone()
        self.encoder = self._build_encoder()
        self.asb_gate = ASBGate(
            in_channels=self.hidden_dim,
            out_channels=self.decoder_hidden_dim,
            mid_channels=max(1, self.hidden_dim // 4)
        )
        
        # ========== Fine-grained MoE Decoder ==========
        # Use passed decoder layers argument
        
        self.decoder = RTDETRTransformerv2(
            num_classes=8,  # 8 classes
            hidden_dim=self.decoder_hidden_dim,
            num_queries=num_queries,
            num_layers=num_decoder_layers,
            nhead=8,
            dim_feedforward=1024,
            dropout=0.1,
            activation='relu',
            feat_channels=[self.decoder_hidden_dim, self.decoder_hidden_dim, self.decoder_hidden_dim],
            feat_strides=[8, 16, 32],
            num_levels=3,
            # Fine-grained MoE config
            use_moe=True,
            num_experts=self.num_experts,
            moe_top_k=top_k,
            moe_noise_std=self.moe_noise_std
        )
        
        print(f"âœ“ MoE Decoder config: {num_decoder_layers} layers, {self.num_experts} experts, top_k={top_k}")
        
        # RT-DETR Loss
        self.detr_criterion = self._build_detr_criterion()
    
    def set_epoch(self, epoch: int):
        """Set current epoch for warmup control.
        
        Args:
            epoch: Current training epoch
        """
        self.current_epoch = epoch
        # Also update encoder's epoch (for token pruning mechanism)
        if hasattr(self, 'encoder') and hasattr(self.encoder, 'set_epoch'):
            self.encoder.set_epoch(epoch)
        
    def _build_backbone(self) -> nn.Module:
        """Build backbone."""
        return create_backbone(self.backbone_type)
    
    def _build_encoder(self) -> nn.Module:
        """Build encoder - Supports DSET dual-sparse mechanism."""
        # Support Shared MoE
        
        return HybridEncoder(
            in_channels=self.encoder_in_channels,
            feat_strides=[8, 16, 32],
            hidden_dim=self.hidden_dim,
            use_encoder_idx=self.use_encoder_idx,
            num_encoder_layers=self.num_encoder_layers,
            expansion=self.encoder_expansion,
            nhead=8,
            dropout=0.0,
            act='silu',
            # DSET dual-sparse params
            token_keep_ratio=self.token_keep_ratio,
            token_pruning_warmup_epochs=self.token_pruning_warmup_epochs,
            encoder_moe_num_experts=self.encoder_moe_num_experts,
            encoder_moe_top_k=self.encoder_moe_top_k,
            # CASS parameters
            use_cass=self.use_cass,
            cass_expansion_ratio=self.cass_expansion_ratio,
            cass_min_size=self.cass_min_size,
            cass_decay_type='gaussian',
            # CASS Loss parameters
            cass_loss_type=self.cass_loss_type,
            cass_focal_alpha=self.cass_focal_alpha,
            cass_focal_beta=self.cass_focal_beta,
            # MoE noise_std parameter
            moe_noise_std=self.moe_noise_std
        )
    
    def _build_detr_criterion(self) -> RTDETRCriterionv2:
        """Build RT-DETR loss function."""
        matcher = HungarianMatcher(
            weight_dict={'cost_class': 2, 'cost_bbox': 5, 'cost_giou': 2},
            use_focal_loss=False,
            alpha=0.25,
            gamma=2.0
        )
        
        # Main loss weights
        main_weight_dict = {
            'loss_vfl': 1.0,
            'loss_bbox': 5.0,
            'loss_giou': 2.0
        }
        
        # Dynamically read decoder layers
        num_decoder_layers = self.num_decoder_layers
        aux_weight_dict = {}
        for i in range(num_decoder_layers - 1):  # First N-1 layers
            aux_weight_dict[f'loss_vfl_aux_{i}'] = 1.0
            aux_weight_dict[f'loss_bbox_aux_{i}'] = 5.0
            aux_weight_dict[f'loss_giou_aux_{i}'] = 2.0
        
        # Encoder auxiliary loss (usually 1 layer)
        aux_weight_dict['loss_vfl_enc_0'] = 1.0
        aux_weight_dict['loss_bbox_enc_0'] = 5.0
        aux_weight_dict['loss_giou_enc_0'] = 2.0
        
        # Denoising auxiliary loss
        num_denoising_layers = num_decoder_layers
        for i in range(num_denoising_layers):
            aux_weight_dict[f'loss_vfl_dn_{i}'] = 1.0
            aux_weight_dict[f'loss_bbox_dn_{i}'] = 5.0
            aux_weight_dict[f'loss_giou_dn_{i}'] = 2.0
        
        # åˆå¹¶æ‰€æœ‰æƒé‡
        weight_dict = {**main_weight_dict, **aux_weight_dict}
        
        criterion = RTDETRCriterionv2(
            matcher=matcher,
            weight_dict=weight_dict,
            losses=['vfl', 'boxes'],
            alpha=0.75,
            gamma=2.0,
            num_classes=8,  # 8ç±»ï¼šCar, Truck, Van, Bus, Pedestrian, Cyclist, Motorcyclist, Trafficcone
            boxes_weight_format=None,
            share_matched_indices=False
        )
        
        return criterion
    
    
    def forward(self, images: torch.Tensor, 
                targets: Optional[List[Dict]] = None) -> Dict[str, torch.Tensor]:
        """å‰å‘ä¼ æ’­ã€‚
        
        Args:
            images: [B, C, H, W] è¾“å…¥å›¾åƒ
            targets: è®­ç»ƒç›®æ ‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            Dict: åŒ…å«æ£€æµ‹ç»“æœå’ŒæŸå¤±çš„å­—å…¸
        """
        # å…±äº«ç‰¹å¾æå–
        backbone_features = self.backbone(images)
        
        # DSET Encoderï¼ˆåŒç¨€ç–ï¼šPatch-level Pruning + Patch-MoEï¼‰
        # âš ï¸ Patch-MoE å’Œ Patch-level Pruning å¿…ç„¶å¯ç”¨ï¼ˆDSETæ ¸å¿ƒç‰¹æ€§ï¼‰
        encoder_features, encoder_info = self.encoder(backbone_features, return_encoder_info=True)
        encoder_features = [self.asb_gate(feat) for feat in encoder_features]
        
        # MoE Decoderå‰å‘ï¼ˆå†…éƒ¨è‡ªåŠ¨å¤„ç†è·¯ç”±å’Œä¸“å®¶èåˆï¼‰
        decoder_output = self.decoder(encoder_features, targets)
        
        # æ„å»ºè¾“å‡ºå­—å…¸
        output = {
            'pred_logits': decoder_output.get('pred_logits'),
            'pred_boxes': decoder_output.get('pred_boxes'),
            'bboxes': decoder_output.get('pred_boxes'),
            'class_scores': decoder_output.get('pred_logits'),
        }
        
        if targets is not None:
            # è®¡ç®—æ£€æµ‹æŸå¤±ï¼ˆè®­ç»ƒå’ŒéªŒè¯éƒ½éœ€è¦ï¼‰
            detection_loss_dict = self.detr_criterion(decoder_output, targets)
            detection_loss = sum(v for v in detection_loss_dict.values() 
                               if isinstance(v, torch.Tensor))
            
            # ========== DSETåŒç¨€ç–æŸå¤± ==========
            # 1. Decoder MoEè´Ÿè½½å‡è¡¡æŸå¤±ï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
            if self.training:
                decoder_moe_loss = decoder_output.get('moe_load_balance_loss', 
                                                     torch.tensor(0.0, device=images.device))
            else:
                decoder_moe_loss = torch.tensor(0.0, device=images.device)
            
            # 2. Encoder Patch-MoEæŸå¤±ï¼ˆä»…è®­ç»ƒæ—¶ï¼‰- è´Ÿè½½å‡è¡¡æŸå¤±
            # âš ï¸ Patch-MoE é»˜è®¤å¯ç”¨ï¼ŒDSETæ ¸å¿ƒç‰¹æ€§
            if self.training:
                encoder_moe_loss_dict = self.encoder.get_encoder_moe_loss(encoder_info)
                encoder_moe_loss = encoder_moe_loss_dict['balance_loss']
                if encoder_moe_loss.device != images.device:
                    encoder_moe_loss = encoder_moe_loss.to(images.device)
            else:
                encoder_moe_loss = torch.tensor(0.0, device=images.device)
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ MOE Balance Warmup æœŸé—´
            # åœ¨ warmup æœŸé—´ï¼ŒMOE å¹³è¡¡æŸå¤±æƒé‡è®¾ä¸º 0ï¼Œè®©ä¸“å®¶è‡ªç„¶åˆ†åŒ–
            in_moe_balance_warmup = self.current_epoch < self.moe_balance_warmup_epochs
            
            # Decoder MoEæƒé‡
            if in_moe_balance_warmup:
                decoder_moe_weight = 0.0
            elif hasattr(self, 'decoder_moe_balance_weight'):
                decoder_moe_weight = self.decoder_moe_balance_weight
            else:
                decoder_moe_weight = 0.05
            
            # Encoder Patch-MoEæƒé‡
            if in_moe_balance_warmup:
                encoder_moe_weight = 0.0
            elif hasattr(self, 'encoder_moe_balance_weight'):
                encoder_moe_weight = self.encoder_moe_balance_weight
            else:
                # é»˜è®¤å€¼ï¼š0.05ï¼ˆä¸­ç­‰å€¼ï¼‰
                encoder_moe_weight = 0.05
            
            # 3. CASS (Context-Aware Soft Supervision) Loss
            # Provides explicit supervision for token importance predictor using GT bboxes
            # âš ï¸ å…³é”®ï¼šCASS Loss å¿…é¡»åœ¨ Warmup æœŸé—´ä¸º 0ï¼Œé¿å…æ‹Ÿåˆéšæœºåˆå§‹åŒ–çš„å™ªå£°åˆ†æ•°
            # CASS å¯ä»¥æ¯” Token Pruning æ›´æ—©ä»‹å…¥ï¼ˆç¬¬ 3ä¸ª epochï¼‰ï¼Œå› ä¸ºé‡è¦æ€§é¢„æµ‹å™¨éœ€è¦æ›´æ—©çš„ç›‘ç£ä¿¡å·
            if self.use_cass and self.training and encoder_info and targets is not None and \
               self.current_epoch >= self.cass_warmup_epochs:
                importance_scores_list = encoder_info.get('importance_scores_list', [])
                feat_shapes_list = encoder_info.get('feat_shapes_list', [])
                
                if importance_scores_list and feat_shapes_list and \
                   hasattr(self.encoder, 'shared_token_pruner') and self.encoder.shared_token_pruner:
                    cass_loss = torch.tensor(0.0, device=images.device)
                    
                    # Get image shape (assuming all images in batch have same size)
                    img_shape = (images.shape[2], images.shape[3])  # (H, W)
                    
                    # Extract gt_bboxes from targets
                    # Note: boxes from _collate_fn are always normalized (cx, cy, w, h in [0, 1])
                    gt_bboxes = []
                    for t in targets:
                        if t is not None and 'boxes' in t:
                            boxes = t['boxes']
                            # Convert normalized (cx, cy, w, h) to absolute (x1, y1, x2, y2)
                            if boxes.numel() > 0:
                                boxes_abs = boxes.clone()
                                cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
                                # è½¬æ¢ä¸ºå½’ä¸€åŒ–çš„ (x1, y1, x2, y2)
                                boxes_abs[:, 0] = cx - w / 2  # x1 (å½’ä¸€åŒ–)
                                boxes_abs[:, 1] = cy - h / 2  # y1 (å½’ä¸€åŒ–)
                                boxes_abs[:, 2] = cx + w / 2  # x2 (å½’ä¸€åŒ–)
                                boxes_abs[:, 3] = cy + h / 2  # y2 (å½’ä¸€åŒ–)
                                # è½¬æ¢ä¸ºç»å¯¹åæ ‡
                                boxes_abs[:, 0] *= img_shape[1]  # x1 (ç»å¯¹)
                                boxes_abs[:, 1] *= img_shape[0]  # y1 (ç»å¯¹)
                                boxes_abs[:, 2] *= img_shape[1]  # x2 (ç»å¯¹)
                                boxes_abs[:, 3] *= img_shape[0]  # y2 (ç»å¯¹)
                                gt_bboxes.append(boxes_abs)
                            else:
                                gt_bboxes.append(boxes)
                        else:
                            gt_bboxes.append(torch.empty(0, 4, device=images.device))
                    
                    pruner = self.encoder.shared_token_pruner
                    scores = importance_scores_list[0]
                    feat_shape = feat_shapes_list[0]
                    if pruner.use_cass:
                        cass_loss = pruner.compute_cass_loss_from_info(
                            info={'token_importance_scores': scores},
                            gt_bboxes=gt_bboxes,
                            feat_shape=feat_shape,
                            img_shape=img_shape
                        )
                        if cass_loss.device != images.device:
                            cass_loss = cass_loss.to(images.device)
                else:
                    cass_loss = torch.tensor(0.0, device=images.device)
            else:
                # Warmup æœŸé—´æˆ–æœªå¯ç”¨ CASSï¼šCASS Loss ä¸º 0
                cass_loss = torch.tensor(0.0, device=images.device)
            
            # CASS Loss weight
            cass_weight = self.cass_loss_weight if hasattr(self, 'cass_loss_weight') else 0.2
            
            # æ€»æŸå¤±ï¼šL = L_task + Decoder MoEæŸå¤± + Encoder MoEæŸå¤± + CASSæŸå¤±
            total_loss = detection_loss + \
                        decoder_moe_weight * decoder_moe_loss + \
                        encoder_moe_weight * encoder_moe_loss + \
                        cass_weight * cass_loss
            
            output['detection_loss'] = detection_loss
            output['decoder_moe_loss'] = decoder_moe_loss
            output['encoder_moe_loss'] = encoder_moe_loss
            output['cass_loss'] = cass_loss
            output['moe_load_balance_loss'] = decoder_moe_loss + encoder_moe_loss  # ä¿æŒå‘åå…¼å®¹
            output['total_loss'] = total_loss
            output['loss_dict'] = detection_loss_dict
            
            output['decoder_moe_weight'] = decoder_moe_weight
            output['encoder_moe_balance_weight'] = encoder_moe_weight
            output['cass_weight'] = cass_weight
            
            # æ·»åŠ encoder infoåˆ°è¾“å‡ºï¼ˆç”¨äºç›‘æ§ï¼‰
            if encoder_info:
                output['encoder_info'] = encoder_info
        
        return output


class DSETTrainer:
    """DSET (Dual-Sparse Expert Transformer) è®­ç»ƒå™¨ã€‚
    
    è´Ÿè´£æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€æ£€æŸ¥ç‚¹ç®¡ç†ç­‰åŠŸèƒ½ã€‚
    æ”¯æŒåŒç¨€ç–æœºåˆ¶çš„æ¸è¿›å¼è®­ç»ƒã€‚
    """
    
    def __init__(self, config: Dict, config_file_path: Optional[str] = None):
        """åˆå§‹åŒ–è®­ç»ƒå™¨ã€‚
        
        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
            config_file_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰ï¼Œç”¨äºéªŒè¯
        """
        self.config = config
        self.config_file_path = config_file_path
        
        # å¦‚æœä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ŒéªŒè¯å¿…éœ€çš„é…ç½®é¡¹
        if config_file_path:
            self._validate_config_file()
        
        # å¦‚æœä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œdeviceå¿…é¡»å­˜åœ¨ï¼Œå¦åˆ™æŠ¥é”™
        if config_file_path:
            if 'misc' not in self.config or 'device' not in self.config['misc']:
                raise ValueError(f"é…ç½®æ–‡ä»¶ {config_file_path} ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹: misc.device")
            device_str = self.config['misc']['device']
        else:
            device_str = self.config.get('misc', {}).get('device', 'cuda')
        self.device = torch.device(device_str)
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.best_map = 0.0  # è®°å½•æœ€ä½³mAP
        self.global_step = 0
        # ä»é…ç½®ä¸­è¯»å– resume_from_checkpointï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼ï¼‰
        self.resume_from_checkpoint = self.config.get('resume_from_checkpoint', None)
        if self.resume_from_checkpoint is None and 'checkpoint' in self.config:
            self.resume_from_checkpoint = self.config['checkpoint'].get('resume_from_checkpoint', None)
        
        # æ¢¯åº¦è£å‰ªå‚æ•°ï¼ˆä»é…ç½®è¯»å–ï¼‰
        self.clip_max_norm = self.config.get('training', {}).get('clip_max_norm', 10.0)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._setup_logging()
        self.model = self._create_model()
        
        pretrained_weights = self.config['model'].get('pretrained_weights', None)
        if pretrained_weights:
            self._load_pretrained_weights(self.model, pretrained_weights)
            
        self.train_loader, self.val_loader = self._create_data_loaders()
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        self.warmup_scheduler = self._create_warmup_scheduler()
        self.ema = self._create_ema()
        self.scaler = self._create_scaler()
        
        self.visualizer = TrainingVisualizer(log_dir=self.log_dir, model_type='dset', experiment_name=self.experiment_name)
        self.early_stopping = self._create_early_stopping()
        
        # åˆå§‹åŒ–æ¨ç†ç›¸å…³ç»„ä»¶
        self._setup_inference_components()
        
        # æ¢å¤æ£€æŸ¥ç‚¹
        if self.resume_from_checkpoint:
            self._resume_from_checkpoint()
    
    def _validate_config_file(self):
        """éªŒè¯é…ç½®æ–‡ä»¶æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„é…ç½®é¡¹"""
        required_keys = {
            'model': ['num_experts', 'backbone', 'hidden_dim', 'num_queries', 'num_decoder_layers', 'top_k'],
            'training': ['epochs', 'batch_size', 'pretrained_lr', 'new_lr', 'warmup_epochs'],
            'data': ['data_root'],
            'misc': ['device', 'num_workers']
        }
        
        missing_keys = []
        for section, keys in required_keys.items():
            if section not in self.config:
                missing_keys.append(f"ç¼ºå°‘é…ç½®èŠ‚: {section}")
                continue
            for key in keys:
                if key not in self.config[section]:
                    missing_keys.append(f"{section}.{key}")
        
        if missing_keys:
            error_msg = f"é…ç½®æ–‡ä»¶ {self.config_file_path} ç¼ºå°‘å¿…éœ€çš„é…ç½®é¡¹:\n"
            error_msg += "\n".join(f"  - {key}" for key in missing_keys)
            raise ValueError(error_msg)
    
    def _setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿã€‚"""
        if self.resume_from_checkpoint:
            checkpoint_path = Path(self.resume_from_checkpoint)
            self.log_dir = checkpoint_path.parent
            # ä»ç›®å½•åä¸­æå–å®éªŒåç§°ï¼ˆå»æ‰æ—¶é—´æˆ³éƒ¨åˆ†ï¼‰
            dir_name = self.log_dir.name
            # å‡è®¾æ ¼å¼ä¸º dset6_r50_20240101_120000ï¼Œæå– dset6_r50
            parts = dir_name.rsplit('_', 2)  # åˆ†å‰²æœ€åä¸¤éƒ¨åˆ†ï¼ˆæ—¥æœŸå’Œæ—¶é—´ï¼‰
            if len(parts) >= 2:
                self.experiment_name = '_'.join(parts[:-2]) if len(parts) > 2 else parts[0]
            else:
                self.experiment_name = dir_name
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # ä»é…ç½®ä¸­è·å–backboneç±»å‹ï¼ŒåŠ å…¥åˆ°ç›®å½•åä¸­
            backbone_type = self.config.get('model', {}).get('backbone', 'unknown')
            # ç§»é™¤presnetå‰ç¼€ï¼Œåªä¿ç•™æ•°å­—éƒ¨åˆ†ï¼ˆå¦‚presnet18 -> r18, presnet34 -> r34ï¼‰
            backbone_short = backbone_type.replace('presnet', 'r').replace('pres', 'r') if 'presnet' in backbone_type or 'pres' in backbone_type else backbone_type
            # ä»é…ç½®æ–‡ä»¶è¯»å–encoderå’Œdecoderä¸“å®¶æ•°é‡
            num_decoder_experts = self.config.get('model', {}).get('num_experts', 6)
            dset_config = self.config.get('model', {}).get('dset', {})
            num_encoder_experts = dset_config.get('encoder_moe_num_experts', 4)
            # ç”Ÿæˆå®éªŒåç§°ï¼ˆä¸å¸¦æ—¶é—´æˆ³ï¼‰
            # å¦‚æœencoderå’Œdecoderç›¸åŒï¼Œä½¿ç”¨ dset{num}_{backbone} æ ¼å¼
            # å¦‚æœä¸åŒï¼Œä½¿ç”¨ dset{encoder}{decoder}_{backbone} æ ¼å¼
            if num_encoder_experts == num_decoder_experts:
                self.experiment_name = f"dset{num_decoder_experts}_{backbone_short}"
            else:
                self.experiment_name = f"dset{num_encoder_experts}{num_decoder_experts}_{backbone_short}"
            self.log_dir = Path(f"logs/{self.experiment_name}_{timestamp}")
            self.log_dir.mkdir(parents=True, exist_ok=True)
        
        handlers = [
            logging.FileHandler(self.log_dir / 'training.log', mode='a'),
            logging.StreamHandler()
        ]
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=handlers,
            force=True
        )
        
        self.logger = logging.getLogger(__name__)
        
        if self.resume_from_checkpoint:
            self.logger.info(f"æ¢å¤è®­ç»ƒï¼Œæ—¥å¿—ç›®å½•: {self.log_dir}")
        
        if not self.resume_from_checkpoint:
            with open(self.log_dir / 'config.yaml', 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, default_flow_style=False)
    
    def _create_model(self) -> DSETRTDETR:
        """åˆ›å»ºDSETæ¨¡å‹ï¼ˆæ”¯æŒåŒç¨€ç–ï¼‰ã€‚"""
        # ä»é…ç½®æ–‡ä»¶è¯»å–encoderé…ç½®
        encoder_config = self.config['model']['encoder']
        encoder_in_channels = encoder_config['in_channels']
        encoder_expansion = encoder_config['expansion']
        use_encoder_idx = encoder_config.get('use_encoder_idx', [1, 2])
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–ä¸“å®¶æ•°é‡
        num_experts = self.config['model'].get('num_experts', 6)
        
        # ä»é…ç½®æ–‡ä»¶è¯»å– MoE noise_std
        moe_noise_std = self.config['model'].get('moe_noise_std', 0.1)
        
        # DSETåŒç¨€ç–é…ç½®
        dset_config = self.config['model'].get('dset', {})
        # âš ï¸ æ³¨æ„ï¼šPatch-MoE å’Œ Patch-level Pruning å¿…ç„¶å¯ç”¨ï¼ˆDSETæ ¸å¿ƒç‰¹æ€§ï¼‰ï¼Œæ— éœ€é…ç½®
        token_keep_ratio = dset_config.get('token_keep_ratio', 0.7)
        token_pruning_warmup_epochs = dset_config.get('token_pruning_warmup_epochs', 10)
        encoder_moe_num_experts = dset_config.get('encoder_moe_num_experts', 4)
        encoder_moe_top_k = dset_config.get('encoder_moe_top_k', 2)
        
        # CASS (Context-Aware Soft Supervision) é…ç½®
        use_cass = dset_config.get('use_cass', False)
        cass_loss_weight = dset_config.get('cass_loss_weight', 0.2)
        cass_expansion_ratio = dset_config.get('cass_expansion_ratio', 0.3)
        cass_min_size = dset_config.get('cass_min_size', 1.0)
        cass_warmup_epochs = dset_config.get('cass_warmup_epochs', 3)  # é»˜è®¤ç¬¬ 3 ä¸ª epoch å¼€å§‹
        # CASS Loss é…ç½®
        cass_loss_type = dset_config.get('cass_loss_type', 'vfl')  # 'focal' or 'vfl'
        cass_focal_alpha = dset_config.get('cass_focal_alpha', 0.75)
        cass_focal_beta = dset_config.get('cass_focal_beta', 2.0)
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–MoEæƒé‡
        decoder_moe_balance_weight = self.config.get('training', {}).get('decoder_moe_balance_weight', None)
        encoder_moe_balance_weight = self.config.get('training', {}).get('encoder_moe_balance_weight', None)
        # MOE Balance Warmup: åœ¨å‰Nä¸ªepochå†…ä¸åº”ç”¨MOEå¹³è¡¡æŸå¤±
        moe_balance_warmup_epochs = self.config.get('training', {}).get('moe_balance_warmup_epochs', 0)
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–num_encoder_layersï¼Œé»˜è®¤ä¸º1
        num_encoder_layers = self.config.get('model', {}).get('encoder', {}).get('num_encoder_layers', 1)
        decoder_hidden_dim = self.config['model'].get('decoder_hidden_dim', None)
        
        model = DSETRTDETR(
            hidden_dim=self.config['model']['hidden_dim'],
            decoder_hidden_dim=decoder_hidden_dim,
            num_queries=self.config['model']['num_queries'],
            top_k=self.config['model']['top_k'],
            backbone_type=self.config['model']['backbone'],
            num_decoder_layers=self.config['model']['num_decoder_layers'],
            encoder_in_channels=encoder_in_channels,
            encoder_expansion=encoder_expansion,
            num_experts=num_experts,
            num_encoder_layers=num_encoder_layers,
            use_encoder_idx=use_encoder_idx,
            # DSETåŒç¨€ç–å‚æ•°ï¼ˆPatch-MoE å¿…ç„¶å¯ç”¨ï¼Œæ— éœ€ä¼ é€’ï¼‰
            token_keep_ratio=token_keep_ratio,
            token_pruning_warmup_epochs=token_pruning_warmup_epochs,
            encoder_moe_num_experts=encoder_moe_num_experts,
            encoder_moe_top_k=encoder_moe_top_k,
            # MoEæƒé‡é…ç½®
            decoder_moe_balance_weight=decoder_moe_balance_weight,
            encoder_moe_balance_weight=encoder_moe_balance_weight,
            moe_balance_warmup_epochs=moe_balance_warmup_epochs,
            # CASSé…ç½®
            use_cass=use_cass,
            cass_loss_weight=cass_loss_weight,
            cass_expansion_ratio=cass_expansion_ratio,
            cass_min_size=cass_min_size,
            cass_warmup_epochs=cass_warmup_epochs,
            # CASS Lossé…ç½®
            cass_loss_type=cass_loss_type,
            cass_focal_alpha=cass_focal_alpha,
            cass_focal_beta=cass_focal_beta,
            # MoE noise_std é…ç½®
            moe_noise_std=moe_noise_std
        )
        
        # [ä¿®å¤] ç§»é™¤ _create_model å†…éƒ¨çš„åŠ è½½é€»è¾‘ï¼Œç»Ÿä¸€åœ¨ DSETTrainer.__init__ ä¸­å¤„ç†
        
        model = model.to(self.device)
        
        # å¯ç”¨GPUä¼˜åŒ–è®¾ç½®
        if torch.cuda.is_available():
            # å¯ç”¨cudnn benchmarkä»¥åŠ é€Ÿå·ç§¯æ“ä½œï¼ˆè¾“å…¥å°ºå¯¸å›ºå®šæ—¶ï¼‰
            torch.backends.cudnn.benchmark = True
            # å¯ç”¨TensorFloat-32ï¼ˆRTX 5090æ”¯æŒï¼Œå¯åŠ é€ŸæŸäº›æ“ä½œï¼‰
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            self.logger.info("âœ“ å·²å¯ç”¨GPUä¼˜åŒ–: cudnn.benchmark=True, TF32=True")
        
        # è·å–å®é™…çš„num_encoder_layersç”¨äºæ—¥å¿—è¾“å‡º
        num_encoder_layers = self.config.get('model', {}).get('encoder', {}).get('num_encoder_layers', 1)
        
        self.logger.info(f"âœ“ åˆ›å»ºDSET RT-DETRæ¨¡å‹")
        self.logger.info(f"  Decoderä¸“å®¶æ•°é‡: {model.num_experts}")
        self.logger.info(f"  Backbone: {model.backbone_type}")
        self.logger.info(f"  Encoder: in_channels={encoder_in_channels}, expansion={encoder_expansion}, num_layers={num_encoder_layers}")
        self.logger.info(f"  Encoder MoEè®¾è®¡: å±‚é—´å…±äº«")
        self.logger.info(f"  åŒç¨€ç–é…ç½®ï¼ˆDSETæ ¸å¿ƒç‰¹æ€§ï¼Œå¿…ç„¶å¯ç”¨ï¼‰:")
        self.logger.info(f"    - Encoder-MoE: å¯ç”¨ (experts={encoder_moe_num_experts}, top_k={encoder_moe_top_k})")
        self.logger.info(f"    - Patch-level Pruning: å¯ç”¨ï¼ˆä¸ Patch-MoE å…¼å®¹ï¼‰")
        self.logger.info(f"      â†’ keep_ratio={token_keep_ratio}, warmup={token_pruning_warmup_epochs}")
        self.logger.info(f"  æŸå¤±æƒé‡é…ç½®:")
        self.logger.info(f"    - CASS Supervision: {use_cass} (loss_type={cass_loss_type}, weight={cass_loss_weight}, expansion={cass_expansion_ratio}, min_size={cass_min_size}, warmup={cass_warmup_epochs} epochs)")
        if use_cass:
            self.logger.info(f"      â†’ CASS Loss params: alpha={cass_focal_alpha}, beta={cass_focal_beta}")
        self.logger.info(f"    - Decoder MoE: {decoder_moe_balance_weight if decoder_moe_balance_weight else 'auto'}")
        self.logger.info(f"    - Encoder MoE: {encoder_moe_balance_weight if encoder_moe_balance_weight else 'auto'}")
        if moe_balance_warmup_epochs > 0:
            self.logger.info(f"    - MOE Balance Warmup: {moe_balance_warmup_epochs} epochs (å»¶è¿Ÿå¹³è¡¡ç­–ç•¥ï¼šå‰{moe_balance_warmup_epochs}ä¸ªepochä¸åº”ç”¨MOEå¹³è¡¡æŸå¤±)")
        
        return model
    
    def _load_pretrained_weights(self, model: DSETRTDETR, pretrained_path: str) -> None:
        """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæƒé‡
        
        Args:
            pretrained_path: æœ¬åœ°æƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ 'pretrained/rtdetrv2_r50vd_6x_coco_ema.pth'ï¼‰
        """
        try:
            pretrained_file = Path(pretrained_path)
            if not pretrained_file.exists():
                self.logger.warning(f"é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {pretrained_path}")
                self.logger.info("å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")
                return
            
            self.logger.info(f"ä»æœ¬åœ°æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
            checkpoint = torch.load(pretrained_file, map_location='cpu', weights_only=False)
            
            # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
            if isinstance(checkpoint, dict):
                if 'ema' in checkpoint and 'module' in checkpoint['ema']:
                    # EMAæ ¼å¼: {'ema': {'module': {...}}}
                    state_dict = checkpoint['ema']['module']
                    self.logger.info("âœ“ æ£€æµ‹åˆ°EMA checkpointæ ¼å¼")
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                elif 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            # è¿‡æ»¤æ‰ç±»åˆ«ç›¸å…³å‚æ•°ï¼ˆå½¢çŠ¶ä¸åŒ¹é…ï¼‰
            filtered_state_dict = {}
            skipped_class_params = 0
            
            for k, v in state_dict.items():
                # è·³è¿‡ç±»åˆ«ç›¸å…³çš„å‚æ•°ï¼ˆè¿™äº›å‚æ•°çš„å½¢çŠ¶ä¼šä¸åŒ¹é…ï¼‰
                if any(keyword in k for keyword in ['class_embed', 'score_head', 'denoising_class_embed']):
                    skipped_class_params += 1
                    continue
                filtered_state_dict[k] = v
            
            # [ä¼˜åŒ–] æ‰‹åŠ¨é€ä¸ªå‚æ•°åŠ è½½ï¼Œè§£å†³ç»´åº¦ä¸åŒ¹é…å¯¼è‡´æ•´ä¸ªåŠ è½½å¤±è´¥çš„é—®é¢˜
            model_state_dict = model.state_dict()
            load_count = 0
            mismatch_count = 0
            
            final_state_dict = {}
            for k, v in filtered_state_dict.items():
                if k in model_state_dict:
                    if v.shape == model_state_dict[k].shape:
                        final_state_dict[k] = v
                        load_count += 1
                    else:
                        mismatch_count += 1
                        # åªåœ¨è°ƒè¯•çº§åˆ«æ‰“å°ï¼Œé¿å…æ—¥å¿—åˆ·å±
                        # self.logger.debug(f"ç»´åº¦ä¸åŒ¹é…è·³è¿‡: {k} {v.shape} -> {model_state_dict[k].shape}")
                else:
                    # é¢„è®­ç»ƒæƒé‡ä¸­æœ‰ï¼Œä½†æ¨¡å‹ä¸­æ²¡æœ‰ï¼ˆå¯èƒ½æ˜¯ unexpected_keysï¼‰
                    pass
            
            # ä½¿ç”¨ strict=False åŠ è½½åŒ¹é…çš„éƒ¨åˆ†
            missing_keys, unexpected_keys = model.load_state_dict(final_state_dict, strict=False)
            
            self.logger.info(f"âœ“ æˆåŠŸåŠ è½½æƒé‡å‚æ•°: {load_count} ä¸ª")
            if mismatch_count > 0:
                self.logger.info(f"  - ç»´åº¦ä¸åŒ¹é…è·³è¿‡: {mismatch_count} ä¸ªå‚æ•° (ä¸»è¦é›†ä¸­åœ¨ 128 ç»´çš„ Decoder éƒ¨åˆ†)")
            
            # ç»Ÿè®¡å„éƒ¨åˆ†çš„åŠ è½½æƒ…å†µ
            backbone_loaded = sum(1 for k in final_state_dict.keys() if 'backbone' in k)
            encoder_loaded = sum(1 for k in final_state_dict.keys() if 'encoder' in k)
            decoder_loaded = sum(1 for k in final_state_dict.keys() if 'decoder' in k)
            
            self.logger.info(f"  - Backbone åŠ è½½: {backbone_loaded} ä¸ªå‚æ•°")
            self.logger.info(f"  - Encoder åŠ è½½: {encoder_loaded} ä¸ªå‚æ•°")
            self.logger.info(f"  - Decoder åŠ è½½: {decoder_loaded} ä¸ªå‚æ•° (é¢„è®¡è¾ƒå°‘)")
            
            if len(missing_keys) > 0:
                self.logger.info(f"  - æœªåŠ è½½çš„å‚æ•° (Missing): {len(missing_keys)} ä¸ª (åŒ…å« ASB-Gate ç­‰æ–°ç»„ä»¶)")
                
        except Exception as e:
            self.logger.error(f"âœ— åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
            self.logger.info("å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")
    
    def _create_data_loaders(self) -> Tuple[DataLoader, DataLoader]:
        """åˆ›å»ºåˆå§‹æ•°æ®åŠ è½½å™¨ã€‚"""
        # åˆå§‹åŠ è½½æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦å¤„äºé¢„çƒ­æœŸ
        token_pruning_warmup_epochs = self.config['model'].get('dset', {}).get('token_pruning_warmup_epochs', 10)
        base_batch_size = self.config['training']['batch_size']
        
        # ğŸš€ åŠ¨æ€ Batch Size ç­–ç•¥ï¼š
        # - é¢„çƒ­æœŸ (0-9 è½®)ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ batch_size
        # - é¢„çƒ­æœŸåï¼šç¿» 2 å€
        warmup_batch_size = base_batch_size
        if self.current_epoch < token_pruning_warmup_epochs:
            current_batch_size = warmup_batch_size
        else:
            current_batch_size = warmup_batch_size * 2  # ç¿» 2 å€
        
        self.logger.info(f"ğŸ“¦ åˆå§‹åŒ–è®­ç»ƒ: epoch={self.current_epoch}, å½“å‰ä½¿ç”¨ batch_size={current_batch_size} (é¢„çƒ­æœŸ={warmup_batch_size}, é…ç½®æ–‡ä»¶åŸºå‡†={base_batch_size})")
        
        train_loader = self._build_train_loader(current_batch_size)
        
        # éªŒè¯é›†é€šå¸¸ä¸å‰ªææˆ–ä¿æŒç¨³å®šï¼Œå¯ä»¥ä½¿ç”¨å›ºå®š batch_sizeï¼ˆæˆ–è€…ä¹Ÿéšä¹‹è°ƒæ•´ï¼‰
        val_dataset = DAIRV2XDetection(
            data_root=self.config['data']['data_root'],
            split='val',
            aug_brightness=0.0,
            aug_contrast=0.0,
            aug_saturation=0.0,
            aug_hue=0.0,
            aug_color_jitter_prob=0.0
        )
        self.val_dataset = val_dataset
        
        num_workers = self.config.get('misc', {}).get('num_workers', 16)
        pin_memory = self.config.get('misc', {}).get('pin_memory', True)
        prefetch_factor = self.config.get('misc', {}).get('prefetch_factor', 4)
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=base_batch_size, # éªŒè¯é›†å¯ä»¥ä½¿ç”¨åŸºå‡†å°ºå¯¸
            shuffle=False,
            num_workers=num_workers,
            collate_fn=self._collate_fn,
            pin_memory=pin_memory,
            persistent_workers=True if num_workers > 0 else False,
            prefetch_factor=prefetch_factor if num_workers > 0 else None
        )
        
        return train_loader, val_loader

    def _build_train_loader(self, batch_size: int) -> DataLoader:
        """æ ¹æ®æŒ‡å®šçš„ batch_size æ„å»ºè®­ç»ƒåŠ è½½å™¨ã€‚"""
        aug_config = self.config.get('data_augmentation', {})
        train_dataset = DAIRV2XDetection(
            data_root=self.config['data']['data_root'],
            split='train',
            aug_brightness=aug_config.get('brightness', 0.15),
            aug_contrast=aug_config.get('contrast', 0.15),
            aug_saturation=aug_config.get('saturation', 0.1),
            aug_hue=aug_config.get('hue', 0.05),
            aug_color_jitter_prob=aug_config.get('color_jitter_prob', 0.0),
            aug_crop_min=aug_config.get('crop_min', 0.3),
            aug_crop_max=aug_config.get('crop_max', 1.0),
            aug_flip_prob=aug_config.get('flip_prob', 0.5),
            train_scales_min=aug_config.get('scales_min', 480),
            train_scales_max=aug_config.get('scales_max', 800),
            train_scales_step=aug_config.get('scales_step', 32),
            train_max_size=aug_config.get('max_size', 1333),
            aug_mosaic_prob=aug_config.get('mosaic', 0.0),
            aug_mixup_prob=aug_config.get('mixup', 0.0)
        )
        
        num_workers = self.config.get('misc', {}).get('num_workers', 16)
        pin_memory = self.config.get('misc', {}).get('pin_memory', True)
        prefetch_factor = self.config.get('misc', {}).get('prefetch_factor', 4)
        
        return DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=num_workers,
            collate_fn=self._collate_fn,
            pin_memory=pin_memory,
            persistent_workers=True if num_workers > 0 else False,
            prefetch_factor=prefetch_factor if num_workers > 0 else None
        )
    
    def _collate_fn(self, batch: List[Tuple]) -> Tuple[torch.Tensor, List[Dict]]:
        """æ•°æ®æ•´ç†å‡½æ•°ã€‚"""
        images, targets = zip(*batch)
        
        # 1. å¤„ç†å›¾åƒ (ä¿æŒ Tensor æ ¼å¼)
        if not isinstance(images[0], torch.Tensor):
            processed_images = [T.functional.to_tensor(img) for img in images]
        else:
            processed_images = list(images)

        # 2. è®¡ç®— Batch æœ€å¤§å°ºå¯¸
        sizes = [img.shape[-2:] for img in processed_images]
        stride = 32
        max_h_raw = max(s[0] for s in sizes)
        max_w_raw = max(s[1] for s in sizes)
        # å‘ä¸Šå–æ•´åˆ° 32 å€æ•°
        max_h = (max_h_raw + stride - 1) // stride * stride
        max_w = (max_w_raw + stride - 1) // stride * stride
        
        # 3. åˆ›å»ºç”»å¸ƒå¹¶å¡«å…… (å·¦ä¸Šè§’å¯¹é½)
        batch_images = torch.zeros(len(processed_images), 3, max_h, max_w, 
                                   dtype=processed_images[0].dtype)
        
        for i, img in enumerate(processed_images):
            h, w = img.shape[-2:]
            batch_images[i, :, :h, :w] = img

        # 4. Normalize targets based on final Batch size
        # [ä¿®å¤] åæ ‡å½’ä¸€åŒ–åŸºå‡†ä¸ç»Ÿä¸€é—®é¢˜
        # å…³é”®ï¼šæ‰€æœ‰æ ·æœ¬çš„ boxes å¿…é¡»ä½¿ç”¨ç›¸åŒçš„å½’ä¸€åŒ–åŸºå‡†ï¼ˆbatch çš„æœ€å¤§å°ºå¯¸ max_w, max_hï¼‰
        # è¿™æ ·å»å™ªåˆ†æ”¯å’Œä¸»åˆ†æ”¯æ‰èƒ½ä½¿ç”¨ç›¸åŒçš„å½’ä¸€åŒ–åŸºå‡†ï¼Œé¿å…æ•°å€¼å´©æºƒ
        new_targets = []
        for i, t in enumerate(list(targets)):
            # [FIX] Use deepcopy or clone to ensure original data is not modified
            new_t = t.copy()
            # Must clone, otherwise boxes[:, 0] = ... modifies source tensor
            boxes = new_t['boxes'].clone()
            
            # æ‰‹åŠ¨å½’ä¸€åŒ–ï¼šé™¤ä»¥ max_w å’Œ max_hï¼ˆbatch æœ€å¤§å°ºå¯¸ï¼Œä¸æ˜¯å•ä¸ªæ ·æœ¬å°ºå¯¸ï¼‰
            # æ ¼å¼æ˜¯ cx, cy, w, h
            # xè½´æ•°æ® (cx, w) é™¤ä»¥ max_w
            # yè½´æ•°æ® (cy, h) é™¤ä»¥ max_h
            # æ³¨æ„ï¼šæ‰€æœ‰æ ·æœ¬éƒ½ä½¿ç”¨ç›¸åŒçš„ max_w å’Œ max_hï¼Œç¡®ä¿å½’ä¸€åŒ–åŸºå‡†ç»Ÿä¸€
            boxes[:, 0] = boxes[:, 0] / max_w
            boxes[:, 1] = boxes[:, 1] / max_h
            boxes[:, 2] = boxes[:, 2] / max_w
            boxes[:, 3] = boxes[:, 3] / max_h
            
            # é™åˆ¶æ•°å€¼åœ¨ 0-1 ä¹‹é—´ (é˜²æ­¢æµ®ç‚¹æº¢å‡º)
            boxes = torch.clamp(boxes, 0.0, 1.0)
            
            new_t['boxes'] = boxes
            # [ä¿®å¤] ä¿å­˜å½’ä¸€åŒ–åŸºå‡†ï¼Œç¡®ä¿å»å™ªåˆ†æ”¯å’Œä¸»åˆ†æ”¯ä½¿ç”¨ç›¸åŒçš„åŸºå‡†
            new_t['normalization_size'] = torch.tensor([max_w, max_h], dtype=torch.float32)
            new_targets.append(new_t)
        
        return batch_images, new_targets
    
    def _create_optimizer(self) -> optim.AdamW:
        """åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨åˆ†ç»„å­¦ä¹ ç‡ï¼Œä¸rt-deträ¿æŒä¸€è‡´ï¼‰ã€‚"""
        # è·å–é…ç½®ä¸­çš„å­¦ä¹ ç‡ï¼Œç¡®ä¿æ˜¯æµ®ç‚¹æ•°ç±»å‹
        new_lr = float(self.config['training']['new_lr'])
        pretrained_lr = float(self.config['training']['pretrained_lr'])
        weight_decay = float(self.config['training'].get('weight_decay', 0.0001))
        
        # åˆ†ç»„å‚æ•°ï¼ˆä¸rt-deträ¿æŒä¸€è‡´çš„åˆ†ç»„ç­–ç•¥ï¼‰
        param_groups = []
        
        # å®šä¹‰æ–°å¢ç»“æ„çš„å…³é”®è¯ï¼ˆMoEã€DSETç­‰ï¼‰
        # åŸºäºå®é™…ä»£ç ä¸­çš„æ¨¡å—å‘½åï¼š
        # - decoder.layers.X.adaptive_expert_layer.* (DSETçš„decoder MoE)
        # - encoder.layers.X.encoder_moe_layer.* (DSETçš„encoder Encoder-MoE)
        # - encoder.shared_token_pruner.* (DSETçš„token pruning)
        # - importance_predictor (token pruningä¸­çš„é‡è¦æ€§é¢„æµ‹å™¨)
        new_structure_keywords = [
            'adaptive_expert_layer',  # decoderä¸­çš„MoEå±‚
            'encoder_moe_layer',        # encoderä¸­çš„Encoder-MoEå±‚
            'shared_token_pruner',    # token pruningæ¨¡å—
            'importance_predictor'     # importance predictor
        ]
        
        # 1. é¢„è®­ç»ƒå‚æ•°ç»„ï¼ˆbackboneã€encoderã€decoderçš„æ ‡å‡†å±‚ï¼Œæ’é™¤normå±‚å’Œæ–°å¢ç»“æ„ï¼‰
        pretrained_params = []
        pretrained_names = []
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                # åˆ¤æ–­æ˜¯å¦ä¸ºé¢„è®­ç»ƒéƒ¨åˆ†ï¼ˆbackboneã€encoderã€decoderï¼‰
                is_pretrained = any(part in name for part in ['backbone', 'encoder', 'decoder'])
                # æ’é™¤normå±‚
                is_norm = any(norm in name for norm in ['norm', 'bn', 'gn', 'ln'])
                # æ’é™¤æ–°å¢ç»“æ„ï¼ˆå³ä½¿å®ƒä»¬åœ¨encoder/decoderä¸­ï¼‰
                is_new_structure = any(keyword in name.lower() for keyword in new_structure_keywords)
                
                if is_pretrained and not is_norm and not is_new_structure:
                    pretrained_params.append(param)
                    pretrained_names.append(name)
        
        if pretrained_params:
            param_groups.append({
                'params': pretrained_params,
                'lr': pretrained_lr,
                'weight_decay': weight_decay
            })
            self.logger.info(f"âœ“ é¢„è®­ç»ƒå‚æ•°ç»„: {len(pretrained_params)} ä¸ªå‚æ•°, lr={pretrained_lr}")
        
        # 2. Normå±‚å‚æ•°ï¼ˆæ— weight decayï¼‰
        norm_params = []
        norm_names = []
        for name, param in self.model.named_parameters():
            if param.requires_grad and any(norm in name for norm in ['norm', 'bn', 'gn', 'ln']):
                norm_params.append(param)
                norm_names.append(name)
        
        if norm_params:
            param_groups.append({
                'params': norm_params,
                'lr': new_lr,
                'weight_decay': 0.0  # Normå±‚ä¸ä½¿ç”¨weight decay
            })
            self.logger.info(f"âœ“ Normå±‚å‚æ•°ç»„: {len(norm_params)} ä¸ªå‚æ•°, lr={new_lr}, wd=0")
        
        # 3. æ–°å‚æ•°ç»„ï¼ˆMoEå±‚ã€DSETå±‚ç­‰æ–°å¢ç»“æ„ï¼Œå³ä½¿å®ƒä»¬åœ¨encoder/decoderä¸­ï¼‰
        new_params = []
        new_names = []
        processed_params = set(id(p) for p in pretrained_params + norm_params)
        
        for name, param in self.model.named_parameters():
            if param.requires_grad and id(param) not in processed_params:
                new_params.append(param)
                new_names.append(name)
        
        if new_params:
            param_groups.append({
                'params': new_params,
                'lr': new_lr,
                'weight_decay': weight_decay
            })
            self.logger.info(f"âœ“ æ–°å‚æ•°ç»„: {len(new_params)} ä¸ªå‚æ•°, lr={new_lr}")
        
        optimizer = optim.AdamW(
            param_groups,
            betas=(0.9, 0.999)
        )
        
        return optimizer
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚"""
        scheduler_type = self.config.get('training', {}).get('scheduler', 'cosine')
        
        if scheduler_type == 'cosine':
            # ä»é…ç½®æ–‡ä»¶è¯»å–eta_minï¼Œé»˜è®¤1e-7
            eta_min = self.config.get('training', {}).get('eta_min', 1e-7)
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, 
                T_max=self.config['training']['epochs'],
                eta_min=eta_min
            )
            self.logger.info(f"âœ“ ä½¿ç”¨CosineAnnealingLRè°ƒåº¦å™¨ (eta_min={eta_min})")
        else:
            # MultiStepLR
            milestones = self.config.get('training', {}).get('milestones', [60, 80])
            gamma = float(self.config.get('training', {}).get('gamma', 0.1))
            scheduler = optim.lr_scheduler.MultiStepLR(
                self.optimizer,
                milestones=milestones,
                gamma=gamma
            )
            self.logger.info(f"âœ“ ä½¿ç”¨MultiStepLRè°ƒåº¦å™¨ (milestones={milestones})")
        
        return scheduler
    
    def _create_warmup_scheduler(self) -> WarmupLR:
        """åˆ›å»ºå­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨ã€‚"""
        # ä¿®æ”¹ï¼šwarmup epochsä»é»˜è®¤10æ”¹ä¸º3
        warmup_epochs = self.config.get('training', {}).get('warmup_epochs', 3)
        # ç¡®ä¿warmup_end_lræ˜¯æµ®ç‚¹æ•°
        warmup_end_lr = float(self.config['training']['new_lr'])
        warmup_scheduler = WarmupLR(
            self.optimizer,
            warmup_epochs=warmup_epochs,
            warmup_start_lr=1e-7,
            warmup_end_lr=warmup_end_lr
        )
        self.logger.info(f"âœ“ å­¦ä¹ ç‡é¢„çƒ­ {warmup_epochs} epochs")
        return warmup_scheduler
    
    def _create_ema(self) -> ModelEMA:
        """åˆ›å»ºEMAæ¨¡å‹ã€‚"""
        ema_decay = self.config.get('training', {}).get('ema_decay', 0.9999)
        return ModelEMA(self.model, decay=ema_decay)
    
    def _create_scaler(self):
        """åˆ›å»ºæ··åˆç²¾åº¦è®­ç»ƒå™¨ã€‚"""
        return torch.amp.GradScaler('cuda')
    
    def _create_early_stopping(self) -> Optional[EarlyStopping]:
        """åˆ›å»ºEarly Stoppingã€‚"""
        training_config = self.config.get('training', {})
        patience = training_config.get('early_stopping_patience', None)
        
        if patience is None or patience <= 0:
            self.logger.info("â±ï¸  Early Stopping: æœªå¯ç”¨")
            return None
        
        metric_name = training_config.get('early_stopping_metric', 'mAP_0.5_0.95')
        mode = 'max' if 'mAP' in metric_name or 'AP' in metric_name else 'min'
        
        self.logger.info(f"â±ï¸  Early Stopping: å¯ç”¨ (patience={patience}, metric={metric_name}, mode={mode})")
        
        return EarlyStopping(
            patience=patience,
            mode=mode,
            min_delta=0.0001,
            metric_name=metric_name,
            logger=self.logger
        )
    
    def _setup_inference_components(self) -> None:
        """åˆå§‹åŒ–æ¨ç†ç›¸å…³ç»„ä»¶"""
        # åˆ›å»ºåå¤„ç†å™¨
        self.postprocessor = DetDETRPostProcessor(
            num_classes=8,  # 8ç±»ï¼šCar, Truck, Van, Bus, Pedestrian, Cyclist, Motorcyclist, Trafficcone
            use_focal_loss=True,
            num_top_queries=300,
            box_process_format=BoxProcessFormat.RESIZE
        )
        
        # åˆ›å»ºæ¨ç†è¾“å‡ºç›®å½•
        self.inference_output_dir = self.log_dir / "inference_samples"
        self.inference_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç±»åˆ«åç§°å’Œé¢œè‰²ï¼ˆç”¨äºæ¨ç†å¯è§†åŒ–ï¼‰- 8ç±»æ­£å¼æ£€æµ‹ç±»åˆ«
        self.class_names = [
            "Car", "Truck", "Van", "Bus", "Pedestrian", 
            "Cyclist", "Motorcyclist", "Trafficcone"
        ]
        self.colors = [
            (255, 0, 0),      # Car - çº¢è‰²
            (0, 255, 0),      # Truck - ç»¿è‰²
            (255, 128, 0),    # Van - æ©™è‰²
            (0, 0, 255),      # Bus - è“è‰²
            (255, 255, 0),    # Pedestrian - é»„è‰²
            (255, 0, 255),    # Cyclist - å“çº¢
            (0, 255, 255),    # Motorcyclist - é’è‰²
            (128, 128, 128),  # Trafficcone - ç°è‰²
        ]
        
        self.logger.info(f"æ¨ç†è¾“å‡ºç›®å½•: {self.inference_output_dir}")
    
    def _inference_single_image_from_batch(self, images, targets, batch_idx, image_idx=0, suffix=None):
        """ä»batchä¸­é€‰æ‹©ä¸€å¼ å›¾ç‰‡è¿›è¡Œæ¨ç†å¹¶ä¿å­˜ç»“æœï¼ˆç›´æ¥å¤ç”¨batch_inference.pyçš„é€»è¾‘ï¼‰
        
        Args:
            images: å›¾åƒtensor
            targets: ç›®æ ‡åˆ—è¡¨
            batch_idx: batchç´¢å¼•
            image_idx: å›¾åƒåœ¨batchä¸­çš„ç´¢å¼•
            suffix: æ–‡ä»¶ååç¼€ï¼ˆé»˜è®¤ä½¿ç”¨epochï¼Œå¦‚"epoch_0"æˆ–"best_model"ï¼‰
        """
        try:
            # ä½¿ç”¨EMAæ¨¡å‹è¿›è¡Œæ¨ç†
            self.ema.module.eval()
            
            # é€‰æ‹©batchä¸­çš„æŒ‡å®šå›¾ç‰‡
            single_image = images[image_idx:image_idx+1]  # [1, 3, H, W]
            single_target = targets[image_idx] if image_idx < len(targets) else None
            
            if single_target is None:
                return
            
            # è·å–image_idç”¨äºå‘½åå’ŒæŸ¥æ‰¾åŸå§‹å›¾åƒ
            image_id = single_target['image_id'].item() if 'image_id' in single_target else batch_idx
            
            # è·å–åŸå§‹å›¾åƒè·¯å¾„
            data_root = Path(self.config['data']['data_root'])
            orig_image_path = data_root / "image" / f"{image_id:06d}.jpg"
            
            if not orig_image_path.exists():
                return
            
            # ä½¿ç”¨batch_inference.pyä¸­çš„å‡½æ•°è¿›è¡Œæ¨ç†ï¼ˆå®Œå…¨å¤ç”¨é€»è¾‘ï¼‰
            if USE_BATCH_INFERENCE_LOGIC:
                result_image = inference_from_preprocessed_image(
                    single_image,
                    self.ema.module,
                    self.postprocessor,
                    orig_image_path,
                    conf_threshold=0.3,
                    target_size=1280,  # [FIX] ä¸éªŒè¯é›†ä¸€è‡´ï¼Œä½¿ç”¨ 1280
                    device=str(self.device),
                    class_names=self.class_names,
                    colors=self.colors,
                    verbose=False  # è®­ç»ƒæ—¶ä¸æ‰“å°è°ƒè¯•ä¿¡æ¯
                )
                
                if result_image is None:
                    self.ema.module.train()
                    return
                
                # ä¿å­˜ç»“æœï¼šå›¾ç‰‡å_suffix.jpg
                image_name = orig_image_path.stem
                if suffix is None:
                    suffix = f"epoch_{self.current_epoch}"
                output_filename = f"{image_name}_{suffix}.jpg"
                output_path = self.inference_output_dir / output_filename
                cv2.imwrite(str(output_path), result_image)
            else:
                # å¤‡ç”¨é€»è¾‘ï¼ˆå¦‚æœæ— æ³•å¯¼å…¥batch_inferenceï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼‰
                with torch.no_grad():
                    outputs = self.ema.module(single_image)
                
                # [FIX] æ ¹æ® box_revert æ–‡æ¡£ï¼Œorig_sizes åº”è¯¥æ˜¯ (w, h)
                # ä¹‹å‰çš„ [h, w] å¯èƒ½å¯¼è‡´äº†å·¦ä¸‹è§’åç§»æˆ–å®½é«˜åè½¬
                eval_sizes = torch.tensor([[w, h]], device=self.device)
                
                results = self.postprocessor(outputs, orig_sizes=eval_sizes)
                
                if len(results) > 0:
                    result = results[0]
                    labels = result['labels'].cpu().numpy()
                    boxes = result['boxes'].cpu().numpy()
                    scores = result['scores'].cpu().numpy()
                    
                    mask = scores >= 0.3
                    labels = labels[mask]
                    boxes = boxes[mask]
                    scores = scores[mask]
                    
                    if len(labels) > 0:
                        orig_image = cv2.imread(str(orig_image_path))
                        if orig_image is not None:
                            result_image = draw_boxes(
                                orig_image.copy(), labels, boxes, scores,
                                class_names=self.class_names,
                                colors=self.colors
                            )
                            image_name = orig_image_path.stem
                            if suffix is None:
                                suffix = f"epoch_{self.current_epoch}"
                            output_filename = f"{image_name}_{suffix}.jpg"
                            output_path = self.inference_output_dir / output_filename
                            cv2.imwrite(str(output_path), result_image)
            
            # æ¢å¤è®­ç»ƒæ¨¡å¼
            self.ema.module.train()
            
        except Exception as e:
            # å¦‚æœæ¨ç†å¤±è´¥ï¼Œä¸å½±å“è®­ç»ƒ
            if hasattr(self, 'logger'):
                self.logger.debug(f"æ¨ç†å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")
            if hasattr(self, 'ema') and hasattr(self.ema, 'module'):
                self.ema.module.train()
    
    def _run_inference_on_best_model(self, best_ema_state=None, best_epoch=None):
        """ä½¿ç”¨best_modelè¿è¡Œæ¨ç†ï¼Œè¾“å‡º5å¼ éªŒè¯å›¾åƒçš„æ¨ç†ç»“æœ
        
        Args:
            best_ema_state: best_modelçš„EMAæ¨¡å‹state_dictï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†
            best_epoch: best_modelä¿å­˜æ—¶çš„epochï¼Œç”¨äºæ–‡ä»¶å
        """
        try:
            # ä¿å­˜å½“å‰EMAæ¨¡å‹çŠ¶æ€ï¼ˆæ¨ç†åæ¢å¤ï¼‰
            original_ema_state = None
            if best_ema_state is not None and hasattr(self, 'ema') and self.ema:
                original_ema_state = self.ema.state_dict()
                # åŠ è½½best_modelçš„EMAå‚æ•°
                self.ema.load_state_dict(best_ema_state)
            
            # ä»éªŒè¯æ•°æ®åŠ è½½å™¨ä¸­è·å–ä¸€ä¸ªbatchç”¨äºæ¨ç†
            inference_images, inference_targets = next(iter(self.val_loader))
            inference_images = inference_images.to(self.device)
            inference_targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                                 for k, v in t.items()} for t in inference_targets]
            
            # æ‰“å°å‰5å¼ æ¨ç†ç»“æœ
            batch_size = len(inference_targets)
            num_inference_images = min(5, batch_size)
            # ä½¿ç”¨best_epochä½œä¸ºæ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä½¿ç”¨current_epochï¼ˆå‘åå…¼å®¹ï¼‰
            epoch_for_filename = best_epoch if best_epoch is not None else self.current_epoch
            self.logger.info(f"  ç”Ÿæˆbest_modelæ¨ç†ç»“æœï¼ˆå‰{num_inference_images}å¼ ï¼Œepoch={epoch_for_filename}ï¼‰...")
            
            for img_idx in range(num_inference_images):
                self._inference_single_image_from_batch(
                    inference_images, inference_targets, 0, image_idx=img_idx,
                    suffix=f"best_model_epoch_{epoch_for_filename}"
                )
            
            self.logger.info(f"  âœ“ æ¨ç†ç»“æœå·²ä¿å­˜åˆ°: {self.inference_output_dir}")
            
            # æ¢å¤åŸå§‹EMAæ¨¡å‹çŠ¶æ€
            if original_ema_state is not None and hasattr(self, 'ema') and self.ema:
                self.ema.load_state_dict(original_ema_state)
                
        except Exception as e:
            # å¦‚æœæ¨ç†å¤±è´¥ï¼Œä¸å½±å“è®­ç»ƒï¼Œä½†å°è¯•æ¢å¤EMAçŠ¶æ€
            if hasattr(self, 'logger'):
                self.logger.warning(f"best_modelæ¨ç†å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")
            if original_ema_state is not None and hasattr(self, 'ema') and self.ema:
                try:
                    self.ema.load_state_dict(original_ema_state)
                except:
                    pass
    
    def _save_token_visualization(self, epoch: int) -> None:
        """ä¿å­˜ Token é‡è¦æ€§çƒ­åŠ›å›¾ï¼ˆé€‚é…å…¨å±€å¤šå°ºåº¦å‰ªæï¼‰ã€‚"""
        try:
            viz_dir = self.log_dir / "visualizations" / f"epoch_{epoch}"
            viz_dir.mkdir(parents=True, exist_ok=True)
            self.ema.module.eval()
            
            # è·å–éªŒè¯æ•°æ®
            images, targets = next(iter(self.val_loader))
            B, _, H_tensor, W_tensor = images.shape
            
            with torch.no_grad():
                # æ˜¾å¼ä¼ é€’ targets ç¡®ä¿ forward è¿”å› encoder_info
                outputs = self.ema.module(images.to(self.device), 
                    [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets])
            
            enc_info = outputs.get('encoder_info', {})
            # ğŸš€ æ ¸å¿ƒä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨ HybridEncoder å‡†å¤‡å¥½çš„ layer_wise_heatmaps
            heatmaps_2d_list = enc_info.get('layer_wise_heatmaps', [])
            
            if not heatmaps_2d_list:
                self.logger.warning(f"ğŸ“¸ Epoch {epoch}: å¯è§†åŒ–è·³è¿‡ï¼Œlayer_wise_heatmaps ä¸ºç©ºã€‚")
                return

            # æˆ‘ä»¬é€šå¸¸åªå¯è§†åŒ–åˆ†è¾¨ç‡æœ€é«˜çš„é‚£ä¸€å±‚ (é€šå¸¸æ˜¯ç¬¬ä¸€å±‚ S4)
            # heatmaps_2d_list é‡Œçš„å½¢çŠ¶æ˜¯ [B, 1, H_i, W_i]
            scores_prob = torch.sigmoid(heatmaps_2d_list[0]) 
            h_feat, w_feat = scores_prob.shape[2], scores_prob.shape[3]
            
            for i in range(min(3, len(targets))):
                img_id = targets[i]['image_id'].item()
                data_root = Path(self.config['data']['data_root'])
                
                # å°è¯•å‘½ååŒ¹é…
                possible_paths = [
                    data_root / "image" / f"{img_id:06d}.jpg",
                    data_root / "image" / f"{img_id}.jpg"
                ]
                orig_img = None
                for p in possible_paths:
                    if p.exists():
                        orig_img = cv2.imread(str(p))
                        break
                
                if orig_img is None: continue

                orig_h, orig_w = orig_img.shape[:2]

                # ç‰©ç†ç©ºé—´æ ¡å‡†
                valid_h_feat = int(round(orig_h * (h_feat / H_tensor)))
                valid_w_feat = int(round(orig_w * (w_feat / W_tensor)))
                
                s_2d = scores_prob[i, 0].cpu().numpy()
                s_valid = s_2d[:valid_h_feat, :valid_w_feat]
                
                s_norm = (s_valid - s_valid.min()) / (s_valid.max() - s_valid.min() + 1e-8)
                heatmap = cv2.applyColorMap((s_norm * 255).astype(np.uint8), cv2.COLORMAP_JET)
                heatmap = cv2.resize(heatmap, (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)
                
                overlay = cv2.addWeighted(orig_img, 0.4, heatmap, 0.6, 0)
                cv2.imwrite(str(viz_dir / f"sample_{img_id}_S4_heatmap.jpg"), overlay)
                
            self.logger.info(f"ğŸ“¸ Epoch {epoch}: å·²ä¿å­˜ S4 å°ºåº¦é‡è¦æ€§çƒ­åŠ›å›¾è‡³ {viz_dir}")
        except Exception as e:
            self.logger.error(f"å¯è§†åŒ–æ¨¡å—è¿è¡Œå´©æºƒ: {e}", exc_info=True)
        except Exception as e:
            self.logger.error(f"å¯è§†åŒ–æ¨¡å—è¿è¡Œå´©æºƒ: {e}", exc_info=True)
    
    def _resume_from_checkpoint(self) -> None:
        """ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚"""
        try:
            checkpoint_path = Path(self.resume_from_checkpoint)
            if not checkpoint_path.exists():
                self.logger.warning(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
                return
            
            self.logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
            
            # æ¢å¤çŠ¶æ€
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.current_epoch = checkpoint.get('epoch', 0) + 1
            self.best_loss = checkpoint.get('best_loss', float('inf'))
            self.best_map = checkpoint.get('best_map', 0.0)
            self.global_step = checkpoint.get('global_step', 0)
            
            if 'ema_state_dict' in checkpoint:
                self.ema.load_state_dict(checkpoint['ema_state_dict'])
            if 'scaler_state_dict' in checkpoint:
                self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
            if 'visualizer_state' in checkpoint:
                self.visualizer.load_state_dict(checkpoint['visualizer_state'])
            if 'early_stopping_state' in checkpoint and self.early_stopping:
                self.early_stopping.load_state_dict(checkpoint['early_stopping_state'])
            
            self.logger.info(f"âœ“ æ¢å¤æˆåŠŸ (epoch={self.current_epoch}, step={self.global_step}, "
                           f"best_loss={self.best_loss:.4f})")
            
        except Exception as e:
            self.logger.error(f"æ¢å¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒDSETæ¸è¿›å¼è®­ç»ƒï¼Œé‡‡ç”¨å³äº§å³æ¸…åŸåˆ™ä¼˜åŒ–ï¼‰ã€‚"""
        self.model.train()
        
        # è®¾ç½®æ¨¡å‹çš„epochï¼ˆç”¨äºToken Pruning Losså’ŒCASS Lossçš„Warmupæ§åˆ¶ï¼‰
        if hasattr(self.model, 'set_epoch'):
            self.model.set_epoch(self.current_epoch)
        
        # [æè‡´ä¼˜åŒ–] æ ¸å¿ƒä¼˜åŒ–ï¼šåœ¨GPUä¸Šç›´æ¥ç»´æŠ¤è®¡æ•°å™¨ï¼Œä¸å†ç¼“å­˜å·¨å¤§çš„Logitsåˆ—è¡¨
        num_decoder_experts = self.model.num_experts
        num_encoder_experts = self.model.encoder_moe_num_experts if hasattr(self.model, 'encoder_moe_num_experts') else 4
        
        # ç»Ÿè®¡æ•´ä¸ªEpochçš„ç´¯åŠ å™¨ï¼ˆåœ¨GPUä¸Šï¼Œæå°çš„å†…å­˜å ç”¨ï¼‰
        decoder_expert_usage_total = torch.zeros(num_decoder_experts, dtype=torch.long, device=self.device)
        encoder_expert_usage_total = torch.zeros(num_encoder_experts, dtype=torch.long, device=self.device)
        total_dec_tokens = 0
        total_enc_tokens = 0
        
        # æŸå¤±ç»Ÿè®¡
        total_loss = 0.0
        detection_loss = 0.0
        moe_lb_loss = 0.0  # MoE load balance loss
        encoder_moe_loss_sum = 0.0  # Encoder Patch-MoE loss
        cass_loss_sum = 0.0  # CASS supervision loss
        token_pruning_ratios = []
        
        for batch_idx, (images, targets) in enumerate(self.train_loader):
            images = images.to(self.device)
            targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                       for k, v in t.items()} for t in targets]
            
            # å‰å‘ä¼ æ’­
            # [å†…å­˜ä¼˜åŒ–] ä½¿ç”¨ set_to_none=True æå‡å†…å­˜æ•ˆç‡
            self.optimizer.zero_grad(set_to_none=True)
            
            with torch.amp.autocast('cuda'):
                outputs = self.model(images, targets)
                loss = outputs.get('total_loss', torch.tensor(0.0, device=self.device))
                if not isinstance(loss, torch.Tensor):
                    loss = torch.tensor(loss, device=self.device)
                if loss.dim() > 0:
                    loss = loss.sum()
            
            # åå‘ä¼ æ’­ï¼ˆæ·»åŠ æ¢¯åº¦è£å‰ªï¼‰
            self.scaler.scale(loss).backward()
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.clip_max_norm)
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.ema.update(self.model)
            
            # æ˜¾å­˜å®šæœŸæ¸…ç†
            if batch_idx % 20 == 0:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # ç»Ÿè®¡å„ç§Loss
            total_loss += loss.item() if isinstance(loss, torch.Tensor) else float(loss)
            if isinstance(outputs, dict):
                if 'detection_loss' in outputs:
                    det_loss_val = outputs['detection_loss']
                    detection_loss += det_loss_val.item() if isinstance(det_loss_val, torch.Tensor) else float(det_loss_val)
                if 'decoder_moe_loss' in outputs:
                    moe_loss_val = outputs['decoder_moe_loss']
                    moe_lb_loss += moe_loss_val.item() if isinstance(moe_loss_val, torch.Tensor) else float(moe_loss_val)
                if 'encoder_moe_loss' in outputs:
                    enc_moe_loss_val = outputs['encoder_moe_loss']
                    encoder_moe_loss_sum += enc_moe_loss_val.item() if isinstance(enc_moe_loss_val, torch.Tensor) else float(enc_moe_loss_val)
                if 'cass_loss' in outputs:
                    cass_loss_val = outputs['cass_loss']
                    cass_loss_sum += cass_loss_val.item() if isinstance(cass_loss_val, torch.Tensor) else float(cass_loss_val)
            
            # [æè‡´ä¼˜åŒ–] å³äº§å³æ¸…ï¼šä¸ä¿ç•™Logitsåˆ—è¡¨ï¼Œè®¡ç®—å®ŒTopKå’Œbincountåç«‹å³é‡Šæ”¾æ˜¾å­˜
            # å¤„ç†Encoder MoEç»Ÿè®¡
            if isinstance(outputs, dict) and 'encoder_info' in outputs:
                enc_info = outputs['encoder_info']
                # Token Pruningæ¯”ä¾‹
                if 'token_pruning_ratios' in enc_info and enc_info['token_pruning_ratios']:
                    avg_ratio = sum(enc_info['token_pruning_ratios']) / len(enc_info['token_pruning_ratios'])
                    token_pruning_ratios.append(avg_ratio)
                
                # Encoderä¸“å®¶ä½¿ç”¨ç‡ç»Ÿè®¡ï¼ˆå³äº§å³æ¸…ï¼‰
                enc_logits = enc_info.get('moe_router_logits', [])
                if enc_logits and isinstance(enc_logits, list) and len(enc_logits) > 0:
                    # [å®‰å…¨è„±é’©] ä½¿ç”¨ detach() åˆ›å»ºç»Ÿè®¡å‰¯æœ¬ï¼Œç¡®ä¿ä¸å½±å“åå‘ä¼ æ’­
                    enc_logits_detached = [logits.detach() if isinstance(logits, torch.Tensor) else logits for logits in enc_logits]
                    enc_logits_tensor = torch.cat(enc_logits_detached, dim=0)
                    enc_top_k = self.model.encoder_moe_top_k if hasattr(self.model, 'encoder_moe_top_k') else 2
                    _, enc_indices = torch.topk(enc_logits_tensor, enc_top_k, dim=-1)
                    encoder_expert_usage_total.add_(torch.bincount(enc_indices.flatten(), minlength=num_encoder_experts))
                    total_enc_tokens += enc_indices.numel()
                    # æ˜¾å¼é‡Šæ”¾ä¸´æ—¶å¼ é‡
                    del enc_logits_detached, enc_logits_tensor, enc_indices
            
            # å¤„ç†Decoder MoEç»Ÿè®¡ï¼ˆå³äº§å³æ¸…ï¼‰
            if self.model.decoder.use_moe:
                for layer in self.model.decoder.decoder.layers:
                    if hasattr(layer, 'adaptive_expert_layer'):
                        dec_logits = layer.adaptive_expert_layer.router_logits_cache
                        if dec_logits:
                            # å¤„ç†åˆ—è¡¨æ ¼å¼çš„logits
                            if isinstance(dec_logits, list) and len(dec_logits) > 0:
                                # [å®‰å…¨è„±é’©] ä½¿ç”¨ detach() åˆ›å»ºç»Ÿè®¡å‰¯æœ¬ï¼Œç¡®ä¿ä¸å½±å“åå‘ä¼ æ’­
                                dec_logits_detached = [logits.detach() if isinstance(logits, torch.Tensor) else logits for logits in dec_logits]
                                dec_logits_tensor = torch.cat(dec_logits_detached, dim=0)
                                del dec_logits_detached
                            elif isinstance(dec_logits, torch.Tensor) and dec_logits.numel() > 0:
                                # [å®‰å…¨è„±é’©] ä½¿ç”¨ detach() åˆ›å»ºç»Ÿè®¡å‰¯æœ¬
                                dec_logits_tensor = dec_logits.detach()
                            else:
                                continue
                            
                            # ä»…åœ¨GPUä¸Šè®¡ç®—TopKç´¢å¼•å¹¶è®¡æ•°ï¼Œå®ŒæˆåLogitså³å¯è¢«é‡Šæ”¾
                            _, dec_indices = torch.topk(dec_logits_tensor, self.model.decoder.moe_top_k, dim=-1)
                            decoder_expert_usage_total.add_(torch.bincount(dec_indices.flatten(), minlength=num_decoder_experts))
                            total_dec_tokens += dec_indices.numel()
                            # æ˜¾å¼é‡Šæ”¾ä¸´æ—¶å¼ é‡
                            del dec_logits_tensor, dec_indices
            
            # [ä¼˜åŒ–] æ—¥å¿—æ‰“å°é€»è¾‘ï¼šæ¯100ä¸ªbatchåªæ˜¾ç¤ºåŸºæœ¬lossä¿¡æ¯
            if batch_idx % 100 == 0:
                det_loss_val = outputs.get('detection_loss', torch.tensor(0.0)).item() if isinstance(outputs, dict) else 0.0
                moe_loss_val = outputs.get('moe_load_balance_loss', torch.tensor(0.0)).item() if isinstance(outputs, dict) else 0.0
                
                self.logger.info(f'Epoch {self.current_epoch} | Batch {batch_idx} | '
                               f'Loss: {loss.item():.2f} (Det: {det_loss_val:.2f}, MoE: {moe_loss_val:.4f})')
            
            self.global_step += 1
        
        # Epochç»“æŸï¼Œè®¡ç®—å¹³å‡å€¼å¹¶è¿”å›ç»Ÿè®¡ç»“æœ
        num_batches = len(self.train_loader)
        avg_loss = total_loss / num_batches
        avg_detection_loss = detection_loss / num_batches
        avg_decoder_moe_lb_loss = moe_lb_loss / num_batches
        avg_encoder_moe_loss = encoder_moe_loss_sum / num_batches
        avg_cass_loss = cass_loss_sum / num_batches
        
        # è®¡ç®—ä¸“å®¶ä½¿ç”¨ç‡ï¼ˆä»GPUç´¯åŠ å™¨è½¬æ¢ï¼‰
        decoder_expert_usage_count = decoder_expert_usage_total.cpu().tolist()
        encoder_expert_usage_count = encoder_expert_usage_total.cpu().tolist()
        
        expert_usage_rate = []
        if total_dec_tokens > 0:
            for count in decoder_expert_usage_count:
                expert_usage_rate.append(count / total_dec_tokens)
        else:
            expert_usage_rate = [1.0 / num_decoder_experts] * num_decoder_experts
        
        encoder_expert_usage_rate = []
        if total_enc_tokens > 0:
            for count in encoder_expert_usage_count:
                encoder_expert_usage_rate.append(count / total_enc_tokens)
        else:
            encoder_expert_usage_rate = [1.0 / num_encoder_experts] * num_encoder_experts
        
        # è®¡ç®—å¹³å‡Token Pruningæ¯”ä¾‹
        avg_token_pruning_ratio = sum(token_pruning_ratios) / len(token_pruning_ratios) if token_pruning_ratios else 0.0
        
        # [å†…å­˜ä¼˜åŒ–] ç»Ÿè®¡å®Œä¸“å®¶ä½¿ç”¨ç‡åï¼Œæ‰‹åŠ¨æ¸…ç©º router_logits_cacheï¼ˆå³äº§å³æ¸…ï¼‰
        # ç¡®ä¿è¿™åªæ˜¯é’ˆå¯¹ç»Ÿè®¡æ—¥å¿—çš„æ¸…ç†ï¼Œä¸å½±å“ detr_criterion çš„è®¡ç®—
        if self.model.decoder.use_moe:
            for layer in self.model.decoder.decoder.layers:
                if hasattr(layer, 'adaptive_expert_layer'):
                    if hasattr(layer.adaptive_expert_layer, 'router_logits_cache'):
                        layer.adaptive_expert_layer.router_logits_cache = []
        
        # å‡†å¤‡è¿”å›ç»“æœ
        result = {
            'total_loss': avg_loss,
            'detection_loss': avg_detection_loss,
            'decoder_moe_loss': avg_decoder_moe_lb_loss,
            'encoder_moe_loss': avg_encoder_moe_loss,  # Encoder Patch-MoE loss
            'cass_loss': avg_cass_loss,  # CASS supervision loss
            'token_pruning_ratio': avg_token_pruning_ratio,
            'moe_load_balance_loss': avg_decoder_moe_lb_loss + avg_encoder_moe_loss,  # æ€»MoEæŸå¤±ï¼ˆå‘åå…¼å®¹ï¼‰
            'expert_usage': decoder_expert_usage_count,
            'expert_usage_rate': expert_usage_rate,
            'encoder_expert_usage_rate': encoder_expert_usage_rate
        }
        
        # [å†…å­˜ä¼˜åŒ–] é‡Šæ”¾ä¸´æ—¶ç»Ÿè®¡å˜é‡
        del decoder_expert_usage_total, encoder_expert_usage_total
        del decoder_expert_usage_count, encoder_expert_usage_count
        
        return result
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹å¹¶è®¡ç®—mAPã€‚"""
        self.ema.module.eval()
        
        # è®¾ç½®encoderçš„epochï¼ˆç”¨äºToken Pruningæ¸è¿›å¼å¯ç”¨ï¼ŒéªŒè¯æ—¶ä¹Ÿéœ€è¦ï¼‰
        # 1. æ›´æ–°è®­ç»ƒæ¨¡å‹ (ä¿æŒåŸæ ·)
        # è®¾ç½®æ¨¡å‹çš„epochï¼ˆç”¨äºToken Pruning Losså’ŒCASS Lossçš„Warmupæ§åˆ¶ï¼‰
        # è¿™ä¼šåŒæ—¶æ›´æ–°encoderçš„epochï¼ˆåœ¨model.set_epochå†…éƒ¨è°ƒç”¨ï¼‰
        if hasattr(self.model, 'set_epoch'):
            self.model.set_epoch(self.current_epoch)
        
        # =========================================================
        # [ä¿®å¤] å¿…é¡»åŒæ—¶æ›´æ–° EMA æ¨¡å‹çš„ epochï¼Œå¦åˆ™éªŒè¯æ—¶ä¸ä¼šå‰ªæï¼
        # EMAæ¨¡å‹æ˜¯deepcopyçš„ç‹¬ç«‹å‰¯æœ¬ï¼Œéœ€è¦å•ç‹¬è®¾ç½®epoch
        # =========================================================
        if hasattr(self.ema.module, 'set_epoch'):
            self.ema.module.set_epoch(self.current_epoch)
            # è°ƒè¯•ï¼šéªŒè¯EMAæ¨¡å‹çš„pruning_enabledçŠ¶æ€
            if hasattr(self.ema.module, 'encoder') and hasattr(self.ema.module.encoder, 'shared_token_pruner') and self.ema.module.encoder.shared_token_pruner:
                pruner = self.ema.module.encoder.shared_token_pruner
                if self.current_epoch >= 10:  # åªåœ¨warmupåæ‰“å°
                    self.logger.debug(f"[éªŒè¯] Epoch {self.current_epoch}: EMA pruner.pruning_enabled={pruner.pruning_enabled}, "
                                   f"current_epoch={pruner.current_epoch}, warmup_epochs={pruner.warmup_epochs}")
        
        total_loss = 0.0
        all_predictions = []
        all_targets = []
        
        image_id_to_size = {}
        
        # ç»Ÿè®¡éªŒè¯æ—¶çš„å‰ªææ¯”ä¾‹
        val_pruning_ratios = []
        
        # éªŒè¯é€»è¾‘
        with torch.no_grad():
            for batch_idx, (images, targets) in enumerate(self.val_loader):
                # åŠ¨æ€è·å– Tensor å°ºå¯¸
                B, C, H_tensor, W_tensor = images.shape

                images = images.to(self.device)
                targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                           for k, v in t.items()} for t in targets]
                
                # è®°å½•è¯¥ batch çš„å°ºå¯¸ä¿¡æ¯
                for i, target in enumerate(targets):
                    image_id = batch_idx * self.config['training']['batch_size'] + i
                    image_id_to_size[image_id] = (W_tensor, H_tensor)
                
                outputs = self.ema.module(images, targets)
                
                # æ”¶é›†éªŒè¯æ—¶çš„å‰ªæä¿¡æ¯
                if isinstance(outputs, dict) and 'encoder_info' in outputs:
                    encoder_info = outputs['encoder_info']
                    if 'token_pruning_ratios' in encoder_info and encoder_info['token_pruning_ratios']:
                        avg_ratio = sum(encoder_info['token_pruning_ratios']) / len(encoder_info['token_pruning_ratios'])
                        val_pruning_ratios.append(avg_ratio)
                
                if isinstance(outputs, dict):
                    if 'total_loss' in outputs:
                        total_loss += outputs['total_loss'].item()
                    
                    # æ”¶é›†é¢„æµ‹ç»“æœï¼ˆåªåœ¨éœ€è¦è®¡ç®—mAPæ—¶æ”¶é›†ï¼Œå‰30ä¸ªepochè·³è¿‡ï¼‰
                    if 'class_scores' in outputs and 'bboxes' in outputs:
                        self._collect_predictions(outputs, targets, batch_idx, all_predictions, all_targets, W_tensor, H_tensor)
        
        # ä¿å­˜é¢„æµ‹ç»“æœç”¨äºåç»­æ‰“å°æ¯ä¸ªç±»åˆ«mAPï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        self._last_val_predictions = all_predictions
        self._last_val_targets = all_targets
        self._last_val_image_id_to_size = image_id_to_size
        
        avg_loss = total_loss / len(self.val_loader)
        
        # è®¡ç®—å¹³å‡éªŒè¯æ—¶çš„å‰ªææ¯”ä¾‹
        avg_val_pruning_ratio = sum(val_pruning_ratios) / len(val_pruning_ratios) if val_pruning_ratios else 0.0
        
        # æ‰“å°éªŒè¯æ—¶çš„å‰ªæçŠ¶æ€ï¼ˆæ¯æ¬¡éªŒè¯éƒ½æ‰“å°ï¼Œç”¨äºç›‘æ§ï¼‰
        if avg_val_pruning_ratio > 0.0:
            self.logger.info(f"  âœ“ éªŒè¯æ—¶Token Pruningç”Ÿæ•ˆ: {avg_val_pruning_ratio:.2%} tokensè¢«å‰ªæ")
        else:
            # WarmupæœŸé—´pruning_ratio=0.0æ˜¯æ­£å¸¸çš„ï¼Œåªåœ¨warmupåè­¦å‘Š
            if self.current_epoch >= 10:
                self.logger.warning(f"  âš  éªŒè¯æ—¶Token Pruningæœªç”Ÿæ•ˆ (pruning_ratio=0.0)! å¯èƒ½EMAæ¨¡å‹epochæœªè®¾ç½®")
            else:
                self.logger.debug(f"  éªŒè¯æ—¶Token Pruning: Warmupé˜¶æ®µ (epoch {self.current_epoch} < warmup_epochs), pruning_ratio=0.0 (æ­£å¸¸)")
        
        # [ä¿®å¤] è®¡ç®— mAP æ—¶ï¼Œä¼ é€’ image_id_to_size ä»¥æ”¯æŒå¤šå°ºåº¦éªŒè¯ç²¾åº¦
        mAP_metrics = self._compute_map_metrics(all_predictions, all_targets, 
                                              image_id_to_size=image_id_to_size,
                                              print_per_category=False)
        
        return {
            'total_loss': avg_loss,
            'mAP_0.5': mAP_metrics.get('mAP_0.5', 0.0),
            'mAP_0.75': mAP_metrics.get('mAP_0.75', 0.0),
            'mAP_0.5_0.95': mAP_metrics.get('mAP_0.5_0.95', 0.0),
            'mAP_s': mAP_metrics.get('mAP_s', 0.0),
            'mAP_m': mAP_metrics.get('mAP_m', 0.0),
            'mAP_l': mAP_metrics.get('mAP_l', 0.0),
            'val_token_pruning_ratio': avg_val_pruning_ratio  # æ·»åŠ éªŒè¯æ—¶çš„å‰ªææ¯”ä¾‹
        }
    
    def _collect_predictions(self, outputs: Dict, targets: List[Dict], batch_idx: int,
                            all_predictions: List, all_targets: List, img_w: int, img_h: int) -> None:
        """æ”¶é›†é¢„æµ‹ç»“æœç”¨äºmAPè®¡ç®—ã€‚ä¿ç•™æ‰€æœ‰æœ‰æ•ˆé¢„æµ‹æ¡†ï¼Œä¸åštop-ké™åˆ¶ã€‚"""
        pred_logits = outputs['class_scores']  # [B, Q, C]
        pred_boxes = outputs['bboxes']  # [B, Q, 4]
        
        batch_size = pred_logits.shape[0]
        
        for i in range(batch_size):
            # [FIX] ä½¿ç”¨ sigmoid æ¿€æ´»å‡½æ•°ï¼Œå¯¹é½ Focal Loss / VFL è®­ç»ƒé€»è¾‘
            pred_scores = torch.sigmoid(pred_logits[i])  # [Q, C]
            max_scores, pred_classes = torch.max(pred_scores, dim=-1)  # [Q]
            
            # è¿‡æ»¤æ— æ•ˆæ¡†ï¼ˆpaddingæ¡†ï¼‰ï¼Œä¿ç•™æ‰€æœ‰æœ‰æ•ˆé¢„æµ‹æ¡†
            valid_boxes_mask = ~torch.all(pred_boxes[i] == 1.0, dim=1)
            valid_indices = torch.where(valid_boxes_mask)[0]
            if len(valid_indices) > 0:
                filtered_boxes = pred_boxes[i][valid_indices]
                filtered_classes = pred_classes[valid_indices]
                filtered_scores = max_scores[valid_indices]
                
                # è½¬æ¢ä¸ºCOCOæ ¼å¼
                if filtered_boxes.shape[0] > 0:
                    boxes_coco = torch.zeros_like(filtered_boxes)
                    if filtered_boxes.max() <= 1.0:
                        # å½’ä¸€åŒ–åæ ‡ -> åƒç´ åæ ‡
                        boxes_coco[:, 0] = (filtered_boxes[:, 0] - filtered_boxes[:, 2] / 2) * img_w
                        boxes_coco[:, 1] = (filtered_boxes[:, 1] - filtered_boxes[:, 3] / 2) * img_h
                        boxes_coco[:, 2] = filtered_boxes[:, 2] * img_w
                        boxes_coco[:, 3] = filtered_boxes[:, 3] * img_h
                    else:
                        boxes_coco = filtered_boxes.clone()
                    
                    # Clampåæ ‡
                    boxes_coco[:, 0] = torch.clamp(boxes_coco[:, 0], 0, img_w)
                    boxes_coco[:, 1] = torch.clamp(boxes_coco[:, 1], 0, img_h)
                    boxes_coco[:, 2] = torch.clamp(boxes_coco[:, 2], 1, img_w)
                    boxes_coco[:, 3] = torch.clamp(boxes_coco[:, 3], 1, img_h)
                    
                    for j in range(boxes_coco.shape[0]):
                        all_predictions.append({
                            'image_id': batch_idx * self.config['training']['batch_size'] + i,
                            'category_id': int(filtered_classes[j].item()) + 1,
                            'bbox': boxes_coco[j].cpu().numpy().tolist(),
                            'score': float(filtered_scores[j].item())
                        })
            
            # å¤„ç†çœŸå®æ ‡ç­¾ï¼ˆè¯„ä¼°æ—¶åŒ…å«iscrowdå­—æ®µï¼ŒCOCOevalä¼šè‡ªåŠ¨å¤„ç†ï¼‰
            if i < len(targets) and 'labels' in targets[i] and 'boxes' in targets[i]:
                true_labels = targets[i]['labels']
                true_boxes = targets[i]['boxes']
                
                if len(true_labels) > 0:
                    img_size = img_h
                    max_val = float(true_boxes.max().item()) if true_boxes.numel() > 0 else 0.0
                    scale = img_size if max_val <= 1.0 + 1e-6 else 1.0
                    
                    true_boxes_coco = torch.zeros_like(true_boxes)
                    if max_val <= 1.0 + 1e-6:
                        true_boxes_coco[:, 0] = (true_boxes[:, 0] - true_boxes[:, 2] / 2) * img_w
                        true_boxes_coco[:, 1] = (true_boxes[:, 1] - true_boxes[:, 3] / 2) * img_h
                        true_boxes_coco[:, 2] = true_boxes[:, 2] * img_w
                        true_boxes_coco[:, 3] = true_boxes[:, 3] * img_h
                    else:
                        true_boxes_coco = true_boxes.clone()
                    
                    true_boxes_coco[:, 0] = torch.clamp(true_boxes_coco[:, 0], 0, img_w)
                    true_boxes_coco[:, 1] = torch.clamp(true_boxes_coco[:, 1], 0, img_h)
                    true_boxes_coco[:, 2] = torch.clamp(true_boxes_coco[:, 2], 1, img_w)
                    true_boxes_coco[:, 3] = torch.clamp(true_boxes_coco[:, 3], 1, img_h)
                    
                    # è·å–iscrowdå­—æ®µï¼ˆè¯„ä¼°æ—¶å­˜åœ¨ï¼‰
                    has_iscrowd = 'iscrowd' in targets[i]
                    iscrowd_values = targets[i]['iscrowd'] if has_iscrowd else torch.zeros(len(true_labels), dtype=torch.int64)
                    
                    for j in range(len(true_labels)):
                        ann_dict = {
                            'image_id': batch_idx * self.config['training']['batch_size'] + i,
                            'category_id': int(true_labels[j].item()) + 1,
                            'bbox': true_boxes_coco[j].cpu().numpy().tolist(),
                            'area': float((true_boxes_coco[j, 2] * true_boxes_coco[j, 3]).item())
                        }
                        # è¯„ä¼°æ—¶æ·»åŠ iscrowdå­—æ®µï¼Œè®©COCOevalè‡ªåŠ¨å¤„ç†
                        if has_iscrowd:
                            ann_dict['iscrowd'] = int(iscrowd_values[j].item())
                        all_targets.append(ann_dict)
    
    def _print_best_model_per_category_map(self):
        """ä½¿ç”¨best_modelæ—¶æ‰“å°è¯¦ç»†çš„æ¯ç±»mAPï¼ˆ8ç±»ï¼‰ï¼Œé‡æ–°è®¡ç®—ä»¥è¾“å‡ºCOCOè¯¦ç»†è¯„ä¼°è¡¨æ ¼
        æ³¨æ„ï¼šåªæœ‰åœ¨epoch >= 30æ—¶æ‰ä¼šè§¦å‘best_modelï¼ˆåŸºäºmAPï¼‰ï¼Œæ­¤æ—¶æ‰ä¼šè®¡ç®—æ¯ç±»çš„mAP
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„é¢„æµ‹ç»“æœï¼ˆåªæœ‰ä»ç¬¬30ä¸ªepochå¼€å§‹æ‰ä¼šæœ‰ï¼‰
            if hasattr(self, '_last_val_predictions') and hasattr(self, '_last_val_targets'):
                if len(self._last_val_predictions) == 0 or len(self._last_val_targets) == 0:
                    self.logger.warning("é¢„æµ‹ç»“æœä¸ºç©ºï¼Œè·³è¿‡æ¯ç±»mAPè®¡ç®—")
                    return
                # é‡æ–°è®¡ç®—mAPï¼Œprint_per_category=Trueä¼šè¾“å‡ºCOCOè¯¦ç»†è¯„ä¼°è¡¨æ ¼
                mAP_metrics = self._compute_map_metrics(
                    self._last_val_predictions, 
                    self._last_val_targets, 
                    image_id_to_size=getattr(self, '_last_val_image_id_to_size', None),
                    print_per_category=True
                )
                per_category_map = mAP_metrics.get('per_category_map', {})
            else:
                # å¦‚æœæ²¡æœ‰ä¿å­˜çš„ç»“æœï¼Œåˆ™é‡æ–°è®¡ç®—ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
                self.logger.warning("æœªæ‰¾åˆ°ä¿å­˜çš„éªŒè¯ç»“æœï¼Œé‡æ–°è®¡ç®—æ¯ä¸ªç±»åˆ«mAP...")
                self.ema.module.eval()
                all_predictions = []
                all_targets = []
                image_id_to_size = {}
                
                with torch.no_grad():
                    for batch_idx, (images, targets) in enumerate(self.val_loader):
                        # åŠ¨æ€è·å– Tensor å°ºå¯¸
                        B, C, H_tensor, W_tensor = images.shape
                        images = images.to(self.device)
                        targets = [{k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                                   for k, v in t.items()} for t in targets]
                        
                        for i, target in enumerate(targets):
                            image_id = batch_idx * self.config['training']['batch_size'] + i
                            image_id_to_size[image_id] = (W_tensor, H_tensor)
                        
                        outputs = self.ema.module(images, targets)
                        
                        if 'class_scores' in outputs and 'bboxes' in outputs:
                            self._collect_predictions(outputs, targets, batch_idx, all_predictions, all_targets, W_tensor, H_tensor)
                
                mAP_metrics = self._compute_map_metrics(
                    all_predictions, 
                    all_targets, 
                    image_id_to_size=image_id_to_size,
                    print_per_category=True
                )
                per_category_map = mAP_metrics.get('per_category_map', {})
        except Exception as e:
            self.logger.warning(f"æ‰“å°best_modelæ¯ç±»mAPå¤±è´¥: {e}")
    
    def _compute_map_metrics(self, predictions: List[Dict], targets: List[Dict], 
                             image_id_to_size: Dict[int, Tuple[int, int]] = None,
                             img_h: int = 736, img_w: int = 1280,
                             print_per_category: bool = False) -> Dict[str, float]:
        """è®¡ç®—mAPæŒ‡æ ‡ã€‚
        
        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
            targets: çœŸå®æ ‡ç­¾åˆ—è¡¨
            image_id_to_size: å›¾åƒIDåˆ°(W, H)çš„æ˜ å°„å­—å…¸ï¼ˆæ¨èï¼‰
            img_h: é»˜è®¤å›¾åƒé«˜åº¦
            img_w: é»˜è®¤å›¾åƒå®½åº¦
            print_per_category: æ˜¯å¦æ‰“å°æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†mAPï¼ˆé»˜è®¤Falseï¼Œåªåœ¨best_modelæ—¶æ‰“å°ï¼‰
        """
        try:
            if len(predictions) == 0:
                return {
                    'mAP_0.5': 0.0,
                    'mAP_0.75': 0.0,
                    'mAP_0.5_0.95': 0.0,
                    'mAP_s': 0.0,
                    'mAP_m': 0.0,
                    'mAP_l': 0.0
                }
            
            # è·å–ç±»åˆ«ä¿¡æ¯
            if hasattr(self, 'val_dataset') and hasattr(self.val_dataset, 'get_categories'):
                categories = self.val_dataset.get_categories()
            else:
                categories = [
                    {'id': 1, 'name': 'Car'},
                    {'id': 2, 'name': 'Truck'},
                    {'id': 3, 'name': 'Van'},
                    {'id': 4, 'name': 'Bus'},
                    {'id': 5, 'name': 'Pedestrian'},
                    {'id': 6, 'name': 'Cyclist'},
                    {'id': 7, 'name': 'Motorcyclist'},
                    {'id': 8, 'name': 'Trafficcone'}
                ]
            
            # åˆ›å»ºCOCOæ ¼å¼æ•°æ®
            coco_gt = {
                'images': [],
                'annotations': [],
                'categories': categories,
                'info': {
                    'description': 'DAIR-V2X Dataset',
                    'version': '1.0',
                    'year': 2024
                }
            }
            
            # [ä¿®å¤] åŠ¨æ€è®¾ç½®æ¯å¼ å›¾åƒçš„æ­£ç¡®å°ºå¯¸
            image_ids = set(target['image_id'] for target in targets)
            for img_id in image_ids:
                if image_id_to_size and img_id in image_id_to_size:
                    w, h = image_id_to_size[img_id]
                else:
                    w, h = img_w, img_h
                coco_gt['images'].append({
                    'id': img_id, 
                    'width': w,
                    'height': h
                })
            
            # æ·»åŠ æ ‡æ³¨
            for i, target in enumerate(targets):
                target['id'] = i + 1
                coco_gt['annotations'].append(target)
            
            # ä½¿ç”¨pycocotoolsè¯„ä¼°ï¼ˆæŠ‘åˆ¶æ‰€æœ‰è¾“å‡ºä»¥èŠ‚çœæ—¶é—´ï¼‰
            from io import StringIO
            import sys
            
            coco_gt_obj = COCO()
            coco_gt_obj.dataset = coco_gt
            # æŠ‘åˆ¶createIndexçš„è¾“å‡º
            old_stdout = sys.stdout
            sys.stdout = StringIO()
            try:
                coco_gt_obj.createIndex()
            finally:
                sys.stdout = old_stdout
            
            # æŠ‘åˆ¶loadResçš„è¾“å‡º
            sys.stdout = StringIO()
            try:
                coco_dt = coco_gt_obj.loadRes(predictions)
            finally:
                sys.stdout = old_stdout
            
            coco_eval = COCOeval(coco_gt_obj, coco_dt, 'bbox')
            # å¦‚æœprint_per_category=Trueï¼ˆä¿å­˜best_modelæ—¶ï¼‰ï¼Œè¾“å‡ºCOCOè¯¦ç»†è¯„ä¼°è¡¨æ ¼ï¼›å¦åˆ™æŠ‘åˆ¶è¾“å‡º
            if print_per_category:
                # åªæŠ‘åˆ¶ä¸­é—´è¿‡ç¨‹è¾“å‡ºï¼Œä¿ç•™summaryè¡¨æ ¼
                sys.stdout = StringIO()
                try:
                    coco_eval.evaluate()
                    coco_eval.accumulate()
                finally:
                    sys.stdout = old_stdout
                # è¾“å‡ºsummaryè¡¨æ ¼
                coco_eval.summarize()
            else:
                # å®Œå…¨æŠ‘åˆ¶è¾“å‡º
                sys.stdout = StringIO()
                try:
                    coco_eval.evaluate()
                    coco_eval.accumulate()
                    coco_eval.summarize()
                finally:
                    sys.stdout = old_stdout
            
            # åªåœ¨éœ€è¦æ—¶ï¼ˆprint_per_category=Trueï¼‰æ‰è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ mAPï¼Œé¿å…æ¯ä¸ªepochéƒ½è®¡ç®—8æ¬¡
            per_category_map = {}
            if print_per_category:
                # æå–æ¯ä¸ªç±»åˆ«çš„ mAP@0.5:0.95
                category_map = {cat['id']: cat['name'] for cat in categories}
                
                # æ–¹æ³•ï¼šä¸ºæ¯ä¸ªç±»åˆ«å•ç‹¬è®¡ç®— AP
                # é€šè¿‡è®¾ç½® catIds å‚æ•°ï¼Œåªè¯„ä¼°ç‰¹å®šç±»åˆ«
                cat_ids = coco_eval.params.catIds
                
                for cat_id, cat_name in category_map.items():
                    if cat_id in cat_ids:
                        try:
                            # ä¸ºå½“å‰ç±»åˆ«åˆ›å»ºå•ç‹¬çš„ COCOeval å¯¹è±¡
                            coco_eval_cat = COCOeval(coco_gt_obj, coco_dt, 'bbox')
                            coco_eval_cat.params.catIds = [cat_id]  # åªè¯„ä¼°å½“å‰ç±»åˆ«
                            # æŠ‘åˆ¶æ‰€æœ‰è¾“å‡ºï¼ˆevaluateã€accumulateã€summarizeéƒ½ä¼šäº§ç”Ÿè¾“å‡ºï¼‰
                            sys.stdout = StringIO()
                            try:
                                coco_eval_cat.evaluate()
                                coco_eval_cat.accumulate()
                                coco_eval_cat.summarize()
                            finally:
                                sys.stdout = old_stdout
                            
                            # æ£€æŸ¥ stats æ˜¯å¦å­˜åœ¨ä¸”æœ‰è¶³å¤Ÿçš„å…ƒç´ 
                            # stats[0] = AP@0.5:0.95, éœ€è¦ç¡®ä¿è‡³å°‘æœ‰1ä¸ªå…ƒç´ 
                            if hasattr(coco_eval_cat, 'stats') and len(coco_eval_cat.stats) > 0:
                                per_category_map[cat_name] = float(coco_eval_cat.stats[0])
                            else:
                                # å¦‚æœæ²¡æœ‰æ£€æµ‹ç»“æœï¼Œstats å¯èƒ½ä¸ºç©ºï¼Œè®¾ä¸º0
                                per_category_map[cat_name] = 0.0
                        except (IndexError, AttributeError, ValueError) as e:
                            # æ•è·å¯èƒ½çš„ç´¢å¼•é”™è¯¯ã€å±æ€§é”™è¯¯æˆ–å€¼é”™è¯¯
                            # å¦‚æœè¯¥ç±»åˆ«æ²¡æœ‰æ£€æµ‹ç»“æœï¼Œè¿™äº›é”™è¯¯æ˜¯æ­£å¸¸çš„
                            per_category_map[cat_name] = 0.0
                        except Exception as e:
                            # å…¶ä»–å¼‚å¸¸ä¹Ÿæ•è·ï¼Œç¡®ä¿ä¸ä¼šä¸­æ–­æ•´ä¸ªè¯„ä¼°è¿‡ç¨‹
                            self.logger.debug(f"ç±»åˆ« {cat_name} APè®¡ç®—å¤±è´¥: {e}")
                            per_category_map[cat_name] = 0.0
                    else:
                        per_category_map[cat_name] = 0.0
            
            # åªåœ¨best_modelæ—¶æ‰“å°æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†mAP
            if print_per_category:
                self.logger.info("  æ¯ä¸ªç±»åˆ«çš„ mAP@0.5:0.95:")
                category_order = ['Car', 'Truck', 'Van', 'Bus', 'Pedestrian', 
                                'Cyclist', 'Motorcyclist', 'Trafficcone']
                for cat_name in category_order:
                    map_val = per_category_map.get(cat_name, 0.0)
                    self.logger.info(f"    {cat_name:12s}: {map_val:.4f}")
            
            result = {
                'mAP_0.5': coco_eval.stats[1],
                'mAP_0.75': coco_eval.stats[2],
                'mAP_0.5_0.95': coco_eval.stats[0],
                'mAP_s': coco_eval.stats[3] if len(coco_eval.stats) > 3 else 0.0,  # Small objects
                'mAP_m': coco_eval.stats[4] if len(coco_eval.stats) > 4 else 0.0,  # Medium objects
                'mAP_l': coco_eval.stats[5] if len(coco_eval.stats) > 5 else 0.0,  # Large objects
                'per_category_map': per_category_map  # ä¿å­˜æ¯ä¸ªç±»åˆ«çš„mAP
            }
            
            # æ·»åŠ æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
            for cat_name in per_category_map.keys():
                result[f'mAP_{cat_name}'] = per_category_map[cat_name]
            
            return result
            
        except Exception as e:
            self.logger.warning(f"mAPè®¡ç®—å¤±è´¥: {e}")
            return {
                'mAP_0.5': 0.0,
                'mAP_0.75': 0.0,
                'mAP_0.5_0.95': 0.0,
                'mAP_s': 0.0,
                'mAP_m': 0.0,
                'mAP_l': 0.0
            }
    
    def _safe_save(self, checkpoint: Dict, path: Path, desc: str = "æ£€æŸ¥ç‚¹") -> bool:
        """å®‰å…¨ä¿å­˜checkpoint - å¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†ã€‚"""
        import time
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # å…ˆä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_path = path.with_suffix('.pth.tmp')
                torch.save(checkpoint, temp_path)
                
                # ç¡®ä¿å†™å…¥å®Œæˆ
                import os
                os.sync()
                
                # é‡å‘½åä¸ºç›®æ ‡æ–‡ä»¶ï¼ˆåŸå­æ“ä½œï¼‰
                temp_path.replace(path)
                self.logger.info(f"ğŸ’¾ ä¿å­˜{desc}: {path}")
                return True
                
            except Exception as e:
                self.logger.warning(f"ä¿å­˜{desc}å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                else:
                    self.logger.error(f"âš ï¸  ä¿å­˜{desc}æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡å¹¶ç»§ç»­è®­ç»ƒ")
                    return False
        
        return False
    
    def save_checkpoint(self, epoch: int, is_best: bool = False) -> None:
        """ä¿å­˜æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'warmup_scheduler_state_dict': self.warmup_scheduler.state_dict(),
            'ema_state_dict': self.ema.state_dict(),
            'scaler_state_dict': self.scaler.state_dict(),
            'config': self.config,
            'best_loss': self.best_loss,
            'best_map': self.best_map,
            'global_step': self.global_step,
            'visualizer_state': self.visualizer.state_dict()
        }
        
        if self.early_stopping:
            checkpoint['early_stopping_state'] = self.early_stopping.state_dict()
        
        if is_best:
            # ä¿å­˜å½“å‰EMAæ¨¡å‹çš„state_dictï¼ˆç”¨äºæ¨ç†æ—¶ç¡®ä¿ä½¿ç”¨best_modelçš„å‚æ•°ï¼‰
            best_ema_state = None
            if hasattr(self, 'ema') and self.ema:
                best_ema_state = self.ema.state_dict()
            
            best_path = self.log_dir / 'best_model.pth'
            self._safe_save(checkpoint, best_path, "æœ€ä½³æ¨¡å‹")
            
            # [å†…å­˜ä¼˜åŒ–] Checkpoint åŸå­åŒ–ç®¡ç†ï¼šä¿å­˜åç«‹å³å›æ”¶ä¸´æ—¶å¯¹è±¡
            del checkpoint
            if best_ema_state is not None:
                del best_ema_state
            gc.collect()
            
            # åœ¨best_modelæ—¶é‡æ–°è®¡ç®—å¹¶æ‰“å°è¯¦ç»†çš„æ¯ç±»mAPï¼ˆ8ç±»ï¼‰
            self._print_best_model_per_category_map()
    
    def save_latest_checkpoint(self, epoch: int) -> None:
        """ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ç”¨äºæ–­ç‚¹ç»­è®­ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'warmup_scheduler_state_dict': self.warmup_scheduler.state_dict(),
            'ema_state_dict': self.ema.state_dict(),
            'scaler_state_dict': self.scaler.state_dict(),
            'config': self.config,
            'best_loss': self.best_loss,
            'best_map': self.best_map,
            'global_step': self.global_step,
            'visualizer_state': self.visualizer.state_dict()
        }
        
        if self.early_stopping:
            checkpoint['early_stopping_state'] = self.early_stopping.state_dict()
        
        latest_path = self.log_dir / 'latest_checkpoint.pth'
        self._safe_save(checkpoint, latest_path, "æœ€æ–°æ£€æŸ¥ç‚¹")
        
        # [å†…å­˜ä¼˜åŒ–] Checkpoint åŸå­åŒ–ç®¡ç†ï¼šä¿å­˜åç«‹å³å›æ”¶ä¸´æ—¶å¯¹è±¡
        del checkpoint
        gc.collect()
    
    def train(self) -> None:
        """ä¸»è®­ç»ƒå¾ªç¯ã€‚"""
        epochs = self.config['training']['epochs']
        self.logger.info(f"å¼€å§‹è®­ç»ƒ {epochs} epochs")
        self.logger.info(f"âœ“ æ¢¯åº¦è£å‰ª: max_norm={self.clip_max_norm}")
        
        for epoch in range(self.current_epoch, epochs):
            self.current_epoch = epoch

            # ğŸš€ åŠ¨æ€è°ƒæ•´ Batch Size é€»è¾‘
            token_pruning_warmup_epochs = self.config['model'].get('dset', {}).get('token_pruning_warmup_epochs', 10)
            base_batch_size = self.config['training']['batch_size']
            
            # åŠ¨æ€ Batch Size ç­–ç•¥ï¼š
            # - é¢„çƒ­æœŸ (0-9 è½®)ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ batch_size
            # - é¢„çƒ­æœŸåï¼šç¿» 4 å€
            warmup_batch_size = base_batch_size
            if epoch < token_pruning_warmup_epochs:
                current_target_batch_size = warmup_batch_size
            else:
                current_target_batch_size = warmup_batch_size * 4
            
            # å¦‚æœå½“å‰åŠ è½½å™¨çš„ batch_size ä¸ç›®æ ‡ä¸ä¸€è‡´ï¼Œåˆ™é‡å»ºåŠ è½½å™¨
            if self.train_loader.batch_size != current_target_batch_size:
                self.logger.info(f"ğŸ”„ åŠ¨æ€è°ƒæ•´ Batch Size: {self.train_loader.batch_size} -> {current_target_batch_size} (Epoch {epoch})")
                # é”€æ¯æ—§çš„è¿­ä»£å™¨ï¼ˆå¦‚æœæœ‰ï¼‰å¹¶é‡å»º
                del self.train_loader
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                self.train_loader = self._build_train_loader(current_target_batch_size)

            # CASS Loss warmupç»“æŸæç¤º
            if hasattr(self.model, 'use_cass') and self.model.use_cass and hasattr(self.model, 'cass_warmup_epochs'):
                if epoch == self.model.cass_warmup_epochs:
                    self.logger.info(f"  âš ï¸  CASS Loss warmupç»“æŸï¼Œä»epoch {epoch}å¼€å§‹åº”ç”¨CASSç›‘ç£æŸå¤± (weight={self.model.cass_loss_weight})")

            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            
            # éªŒè¯ç­–ç•¥ï¼š
            # - å‰100 epochï¼šæ¯10è½®éªŒè¯ä¸€æ¬¡
            # - 100-140 epochï¼šæ¯5è½®éªŒè¯ä¸€æ¬¡
            # - 140 epochä»¥åï¼šæ¯è½®éªŒè¯
            should_validate = False
            if epoch < 100:
                if (epoch + 1) % 10 == 0:
                    should_validate = True
            elif epoch < 140:
                if (epoch + 1) % 5 == 0:
                    should_validate = True
            else:
                should_validate = True
            
            if should_validate:
                val_metrics = self.validate()
            else:
                val_metrics = {}
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.current_epoch < self.warmup_scheduler.warmup_epochs:
                self.warmup_scheduler.step()
            else:
                self.scheduler.step()
            
            # è¾“å‡ºæ—¥å¿—
            self.logger.info(f"Epoch {epoch}:")
            if should_validate:
                current_map = val_metrics.get('mAP_0.5_0.95', 0.0)
                current_map_50 = val_metrics.get('mAP_0.5', 0.0)
                self.logger.info(f"  è®­ç»ƒæŸå¤±: {train_metrics.get('total_loss', 0.0):.2f} | éªŒè¯æŸå¤±: {val_metrics.get('total_loss', 0.0):.2f}")
                self.logger.info(f"  ğŸ“Š å½“å‰mAP: {current_map:.4f} (mAP@50: {current_map_50:.4f})")
            else:
                self.logger.info(f"  è®­ç»ƒæŸå¤±: {train_metrics.get('total_loss', 0.0):.2f} | éªŒè¯æŸå¤±: Skipped")
            
            # æ˜¾ç¤ºè¯¦ç»†æŸå¤±ï¼ˆä¸éªŒè¯é¢‘ç‡ä¿æŒä¸€è‡´ï¼Œæˆ–è€…å§‹ç»ˆæ˜¾ç¤ºï¼‰
            # è¿™é‡Œæ”¹ä¸ºå§‹ç»ˆæ˜¾ç¤ºè¯¦ç»†æŸå¤±ï¼Œå› ä¸ºä¸éªŒè¯æ—¶ä¹Ÿéœ€è¦ç›‘æ§è®­ç»ƒLoss
            should_show_details = True
            if should_show_details:
                self.logger.info(f"  æ£€æµ‹æŸå¤±: {train_metrics['detection_loss']:.2f}")
                self.logger.info(f"  Decoder MoEæŸå¤±: {train_metrics.get('decoder_moe_loss', 0.0):.4f}")
                self.logger.info(f"  Encoder MoEæŸå¤±: {train_metrics.get('encoder_moe_loss', 0.0):.4f}")
                if self.model.use_cass:
                    self.logger.info(f"  CASS Loss: {train_metrics.get('cass_loss', 0.0):.4f}")
                self.logger.info(f"  MoEæ€»æŸå¤±: {train_metrics['moe_load_balance_loss']:.4f}")
                # æ˜¾ç¤ºä¸“å®¶ä½¿ç”¨ç‡ï¼ˆæ¯ä¸ªepochæ˜¾ç¤ºä¸€æ¬¡ï¼‰
                usage_str = [f"{rate*100:.2f}%" for rate in train_metrics['expert_usage_rate']]
                self.logger.info(f"  Decoderä¸“å®¶ä½¿ç”¨ç‡: [{', '.join(usage_str)}]")
                if 'encoder_expert_usage_rate' in train_metrics and train_metrics['encoder_expert_usage_rate']:
                    enc_usage_str = [f"{rate*100:.2f}%" for rate in train_metrics['encoder_expert_usage_rate']]
                    self.logger.info(f"  Encoderä¸“å®¶ä½¿ç”¨ç‡: [{', '.join(enc_usage_str)}]")
            
            # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°å¯è§†åŒ–å™¨
            current_lr = self.optimizer.param_groups[0]['lr']
            self.visualizer.record(
                epoch=epoch,
                train_loss=train_metrics.get('total_loss', 0.0),
                val_loss=val_metrics.get('total_loss', 0.0),
                mAP_0_5=val_metrics.get('mAP_0.5', 0.0),
                mAP_0_75=val_metrics.get('mAP_0.75', 0.0),
                mAP_0_5_0_95=val_metrics.get('mAP_0.5_0.95', 0.0),
                learning_rate=current_lr,
                # DSETç‰¹æœ‰çš„å¯è§†åŒ–å‚æ•°
                detection_loss=train_metrics.get('detection_loss', 0.0),
                encoder_moe_loss=train_metrics.get('encoder_moe_loss', 0.0),  # Encoder Patch-MoE loss
                decoder_moe_loss=train_metrics.get('decoder_moe_loss', 0.0),
                token_pruning_ratio=train_metrics.get('token_pruning_ratio', 0.0),
                # ä¼ é€’encoderå’Œdecoderä¸“å®¶ä½¿ç”¨ç‡
                encoder_expert_usage=train_metrics.get('encoder_expert_usage_rate', []),
                decoder_expert_usage=train_metrics.get('expert_usage_rate', [])
            )
            
            # ä¿å­˜æ£€æŸ¥ç‚¹ - åŒæ—¶è€ƒè™‘losså’ŒmAP
            is_best_loss = val_metrics.get('total_loss', float('inf')) < self.best_loss
            is_best_map = val_metrics.get('mAP_0.5_0.95', 0.0) > self.best_map
            
            if is_best_loss:
                self.best_loss = val_metrics.get('total_loss', float('inf'))
                self.logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.2f}")
            
            if is_best_map:
                self.best_map = val_metrics.get('mAP_0.5_0.95', 0.0)
                self.logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³mAP: {self.best_map:.4f}")
                self.save_checkpoint(epoch, is_best=True)
            
            # Early Stoppingæ£€æŸ¥ï¼ˆå‰30ä¸ªepochä¸æ£€æŸ¥mAPç›¸å…³çš„æŒ‡æ ‡ï¼‰
            if self.early_stopping and should_validate:
                # è·å–è¦ç›‘æ§çš„æŒ‡æ ‡å€¼
                metric_name = self.early_stopping.metric_name
                # å¦‚æœç›‘æ§çš„æ˜¯mAPç›¸å…³æŒ‡æ ‡ä¸”epoch < 30ï¼Œè·³è¿‡Early Stoppingæ£€æŸ¥
                is_map_metric = any(x in metric_name for x in ['mAP', 'AP'])
                if is_map_metric and epoch < 30:
                    # å‰30ä¸ªepochä¸è¿›è¡ŒmAPè¯„ä¼°ï¼Œè·³è¿‡Early Stoppingæ£€æŸ¥
                    pass
                else:
                    if 'mAP_0.5_0.95' in metric_name or 'mAP_0.5:0.95' in metric_name:
                        metric_value = val_metrics.get('mAP_0.5_0.95', 0.0)
                    elif 'mAP_0.5' in metric_name:
                        metric_value = val_metrics.get('mAP_0.5', 0.0)
                    elif 'mAP_0.75' in metric_name:
                        metric_value = val_metrics.get('mAP_0.75', 0.0)
                    elif 'loss' in metric_name.lower():
                        metric_value = val_metrics.get('total_loss', float('inf'))
                    else:
                        metric_value = val_metrics.get('mAP_0.5_0.95', 0.0)  # é»˜è®¤
                    
                    if self.early_stopping(metric_value, epoch):
                        self.logger.info(f"Early Stoppingåœ¨epoch {epoch}è§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                        break
            
            # æ¯ä¸ªepochéƒ½ä¿å­˜latestç”¨äºæ–­ç‚¹ç»­è®­ï¼ˆä¸ä¼šå †ç§¯æ–‡ä»¶ï¼‰
            self.save_latest_checkpoint(epoch)
            
            # [å†…å­˜ä¼˜åŒ–] åœ¨æ¯ä¸ª Epoch ç»“æŸã€ä¿å­˜å®Œæ¨¡å‹åï¼Œæ˜¾å¼æ¸…ç†ä¸´æ—¶æŒ‡æ ‡å˜é‡
            del train_metrics, val_metrics
            if should_validate:
                # éªŒè¯æ—¶ä¼šäº§ç”Ÿé¢å¤–çš„ä¸´æ—¶å˜é‡ï¼Œä¹Ÿéœ€è¦æ¸…ç†
                pass
            # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
            gc.collect()
            # å¦‚æœä½¿ç”¨CUDAï¼Œæ¸…ç©ºç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # æ¯11ä¸ªepochä¿å­˜Tokené‡è¦æ€§çƒ­åŠ›å›¾ï¼ˆç¬¬11ã€21ã€31...æ¬¡ï¼‰
            if (epoch + 1) % 10 == 0:
                try:
                    self._save_token_visualization(epoch)
                except Exception as e:
                    self.logger.debug(f"Tokenå¯è§†åŒ–å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆæ¯ä¸ªepochéƒ½æ›´æ–°ï¼‰
            try:
                self.visualizer.plot()
            except Exception as e:
                self.logger.warning(f"ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¤±è´¥: {e}")
        
        self.logger.info("âœ“ è®­ç»ƒå®Œæˆï¼")
        
        # æœ€åç»˜åˆ¶ä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒæ›²çº¿å¹¶å¯¼å‡ºCSV
        try:
            self.visualizer.plot()
            self.visualizer.export_to_csv()
            self.logger.info(f"âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {self.log_dir}/training_curves.png")
            self.logger.info(f"âœ“ è®­ç»ƒå†å²å·²å¯¼å‡ºåˆ°: {self.log_dir}/training_history.csv")
        except Exception as e:
            self.logger.error(f"ç»˜åˆ¶æœ€ç»ˆè®­ç»ƒæ›²çº¿å¤±è´¥: {e}")
        
        # è®­ç»ƒç»“æŸæ—¶ä½¿ç”¨best_modelè¾“å‡º5å¼ æ¨ç†å›¾åƒ
        self.logger.info("=" * 60)
        self.logger.info("ä½¿ç”¨best_modelç”Ÿæˆæ¨ç†ç»“æœï¼ˆ5å¼ å›¾åƒï¼‰...")
        try:
            best_model_path = self.log_dir / 'best_model.pth'
            if best_model_path.exists():
                # åŠ è½½best_modelçš„checkpoint
                checkpoint = torch.load(best_model_path, map_location=self.device, weights_only=False)
                best_ema_state = checkpoint.get('ema_state_dict', None)
                best_epoch = checkpoint.get('epoch', None)  # è·å–best_modelä¿å­˜æ—¶çš„epoch
                
                # ä½¿ç”¨best_modelè¿›è¡Œæ¨ç†ï¼ˆä¼ å…¥best_epochç”¨äºæ–‡ä»¶åï¼‰
                self._run_inference_on_best_model(best_ema_state, best_epoch=best_epoch)
            else:
                self.logger.warning("æœªæ‰¾åˆ°best_model.pthï¼Œè·³è¿‡æ¨ç†")
        except Exception as e:
            self.logger.warning(f"è®­ç»ƒç»“æŸæ—¶æ¨ç†å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒç»“æœï¼‰: {e}")


def main() -> None:
    """ä¸»å‡½æ•°ã€‚"""
    

    parser = argparse.ArgumentParser(description='è‡ªé€‚åº”ä¸“å®¶RT-DETRè®­ç»ƒ')
    parser.add_argument('--config', type=str, default='A', 
                       help='ä¸“å®¶é…ç½® (A: 6ä¸“å®¶, B: 3ä¸“å®¶) æˆ–YAMLé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--backbone', type=str, default='presnet34', 
                       choices=['presnet18', 'presnet34', 'presnet50', 'presnet101',
                               'hgnetv2_l', 'hgnetv2_x', 'hgnetv2_h',
                               'cspresnet_s', 'cspresnet_m', 'cspresnet_l', 'cspresnet_x',
                               'cspdarknet', 'mresnet'],
                       help='Backboneç±»å‹')
    parser.add_argument('--data_root', type=str, default='datasets/DAIR-V2X', 
                       help='DAIR-V2Xæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å° (RTX 5090ä¼˜åŒ–)')
    parser.add_argument('--pretrained_lr', type=float, default=1e-5, help='é¢„è®­ç»ƒç»„ä»¶å­¦ä¹ ç‡')
    parser.add_argument('--new_lr', type=float, default=1e-4, help='æ–°ç»„ä»¶å­¦ä¹ ç‡')
    parser.add_argument('--top_k', type=int, default=3, help='è·¯ç”±å™¨Top-K')
    parser.add_argument('--pretrained_weights', type=str, default=None,
                       help='é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆRT-DETR COCOé¢„è®­ç»ƒæ¨¡å‹ï¼‰')
    parser.add_argument('--resume_from_checkpoint', type=str, default=None,
                       help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼ˆæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼‰')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­ï¼Œç”¨äºç¡®ä¿å®éªŒå¯é‡å¤æ€§ï¼ˆé»˜è®¤ï¼š42ï¼‰')
    parser.add_argument('--deterministic', action='store_true',
                       help='ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ï¼ˆä¼šé™ä½é€Ÿåº¦ä½†ä¿è¯å®Œå…¨å¯é‡å¤ï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰æ“ä½œä¹‹å‰ï¼‰
    print("\n" + "="*60)
    print("ğŸ”§ åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ")
    print("="*60)

    if torch.cuda.is_available():
        import os
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        torch.cuda.empty_cache()
        print("âœ“ å·²å¯ç”¨æ˜¾å­˜ç¢ç‰‡æ•´ç†ç­–ç•¥: expandable_segments=True")

    set_seed(args.seed, deterministic=args.deterministic)
    
    # åŠ è½½é…ç½®
    config_file_path = None
    if args.config and args.config.endswith('.yaml'):
        # ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
        config_file_path = args.config
        with open(args.config, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"ğŸ“„ ä»é…ç½®æ–‡ä»¶åŠ è½½: {args.config}")
        
        # ç¡®ä¿å­¦ä¹ ç‡ç›¸å…³å€¼æ˜¯æµ®ç‚¹æ•°ï¼ˆYAMLä¸­çš„ç§‘å­¦è®¡æ•°æ³•å¯èƒ½è¢«è§£æä¸ºå­—ç¬¦ä¸²ï¼‰
        if 'training' in config:
            if 'pretrained_lr' in config['training']:
                config['training']['pretrained_lr'] = float(config['training']['pretrained_lr'])
            if 'new_lr' in config['training']:
                config['training']['new_lr'] = float(config['training']['new_lr'])
            if 'eta_min' in config['training']:
                config['training']['eta_min'] = float(config['training']['eta_min'])
        
        # åªå…è®¸æ˜¾å¼ä¼ é€’çš„å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶ï¼ˆä¸ç­‰äºé»˜è®¤å€¼çš„æ‰è¦†ç›–ï¼‰
        if args.backbone != 'presnet34':
            config['model']['backbone'] = args.backbone
        if args.epochs != 100:
            config['training']['epochs'] = args.epochs
        if args.batch_size != 16:
            config['training']['batch_size'] = args.batch_size
        if args.pretrained_lr != 1e-5:
            config['training']['pretrained_lr'] = args.pretrained_lr
        if args.new_lr != 1e-4:
            config['training']['new_lr'] = args.new_lr
        if args.top_k != 3:
            config['model']['top_k'] = args.top_k
        if args.data_root != 'datasets/DAIR-V2X':
            config['data']['data_root'] = args.data_root
        if args.pretrained_weights:
            config['model']['pretrained_weights'] = args.pretrained_weights
        
        # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„ resume_from_checkpoint
        if args.resume_from_checkpoint:
            if 'checkpoint' not in config:
                config['checkpoint'] = {}
            config['checkpoint']['resume_from_checkpoint'] = args.resume_from_checkpoint
    else:
        # åˆ›å»ºé»˜è®¤é…ç½®
        config = {
            'model': {
                'config': args.config,
                'hidden_dim': 256,
                'decoder_hidden_dim': 128,
                'num_queries': 300,
                'top_k': args.top_k,
                'backbone': args.backbone,
                'num_decoder_layers': 3,
                'encoder': {
                    'in_channels': [512, 1024, 2048],
                    'expansion': 1.0,
                    'num_encoder_layers': 1,
                    'use_encoder_idx': [1, 2]
                }
            },
            'data': {
                'data_root': args.data_root
            },
            'training': {
                'epochs': args.epochs,
                'batch_size': args.batch_size,
                'pretrained_lr': args.pretrained_lr,
                'new_lr': args.new_lr,
                'use_mosaic': False,  # ç¦ç”¨Mosaicï¼Œä¸é€‚åˆè·¯æµ‹æ¢å¤´åœºæ™¯ï¼ˆä¼šç ´åç©ºé—´å…³ç³»ï¼‰
                'warmup_epochs': 3,
                'ema_decay': 0.9999
            },
            'misc': {
                'device': 'cuda',
                'num_workers': 16,
                'pin_memory': True
            },
            'data_augmentation': {
                # [ä¿®æ”¹] å¤§å¹…æå‡å…‰ç…§å˜åŒ–çš„å¼ºåº¦ï¼Œå¯¹é½ YOLOv10
                'brightness': 0.4,   # åŸ 0.15 -> 0.4
                'contrast': 0.4,     # åŸ 0.15 -> 0.4
                'saturation': 0.7,   # åŸ 0.1 -> 0.7
                'hue': 0.015,        # åŸ 0.05 -> 0.015
                'crop_min': 0.1,
                'crop_max': 1.0,
                'flip_prob': 0.5,
                'color_jitter_prob': 0.0
            }
        }
        
        if args.pretrained_weights:
            config['model']['pretrained_weights'] = args.pretrained_weights
        
        if args.resume_from_checkpoint:
            config['checkpoint'] = {'resume_from_checkpoint': args.resume_from_checkpoint}
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DSETTrainer(config, config_file_path=config_file_path)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == '__main__':
    main()
