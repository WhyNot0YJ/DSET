#!/usr/bin/env python3
"""
æ¨¡å‹ç†è®ºæ•ˆç‡è¯„ä¼°è„šæœ¬ - æ”¯æŒ DSET, RT-DETR, Deformable-DETR, YOLOv8, YOLOv10

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨ä» logs/ ç›®å½•æŸ¥æ‰¾æœ€æ–°çš„ best_model.pth æˆ–ä½¿ç”¨æŒ‡å®šçš„æ£€æŸ¥ç‚¹
2. ä½¿ç”¨ pycocotools åœ¨éªŒè¯é›†ä¸Šè¿è¡Œ COCO è¯„ä¼°ï¼ˆä»…ç²¾åº¦æŒ‡æ ‡ï¼‰
3. è®¡ç®—æ¨¡å‹å‚æ•°é‡å’Œç†è®º FLOPsï¼ˆè€ƒè™‘ token pruning å’Œ MoE ç¨€ç–æ€§ï¼‰
4. ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ batch_sizeï¼ˆCOCO è¯„ä¼°ç»“æœä¸ batch_size æ— å…³ï¼Œå¯ä½¿ç”¨æ›´å¤§ batch_size åŠ é€Ÿï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python generate_benchmark_table.py --model_type dset
    python generate_benchmark_table.py --models_config models.json
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import time
import json
from pathlib import Path
from typing import Dict, Optional, Tuple, List
from io import StringIO
from pycocotools.cocoeval import COCOeval
from pycocotools.coco import COCO

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
_project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)
project_root = _project_root

# å°è¯•å¯¼å…¥ thop
try:
    from thop import profile
    HAS_THOP = True
except ImportError:
    HAS_THOP = False
    print("è­¦å‘Š: thop æœªå®‰è£…ï¼Œå°†æ— æ³•è®¡ç®— FLOPsã€‚è¯·è¿è¡Œ: pip install thop")


def _cuda_sync_if_available(device: str):
    """åŒæ­¥ CUDAï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    if device == "cuda" and torch.cuda.is_available():
        torch.cuda.synchronize()


def _load_checkpoint(checkpoint_path: str) -> dict:
    """åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    try:
        return torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    except TypeError:
        return torch.load(checkpoint_path, map_location='cpu')


def _extract_state_dict(checkpoint: dict) -> dict:
    """ä»æ£€æŸ¥ç‚¹ä¸­æå–çŠ¶æ€å­—å…¸"""
    if 'ema_state_dict' in checkpoint:
        state_dict = checkpoint['ema_state_dict']
        if isinstance(state_dict, dict) and 'module' in state_dict:
            state_dict = state_dict['module']
        return state_dict
    elif 'ema' in checkpoint and 'module' in checkpoint['ema']:
        return checkpoint['ema']['module']
    elif 'model_state_dict' in checkpoint:
        return checkpoint['model_state_dict']
    elif 'model' in checkpoint:
        return checkpoint['model']
    return checkpoint


# ==============================================================================
# Custom Ops Definitions (Based on Audit Report)
# ==============================================================================

# å…¨å±€å˜é‡ç”¨äºè·Ÿè¸ª MoE å±‚è°ƒç”¨ï¼ˆç”¨äºè°ƒè¯•ï¼‰
_moe_layer_call_count = {}
_moe_layer_debug = False

def count_moe_layer(m, x, y):
    """
    MoELayer Custom Op (Physics-Level Accurate)
    
    MoE å±‚æ— æ³•é€šè¿‡ç‰©ç† Shape è‡ªåŠ¨é™ç®—åŠ›ï¼ˆå› ä¸ºæ‰€æœ‰ä¸“å®¶éƒ½åœ¨è®¡ç®—å›¾ä¸­ï¼‰ï¼Œ
    éœ€è¦æ‰‹åŠ¨è®¡ç®—æ¿€æ´»è·¯å¾„çš„ FLOPsã€‚
    
    Formula: G_moe = G_router + (top_k / expert_num Ã— G_all_experts)
    - Router: å®Œæ•´çš„ Dense FLOPs (æ‰€æœ‰ä¸“å®¶éƒ½éœ€è¦è·¯ç”±è®¡ç®—)
    - Experts: è®¡ç®—æ‰€æœ‰ä¸“å®¶çš„ FLOPsï¼Œç„¶åä¹˜ä»¥æ¿€æ´»æ¯”ä¾‹ (top_k / expert_num)
    
    Args:
        m: MoE Layer module
        x: Input tuple, x[0] is [B, N, C] tensor (N is already pruned if inside Encoder)
        y: Output (not used)
    """
    global _moe_layer_call_count, _moe_layer_debug
    
    inp = x[0]
    if not torch.is_tensor(inp):
        return
    
    B, N, C = inp.shape
    
    num_experts = getattr(m, 'num_experts', 1)
    top_k = getattr(m, 'top_k', 1)
    dim_feedforward = getattr(m, 'dim_feedforward', 2048)
    
    # 1. Router: Linear(C, E) -> B*N*C*E (å®Œæ•´çš„ Dense FLOPs)
    router_flops = B * N * C * num_experts
    
    # 2. Experts: è®¡ç®—æ‰€æœ‰ä¸“å®¶çš„ FLOPsï¼Œç„¶åä¹˜ä»¥æ¿€æ´»æ¯”ä¾‹ (top_k / expert_num)
    # å•ä¸ª Expert çš„ MLP: Linear1(C->D) + Linear2(D->C) = 2 * C * D_ffn
    single_expert_flops = B * N * (2 * C * dim_feedforward)
    all_experts_flops = single_expert_flops * num_experts
    
    # åº”ç”¨æ¿€æ´»æ¯”ä¾‹ï¼šå®é™…åªæ¿€æ´»äº† top_k / expert_num æ¯”ä¾‹çš„ä¸“å®¶
    activation_ratio = top_k / max(num_experts, 1)  # é¿å…é™¤é›¶
    expert_flops = all_experts_flops * activation_ratio
    
    total = router_flops + expert_flops
    
    # è®¡ç®—å¦‚æœä½¿ç”¨æ‰€æœ‰ä¸“å®¶çš„ FLOPsï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    dense_flops = router_flops + all_experts_flops
    
    # è°ƒè¯•è¾“å‡º
    if _moe_layer_debug:
        # è·å–æ¨¡å—çš„å®Œæ•´åç§°ï¼ˆé€šè¿‡ id è·Ÿè¸ªï¼‰
        module_id = id(m)
        if module_id not in _moe_layer_call_count:
            _moe_layer_call_count[module_id] = {
                'count': 0,
                'name': str(type(m).__name__),
                'total_flops': 0,
                'dense_flops': 0
            }
        _moe_layer_call_count[module_id]['count'] += 1
        _moe_layer_call_count[module_id]['total_flops'] += total
        _moe_layer_call_count[module_id]['dense_flops'] += dense_flops
        call_num = _moe_layer_call_count[module_id]['count']
        
        total_gflops = total / 1e9
        dense_gflops = dense_flops / 1e9
        reduction = (1 - total / dense_flops) * 100 if dense_flops > 0 else 0
        
        print(f"      ğŸ”¹ MoE Layer è°ƒç”¨ #{call_num}: "
              f"shape=[B={B}, N={N}, C={C}], "
              f"experts={num_experts}, top_k={top_k}, "
              f"æœ¬æ¬¡ FLOPs={total_gflops:.4f}G "
              f"(Dense={dense_gflops:.4f}G, å‡å°‘={reduction:.1f}%)")
    
    m.total_ops += torch.DoubleTensor([int(total)])


def get_model_info(model, input_size: Tuple[int, int, int, int] = (1, 3, 736, 1280), 
                   is_yolo: bool = False, config: Dict = None, model_type: str = "dset",
                   debug: bool = False) -> Tuple[float, float, float, float]:
    """
    è®¡ç®—æ¨¡å‹çš„å‚æ•°é‡å’Œç†è®º FLOPs (Physics-Level Accurate)
    
    ä½¿ç”¨ thop custom_ops å®ç°ç‰©ç†çº§ç²¾ç¡®è®¡ç®—ï¼š
    - MoE Layer: Router (Dense) + TopK Experts (Sparse)
      Formula: G_moe = G_router + (top_k / expert_num Ã— G_all_experts)
      Note: MoE å±‚æ— æ³•é€šè¿‡ç‰©ç† Shape è‡ªåŠ¨é™ç®—åŠ›ï¼ˆå› ä¸ºæ‰€æœ‰ä¸“å®¶éƒ½åœ¨è®¡ç®—å›¾ä¸­ï¼‰ï¼Œ
            éœ€è¦åœ¨è‡ªå®šä¹‰ç®—å­ä¸­æ‰‹åŠ¨è®¡ç®—æ¿€æ´»è·¯å¾„çš„ FLOPsã€‚
    
    æ ¸å¿ƒé€»è¾‘ï¼š
    1. Token Pruning æ˜¯ç‰©ç†å‰ªæï¼ˆé€šè¿‡ TokenPruner å¯¹ Tensor è¿›è¡Œç‰©ç†åˆ‡ç‰‡ï¼‰ï¼Œ
       ç›´æ¥æ”¹å˜äº†åç»­ç®—å­çš„è¾“å…¥ Shapeï¼Œthop çš„ profile ä¼šè‡ªåŠ¨æ•è·è®¡ç®—é‡çš„ä¸‹é™ã€‚
       å› æ­¤ MSDeformableAttention å’Œ MultiheadAttention ç­‰ç®—å­ä¸éœ€è¦è‡ªå®šä¹‰å‡½æ•°ï¼Œ
       åªéœ€ç¡®ä¿åœ¨è¿è¡Œ profile å‰ï¼Œé€šè¿‡ model.set_epoch(999) æ¿€æ´»å‰ªæé€»è¾‘å³å¯ã€‚
    2. MoE å±‚æ— æ³•é€šè¿‡ç‰©ç† Shape è‡ªåŠ¨é™ç®—åŠ›ï¼ˆå› ä¸ºæ‰€æœ‰ä¸“å®¶éƒ½åœ¨è®¡ç®—å›¾ä¸­ï¼‰ï¼Œ
       éœ€è¦åœ¨è‡ªå®šä¹‰ç®—å­ä¸­æ‰‹åŠ¨è®¡ç®—ï¼šRouter å®Œæ•´è®¡ç®—ï¼ŒExperts è®¡ç®—æ‰€æœ‰ä¸“å®¶åä¹˜ä»¥æ¿€æ´»æ¯”ä¾‹ (top_k / expert_num)ã€‚
    3. ç›´æ¥è®© profile åœ¨å¯ç”¨å‰ªæçš„æ¨¡å‹ä¸Šè¿è¡Œä¸€éï¼Œç»“æœå³ä¸ºçœŸå®çš„ Theory GFLOPsã€‚
       ä¸éœ€è¦æ‰‹åŠ¨ç¼©æ”¾æˆ–æ‹†åˆ†æ¨¡å—ã€‚
    """
    # ========================== 1. å‚æ•°é‡è®¡ç®— ==========================
    if is_yolo and hasattr(model, 'model'):
        pytorch_model = model.model
    else:
        pytorch_model = model
    total_params = sum(p.numel() for p in pytorch_model.parameters())
    
    # Active Params (Simple Estimation based on config)
    active_params = total_params
    if model_type == "dset" and config:
        dset_cfg = config.get('model', {}).get('dset', {})
        enc_k = dset_cfg.get('encoder_moe_top_k', 1)
        enc_e = dset_cfg.get('encoder_moe_num_experts', 1)
        dec_k = config.get('model', {}).get('top_k', 3)
        dec_e = config.get('model', {}).get('num_experts', 1)
        
        enc_r = min(enc_k, enc_e) / max(enc_e, 1) if enc_e > 0 else 1
        dec_r = min(dec_k, dec_e) / max(dec_e, 1) if dec_e > 0 else 1
        
        p_moe = 0
        for n, p in pytorch_model.named_parameters():
            if 'expert' in n.lower() or 'moe' in n.lower():
                ratio = enc_r if 'encoder' in n.lower() else dec_r
                active_params -= p.numel() * (1 - ratio)
    
    total_params_m = total_params / 1e6
    active_params_m = active_params / 1e6
    print(f"\n  ğŸ“Š Params: Total={total_params_m:.2f}M, Active={active_params_m:.2f}M")

    # ========================== 2. FLOPs Calculation (Physics-Level Accurate) ==========================
    base_flops_g = 0.0
    theory_flops_g = 0.0
    
    if HAS_THOP:
        try:
            from copy import deepcopy
            model_eval = deepcopy(model).eval()
            device = next(model_eval.parameters()).device
            dummy_img = torch.randn(input_size).to(device)
            
            # --- Auto-Register Custom Ops ---
            # æ³¨æ„ï¼šåªæœ‰ MoE Layer éœ€è¦è‡ªå®šä¹‰å‡½æ•°ï¼Œå› ä¸º MoE çš„ç¨€ç–æ€§æ— æ³•é€šè¿‡ç‰©ç† shape è‡ªåŠ¨æ•è·
            # ï¼ˆæ‰€æœ‰ä¸“å®¶éƒ½åœ¨è®¡ç®—å›¾ä¸­ï¼Œéœ€è¦æ‰‹åŠ¨è®¡ç®—æ¿€æ´»è·¯å¾„çš„ FLOPsï¼‰
            # MSDeformableAttention å’Œ MultiheadAttention ä¸éœ€è¦è‡ªå®šä¹‰å‡½æ•°ï¼š
            # Token Pruning æ˜¯ç‰©ç†å‰ªæï¼Œä¼šç›´æ¥æ”¹å˜è¾“å…¥ tensor çš„ shapeï¼Œ
            # thop çš„ profile ä¼šè‡ªåŠ¨æ•è·è®¡ç®—é‡çš„ä¸‹é™ã€‚
            custom_ops_map = {}
            
            # Debug: æ‰“å°æ‰€æœ‰æ¨¡å—åç§°å’Œç±»å‹ï¼Œç‰¹åˆ«å…³æ³¨ MoE ç›¸å…³æ¨¡å—
            if debug:
                print("\n  ğŸ” æ‰«ææ‰€æœ‰æ¨¡å—...")
                moe_candidates = []
                all_modules_info = []
                for name, module in model_eval.named_modules():
                    module_type = module.__class__.__name__
                    all_modules_info.append((name, module_type))
                    # æŸ¥æ‰¾å¯èƒ½åŒ…å« MoE çš„æ¨¡å—
                    if "MoE" in module_type or "moe" in name.lower() or "expert" in name.lower():
                        moe_candidates.append((name, module_type))
                
                print(f"  ğŸ“‹ æ€»å…±æ‰¾åˆ° {len(all_modules_info)} ä¸ªæ¨¡å—")
                if moe_candidates:
                    print(f"  ğŸ¯ æ‰¾åˆ° {len(moe_candidates)} ä¸ªå¯èƒ½çš„ MoE ç›¸å…³æ¨¡å—:")
                    for name, module_type in moe_candidates:
                        print(f"      - {name}: {module_type}")
                else:
                    print("  âš  æœªæ‰¾åˆ° MoE ç›¸å…³æ¨¡å—")
                
                # æ‰“å°æ‰€æœ‰æ¨¡å—ç±»å‹ï¼ˆå»é‡ï¼‰
                unique_types = {}
                for name, module_type in all_modules_info:
                    if module_type not in unique_types:
                        unique_types[module_type] = []
                    unique_types[module_type].append(name)
                
                print(f"\n  ğŸ“Š æ‰€æœ‰æ¨¡å—ç±»å‹ç»Ÿè®¡ï¼ˆå…± {len(unique_types)} ç§ï¼‰:")
                for module_type in sorted(unique_types.keys()):
                    count = len(unique_types[module_type])
                    if count <= 3:
                        examples = ", ".join(unique_types[module_type])
                        print(f"      {module_type}: {count} ä¸ª (ä¾‹å¦‚: {examples})")
                    else:
                        examples = ", ".join(unique_types[module_type][:3]) + "..."
                        print(f"      {module_type}: {count} ä¸ª (ä¾‹å¦‚: {examples})")
            
            # æ³¨å†Œ MoE Layer è‡ªå®šä¹‰å‡½æ•°
            global _moe_layer_call_count, _moe_layer_debug
            _moe_layer_debug = debug
            
            for m in model_eval.modules():
                name = m.__class__.__name__
                if "MoELayer" in name:
                    custom_ops_map[m.__class__] = count_moe_layer
            
            if debug:
                print(f"\n  âœ… Custom Ops Registered: {list(k.__name__ for k in custom_ops_map.keys())}")
                if not custom_ops_map:
                    print("  âš  è­¦å‘Š: æœªæ³¨å†Œä»»ä½•è‡ªå®šä¹‰æ“ä½œï¼Œå¯èƒ½ MoE å±‚çš„ç±»åä¸æ˜¯ 'MoELayer'")
                    print("  ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ä¸Šé¢çš„æ¨¡å—ç±»å‹åˆ—è¡¨ï¼Œæ‰¾åˆ°æ­£ç¡®çš„ MoE å±‚ç±»å")

            # --- A. Measure Base FLOPs (Dense, r=1.0) ---
            # Disable Pruning for Base calculation
            for m in model_eval.modules():
                if hasattr(m, 'pruning_enabled'):
                    m.pruning_enabled = False
                if hasattr(m, 'set_epoch'):
                    # Set epoch to 0 to disable pruning
                    m.set_epoch(0)
            
            if debug:
                _moe_layer_call_count.clear()
                print(f"\n  ğŸ“Š è®¡ç®— Base FLOPs (Dense, r=1.0)...")
                if custom_ops_map:
                    print(f"      MoE å±‚è°ƒç”¨ç»Ÿè®¡:")
            
            base_macs, _ = profile(model_eval, inputs=(dummy_img,), custom_ops=custom_ops_map, verbose=False)
            base_flops_g = base_macs / 1e9
            print(f"  âœ“ Base FLOPs (Dense, r=1.0): {base_flops_g:.2f} G")
            
            # --- B. Measure Theory FLOPs (With Pruning) ---
            # Token Pruning æ˜¯ç‰©ç†å‰ªæï¼ˆé€šè¿‡ TokenPruner å¯¹ Tensor è¿›è¡Œç‰©ç†åˆ‡ç‰‡ï¼‰ï¼Œ
            # ç›´æ¥æ”¹å˜äº†åç»­ç®—å­çš„è¾“å…¥ Shapeï¼Œthop çš„ profile ä¼šè‡ªåŠ¨æ•è·è®¡ç®—é‡çš„ä¸‹é™ã€‚
            # åªéœ€ç¡®ä¿åœ¨è¿è¡Œ profile å‰ï¼Œé€šè¿‡ model.set_epoch(999) æ¿€æ´»å‰ªæé€»è¾‘å³å¯ã€‚
            
            if model_type == "dset":
                # Enable Pruning: Set epoch to a large value to ensure pruning is fully enabled
                for m in model_eval.modules():
                    if hasattr(m, 'set_epoch'):
                        m.set_epoch(999)  # Large epoch to ensure warmup is done
                    if hasattr(m, 'pruning_enabled'):
                        m.pruning_enabled = True
                    if hasattr(m, 'current_epoch'):
                        m.current_epoch = 999
                
                # Get token_keep_ratio for display purposes only
                dset_cfg = config.get('model', {}).get('dset', {})
                r = dset_cfg.get('token_keep_ratio', 1.0)
                if isinstance(r, dict):
                    r = max(r.values())
            else:
                r = 1.0  # é»˜è®¤å€¼ï¼Œç”¨äºé DSET æ¨¡å‹
            
            # Direct profile on pruned model - this is the Theory FLOPs
            # No manual scaling needed: physical pruning changes tensor shapes automatically
            if debug:
                _moe_layer_call_count.clear()
                print(f"\n  ğŸ“Š è®¡ç®— Theory FLOPs (With Pruning, r={r:.2f})...")
                if custom_ops_map:
                    print(f"      MoE å±‚è°ƒç”¨ç»Ÿè®¡:")
            
            theory_macs, _ = profile(model_eval, inputs=(dummy_img,), custom_ops=custom_ops_map, verbose=False)
            theory_flops_g = theory_macs / 1e9
            
            if debug and _moe_layer_call_count:
                print(f"\n      ğŸ“ˆ MoE å±‚è°ƒç”¨æ€»ç»“:")
                total_moe_flops = 0
                total_moe_dense_flops = 0
                for module_id, info in _moe_layer_call_count.items():
                    total_moe_flops += info['total_flops']
                    total_moe_dense_flops += info['dense_flops']
                    total_gflops = info['total_flops'] / 1e9
                    dense_gflops = info['dense_flops'] / 1e9
                    reduction = (1 - info['total_flops'] / info['dense_flops']) * 100 if info['dense_flops'] > 0 else 0
                    print(f"        - {info['name']} (id={module_id}): "
                          f"è°ƒç”¨ {info['count']} æ¬¡, "
                          f"ç´¯è®¡ FLOPs={total_gflops:.4f}G "
                          f"(Dense={dense_gflops:.4f}G, å‡å°‘={reduction:.1f}%)")
                
                if len(_moe_layer_call_count) > 1:
                    total_moe_gflops = total_moe_flops / 1e9
                    total_moe_dense_gflops = total_moe_dense_flops / 1e9
                    total_reduction = (1 - total_moe_flops / total_moe_dense_flops) * 100 if total_moe_dense_flops > 0 else 0
                    print(f"        ğŸ“Š æ‰€æœ‰ MoE å±‚æ€»è®¡: "
                          f"FLOPs={total_moe_gflops:.4f}G "
                          f"(Dense={total_moe_dense_gflops:.4f}G, æ€»å‡å°‘={total_reduction:.1f}%)")
            
            print(f"  âœ“ Theory FLOPs (With Pruning, r={r:.2f}): {theory_flops_g:.2f} G")
            if model_type == "dset" and r < 1.0:
                reduction = (1 - theory_flops_g / base_flops_g) * 100 if base_flops_g > 0 else 0
                print(f"  ğŸ’¡ FLOPs Reduction: {reduction:.1f}% (automatically captured by physical pruning)")
                
            del model_eval
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"  âš  FLOPs Calculation Failed: {e}")
            import traceback
            traceback.print_exc()
            theory_flops_g = base_flops_g

    return total_params_m, active_params_m, base_flops_g, theory_flops_g


def load_dset_model(config_path: str, checkpoint_path: str, device: str = "cuda"):
    """åŠ è½½ DSET æ¨¡å‹"""
    try:
        from experiments.dset.train import DSETTrainer
    except ImportError:
        dset_dir = Path(config_path).parent.parent
        if str(dset_dir) not in sys.path:
            sys.path.insert(0, str(dset_dir))
        from train import DSETTrainer
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    if 'misc' not in config:
        config['misc'] = {}
    config['misc']['device'] = device
    
    trainer = DSETTrainer(config, config_file_path=str(config_path))
    model = trainer.model
    
    checkpoint = _load_checkpoint(checkpoint_path)
    state_dict = _extract_state_dict(checkpoint)
    load_result = model.load_state_dict(state_dict, strict=False)
    if load_result.missing_keys or load_result.unexpected_keys:
        if load_result.missing_keys:
            print(f"  âš  missing_keys: {len(load_result.missing_keys)} ä¸ª")
            for k in load_result.missing_keys[:5]:
                print(f"      - {k}")
            if len(load_result.missing_keys) > 5:
                print(f"      ... ç­‰ {len(load_result.missing_keys)} ä¸ª")
        if load_result.unexpected_keys:
            print(f"  âš  unexpected_keys: {len(load_result.unexpected_keys)} ä¸ª")
            for k in load_result.unexpected_keys[:5]:
                print(f"      - {k}")
            if len(load_result.unexpected_keys) > 5:
                print(f"      ... ç­‰ {len(load_result.unexpected_keys)} ä¸ª")
    model.eval()
    
    # å¯ç”¨ token pruning - å¼ºåˆ¶è®¾ç½® epoch=100 ä»¥è·¨è¶Š warmup é˜¶æ®µ
    if hasattr(model, 'encoder') and hasattr(model.encoder, 'set_epoch'):
        dset_config = config.get('model', {}).get('dset', {})
        warmup_epochs = dset_config.get('token_pruning_warmup_epochs', 10)
        target_keep_ratio = dset_config.get('token_keep_ratio', 1.0)
        
        # å¼ºåˆ¶ epoch=100 ä»¥ç¡®ä¿å‰ªæå®Œå…¨æ¿€æ´»ï¼ˆprogress=1.0ï¼‰
        forced_epoch = 100
        model.encoder.set_epoch(forced_epoch)
        
        # éªŒè¯å‰ªæçŠ¶æ€
        if hasattr(model.encoder, 'token_pruners') and model.encoder.token_pruners:
            pruner = model.encoder.token_pruners[0]
            actual_keep_ratio = pruner.get_current_keep_ratio() if hasattr(pruner, 'get_current_keep_ratio') else None
            pruning_enabled = pruner.pruning_enabled if hasattr(pruner, 'pruning_enabled') else False
            print(f"  âœ“ Token Pruning: epoch={forced_epoch}, warmup={warmup_epochs}")
            print(f"    - pruning_enabled: {pruning_enabled}")
            print(f"    - target_keep_ratio: {target_keep_ratio}")
            print(f"    - actual_keep_ratio: {actual_keep_ratio}")
        else:
            print(f"  âœ“ å·²å¯ç”¨ token pruning (epoch={forced_epoch})")
    
    return model, config


def load_rtdetr_model(config_path: str, checkpoint_path: str, device: str = "cuda"):
    """åŠ è½½ RT-DETRv2 æ¨¡å‹"""
    rtdetr_dir = Path(config_path).parent.parent
    if str(rtdetr_dir) not in sys.path:
        sys.path.insert(0, str(rtdetr_dir))
    from train import RTDETRTrainer
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    if 'misc' not in config:
        config['misc'] = {}
    config['misc']['device'] = device
    
    trainer = RTDETRTrainer(config)
    if trainer.logger is None:
        class SimpleLogger:
            def info(self, msg): pass
        trainer.logger = SimpleLogger()
    
    model = trainer.create_model()
    checkpoint = _load_checkpoint(checkpoint_path)
    state_dict = _extract_state_dict(checkpoint)
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    model = model.to(device)
    
    return model, config


def load_deformable_detr_model(checkpoint_path: str, device: str = "cuda", config_path: str = None):
    """åŠ è½½ Deformable-DETR æ¨¡å‹"""
    try:
        from mmengine.config import Config
        from mmdet.registry import MODELS
    except ImportError:
        raise ImportError("éœ€è¦å®‰è£… mmengine å’Œ mmdet")

    # å…³é”®ï¼šç¡®ä¿ mmdet çš„æ‰€æœ‰æ¨¡å—éƒ½å·²æ³¨å†Œåˆ° mmengine Registryï¼Œå¦åˆ™ä¼šå‡ºç°
    # "DetDataPreprocessor is not in the mmengine::model registry" ä¹‹ç±»çš„é”™è¯¯ã€‚
    try:
        from mmdet.utils import register_all_modules
        try:
            # æ–°ç‰ˆ mmdet æ¨èï¼šåŒæ—¶åˆå§‹åŒ– default scope
            register_all_modules(init_default_scope=True)
        except TypeError:
            # å…¼å®¹æ—§ç‰ˆç­¾å
            register_all_modules()
    except Exception:
        # æŸäº›ç¯å¢ƒå¯èƒ½æ²¡æœ‰è¯¥å·¥å…·å‡½æ•°ï¼Œä½†æ­£å¸¸ import mmdet ä¹Ÿä¼šè§¦å‘æ³¨å†Œ
        try:
            import mmdet  # noqa: F401
        except Exception:
            pass
    
    checkpoint = _load_checkpoint(checkpoint_path)
    state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))
    
    cfg = None
    # ä¼˜å…ˆä½¿ç”¨æ˜¾å¼æä¾›çš„ config_pathï¼ˆæ›´ç¨³å®šã€å¯å¤ç°ï¼‰
    if config_path and os.path.exists(config_path):
        cfg = Config.fromfile(config_path)
    # å›é€€ï¼šå°è¯•ä» checkpoint meta ä¸­æ¢å¤é…ç½®
    elif 'meta' in checkpoint and 'cfg' in checkpoint['meta']:
        meta_cfg = checkpoint['meta']['cfg']
        # å…¼å®¹ï¼šæœ‰äº› mmdet/mmengine checkpoint ä¼šæŠŠ cfg ä¿å­˜ä¸º dictï¼›ä¹Ÿæœ‰å¾ˆå¤šä¿å­˜ä¸º strï¼ˆæ–‡ä»¶è·¯å¾„æˆ–æ–‡æœ¬å†…å®¹ï¼‰
        if isinstance(meta_cfg, dict):
            cfg = Config(meta_cfg)
        elif isinstance(meta_cfg, str):
            # 1) å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„
            if os.path.exists(meta_cfg):
                cfg = Config.fromfile(meta_cfg)
            else:
                # 2) å¯èƒ½æ˜¯ python config æ–‡æœ¬å†…å®¹
                if hasattr(Config, 'fromstring'):
                    try:
                        cfg = Config.fromstring(meta_cfg, file_format='.py')
                    except TypeError:
                        # å…¼å®¹æ—§ç‰ˆæœ¬ç­¾å
                        cfg = Config.fromstring(meta_cfg)
                else:
                    # 3) æç«¯å…¼å®¹ï¼šå†™å…¥ä¸´æ—¶æ–‡ä»¶å† fromfile
                    import tempfile
                    with tempfile.NamedTemporaryFile('w', suffix='.py', delete=False, encoding='utf-8') as f:
                        f.write(meta_cfg)
                        tmp_cfg_path = f.name
                    cfg = Config.fromfile(tmp_cfg_path)
    
    if cfg is None:
        raise FileNotFoundError("æ— æ³•æ‰¾åˆ° Deformable-DETR é…ç½®æ–‡ä»¶")
    
    model = MODELS.build(cfg.model)
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    model = model.to(device)
    
    # å°† Config å¯¹è±¡è½¬æ¢ä¸º dict ä»¥ä¾¿åç»­å¤„ç†
    config_dict = {}
    try:
        if hasattr(cfg, '_cfg_dict'):
            config_dict = cfg._cfg_dict
        elif hasattr(cfg, 'to_dict'):
            config_dict = cfg.to_dict()
        elif hasattr(cfg, '__dict__'):
            config_dict = dict(cfg.__dict__)
    except:
        pass
    
    return model, config_dict


def _ensure_yolo_checkpoint_path(checkpoint_path: str) -> str:
    """ç¡®ä¿ YOLO æ£€æŸ¥ç‚¹è·¯å¾„ä½¿ç”¨ .pt åç¼€"""
    checkpoint_path_obj = Path(checkpoint_path)
    
    if checkpoint_path_obj.suffix.lower() == '.pt':
        return str(checkpoint_path_obj)
    
    if checkpoint_path_obj.suffix.lower() == '.pth':
        pt_path = checkpoint_path_obj.with_suffix('.pt')
        if pt_path.exists():
            return str(pt_path)
        if not checkpoint_path_obj.exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path_obj}")
        
        try:
            pt_path.symlink_to(checkpoint_path_obj)
            return str(pt_path)
        except OSError:
            import shutil
            shutil.copy2(checkpoint_path_obj, pt_path)
            return str(pt_path)
    
    return str(checkpoint_path_obj)


def load_yolov8_model(checkpoint_path: str, device: str = "cuda"):
    """åŠ è½½ YOLOv8 æ¨¡å‹"""
    yolov8_dir = Path(__file__).parent.parent.parent / "experiments" / "yolov8"
    if str(yolov8_dir) not in sys.path:
        sys.path.insert(0, str(yolov8_dir))
    from ultralytics import YOLO
    
    checkpoint_path_pt = _ensure_yolo_checkpoint_path(checkpoint_path)
    model = YOLO(checkpoint_path_pt)
    model.to(device)
    model.eval()
    
    # æ„å»ºé…ç½®å­—å…¸
    config = {
        'model_type': 'yolov8',
        'data': None
    }
    
    # å°è¯•ä»æ¨¡å‹å¯¹è±¡ä¸­è·å–æ•°æ®é›†è·¯å¾„
    try:
        if hasattr(model, 'ckpt') and model.ckpt and hasattr(model.ckpt, 'data'):
            config['data'] = model.ckpt.data
    except:
        pass
    
    return model, config


def load_yolov10_model(checkpoint_path: str, device: str = "cuda"):
    """åŠ è½½ YOLOv10 æ¨¡å‹"""
    yolov10_dir = Path(__file__).parent.parent.parent / "experiments" / "yolov10"
    if str(yolov10_dir) not in sys.path:
        sys.path.insert(0, str(yolov10_dir))
    from ultralytics import YOLO
    
    checkpoint_path_pt = _ensure_yolo_checkpoint_path(checkpoint_path)
    model = YOLO(checkpoint_path_pt)
    model.to(device)
    model.eval()
    
    # æ„å»ºé…ç½®å­—å…¸
    config = {
        'model_type': 'yolov10',
        'data': None
    }
    
    # å°è¯•ä»æ¨¡å‹å¯¹è±¡ä¸­è·å–æ•°æ®é›†è·¯å¾„
    try:
        if hasattr(model, 'ckpt') and model.ckpt and hasattr(model.ckpt, 'data'):
            config['data'] = model.ckpt.data
    except:
        pass
    
    return model, config


def evaluate_yolo_accuracy(model, config_path: str, device: str = "cuda", max_samples: int = 300) -> Dict[str, float]:
    """ä½¿ç”¨ YOLO model.val() è¿›è¡Œè¯„ä¼°ï¼ˆä»…ç²¾åº¦ï¼Œæ— æ€§èƒ½æµ‹è¯•ï¼‰"""
    try:
        print(f"  âœ“ ä½¿ç”¨ YOLO model.val() è¿›è¡Œè¯„ä¼°")
        
        results = model.val(
            data=str(config_path),
            device=device,
            verbose=False,
            max_det=300,
            batch=1
        )
        
        metrics = {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}
        
        if hasattr(results, 'box'):
            if hasattr(results.box, 'map'):
                metrics['mAP'] = float(results.box.map)
            if hasattr(results.box, 'map50'):
                metrics['AP50'] = float(results.box.map50)
            if hasattr(results.box, 'maps') and len(results.box.maps) > 1:
                metrics['APS'] = float(results.box.maps[1])
        
        print(f"  âœ“ mAP: {metrics['mAP']:.3f}, AP50: {metrics['AP50']:.3f}, APS: {metrics['APS']:.3f}")
        return metrics
        
    except Exception as e:
        print(f"  âš  YOLO è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}


def evaluate_deformable_detr_accuracy(model, config_path: str, device: str = "cuda") -> Dict[str, float]:
    """è¯„ä¼° Deformable-DETR æ¨¡å‹ï¼ˆä½¿ç”¨ mmdet/mmengine Runner.test()ï¼Œä»…ç²¾åº¦ï¼‰"""
    metrics = evaluate_deformable_detr_full(
        config_path=config_path,
        checkpoint_path=None,
        device=device,
    )
    return metrics


def _safe_get_metric(metrics: Dict, keys: List[str], default: float = 0.0) -> float:
    """Robust metric key lookup across mmdet versions."""
    for k in keys:
        if k in metrics:
            try:
                return float(metrics[k])
            except Exception:
                pass
    # å…¼å®¹ï¼šæœ‰äº› evaluator ä¼šæŠŠ key å†™æˆ 'coco/bbox_mAP' æˆ– 'bbox_mAP'
    for k, v in metrics.items():
        if isinstance(k, str):
            for cand in keys:
                if k.endswith(cand) or cand in k:
                    try:
                        return float(v)
                    except Exception:
                        pass
    return float(default)


def _move_to_device(obj, device: str):
    """Recursively move tensors to device (for mmengine/mmdet batch dict)."""
    if torch.is_tensor(obj):
        return obj.to(device)
    if isinstance(obj, dict):
        return {k: _move_to_device(v, device) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        moved = [_move_to_device(v, device) for v in obj]
        return type(obj)(moved) if not isinstance(obj, tuple) else tuple(moved)
    return obj


def evaluate_deformable_detr_full(config_path: str,
                                  checkpoint_path: Optional[str],
                                  device: str = "cuda") -> Dict[str, float]:
    """è¯„ä¼° Deformable-DETR æ¨¡å‹ï¼ˆä»…ç²¾åº¦ï¼Œä½¿ç”¨ mmdet Runner.test()ï¼‰"""
    try:
        from mmengine.config import Config
        from mmengine.runner import Runner
        from mmdet.utils import register_all_modules
    except Exception as e:
        print(f"  âš  Deformable-DETR è¯„ä¼°ä¾èµ–å¯¼å…¥å¤±è´¥: {e!r}")
        return {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}
    
    # æ³¨å†Œ mmdet æ¨¡å—ï¼ˆä¸åŒç‰ˆæœ¬ç­¾åç•¥æœ‰å·®å¼‚ï¼‰
    try:
        register_all_modules(init_default_scope=True)
    except TypeError:
        register_all_modules()
    
    cfg = Config.fromfile(config_path)
    if checkpoint_path:
        cfg.load_from = checkpoint_path
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ batch_sizeï¼ˆä¸å¼ºåˆ¶ä¸º 1ï¼ŒCOCO è¯„ä¼°ç»“æœä¸ batch_size æ— å…³ï¼‰
    # å…¶ä½™é…ç½®ï¼ˆpipeline/resize/evaluator/proposal_nums/metric_items/num_workers ç­‰ï¼‰
    # ä¸¥æ ¼æ²¿ç”¨è®­ç»ƒè„šæœ¬ï¼ˆå¦‚ train_deformable_r18.pyï¼‰ç”Ÿæˆçš„ configï¼Œä¿è¯ä¸€è‡´æ€§ã€‚
    
    # é¿å…å†™æ—¥å¿—åˆ° work_dirï¼ˆRunner éœ€è¦ä½†æˆ‘ä»¬ä¸å…³å¿ƒï¼‰
    try:
        import tempfile
        cfg.work_dir = tempfile.mkdtemp(prefix='bench_deformable_detr_')
    except Exception:
        cfg.work_dir = cfg.get('work_dir', './work_dirs/bench_deformable_detr')
    
    runner = Runner.from_cfg(cfg)
    runner.model = runner.model.to(device)
    runner.model.eval()
    
    # ç²¾åº¦è¯„ä¼°ï¼ˆCOCO mAPï¼‰
    test_metrics = runner.test()
    
    mAP = _safe_get_metric(test_metrics, ['coco/bbox_mAP', 'bbox_mAP', 'mAP'], 0.0)
    AP50 = _safe_get_metric(test_metrics, ['coco/bbox_mAP_50', 'bbox_mAP_50', 'AP50', 'mAP_50'], 0.0)
    APS = _safe_get_metric(test_metrics, ['coco/bbox_mAP_s', 'bbox_mAP_s', 'APS', 'mAP_s'], 0.0)
    
    return {'mAP': mAP, 'AP50': AP50, 'APS': APS}


def _get_outputs_info(outputs: Dict) -> Tuple[torch.Tensor, torch.Tensor, bool]:
    """ä»æ¨¡å‹è¾“å‡ºä¸­æå– logits å’Œ boxes
    
    æ³¨æ„ï¼šDSET å’Œ RT-DETR å‡ä½¿ç”¨ Focal Lossï¼Œæ¨ç†æ—¶åº”ä½¿ç”¨ Sigmoid æ¿€æ´»
    """
    if 'pred_logits' in outputs:
        return outputs['pred_logits'], outputs['pred_boxes'], True  # RT-DETR: sigmoid
    elif 'class_scores' in outputs:
        return outputs['class_scores'], outputs['bboxes'], True  # DSET: sigmoid (Focal Loss)
    return None, None, False


def _collect_predictions_for_coco(outputs: Dict, targets: List[Dict], batch_idx: int,
                                  all_predictions: List, all_targets: List,
                                  img_w: int, img_h: int, batch_size: int) -> None:
    """æ”¶é›†é¢„æµ‹ç»“æœç”¨äº COCO è¯„ä¼°"""
    pred_logits, pred_boxes, use_sigmoid = _get_outputs_info(outputs)
    if pred_logits is None:
        return
    
    batch_size_actual = pred_logits.shape[0]
    
    for i in range(batch_size_actual):
        # å°è¯•ä» target ä¸­ç›´æ¥è·å–åŸå§‹ ID
        if i < len(targets) and 'image_id' in targets[i]:
            img_id = int(targets[i]['image_id'].item())
        else:
            # å¦‚æœæ²¡æœ‰ï¼Œå†é€€å›åˆ°ç´¢å¼•è®¡ç®—é€»è¾‘ï¼Œä½†è¦ç¡®ä¿ batch_size æ­£ç¡®
            img_id = batch_idx * batch_size + i
        
        if use_sigmoid:
            pred_scores = torch.sigmoid(pred_logits[i])
        else:
            pred_scores = torch.softmax(pred_logits[i], dim=-1)
        max_scores, pred_classes = torch.max(pred_scores, dim=-1)
        
        valid_boxes_mask = ~torch.all(pred_boxes[i] == 1.0, dim=1)
        valid_indices = torch.where(valid_boxes_mask)[0]
        
        if len(valid_indices) > 0:
            filtered_boxes = pred_boxes[i][valid_indices]
            filtered_classes = pred_classes[valid_indices]
            filtered_scores = max_scores[valid_indices]
            
            if filtered_boxes.shape[0] > 0:
                boxes_coco = torch.zeros_like(filtered_boxes)
                if filtered_boxes.max() <= 1.0:
                    boxes_coco[:, 0] = (filtered_boxes[:, 0] - filtered_boxes[:, 2] / 2) * img_w
                    boxes_coco[:, 1] = (filtered_boxes[:, 1] - filtered_boxes[:, 3] / 2) * img_h
                    boxes_coco[:, 2] = filtered_boxes[:, 2] * img_w
                    boxes_coco[:, 3] = filtered_boxes[:, 3] * img_h
                else:
                    boxes_coco = filtered_boxes.clone()
                
                boxes_coco[:, 0] = torch.clamp(boxes_coco[:, 0], 0, img_w)
                boxes_coco[:, 1] = torch.clamp(boxes_coco[:, 1], 0, img_h)
                boxes_coco[:, 2] = torch.clamp(boxes_coco[:, 2], 1, img_w)
                boxes_coco[:, 3] = torch.clamp(boxes_coco[:, 3], 1, img_h)
                
                for j in range(boxes_coco.shape[0]):
                    x, y, w, h = boxes_coco[j].cpu().numpy()
                    all_predictions.append({
                        'image_id': img_id,
                        'category_id': int(filtered_classes[j].item()) + 1,
                        'bbox': [float(x), float(y), float(w), float(h)],
                        'score': float(filtered_scores[j].item())
                    })
        
        # å¤„ç†çœŸå®æ ‡ç­¾
        if i < len(targets) and 'labels' in targets[i] and 'boxes' in targets[i]:
            true_labels = targets[i]['labels']
            true_boxes = targets[i]['boxes']
            
            if len(true_labels) > 0:
                max_val = float(true_boxes.max().item()) if true_boxes.numel() > 0 else 0.0
                true_boxes_coco = torch.zeros_like(true_boxes)
                
                if max_val <= 1.0 + 1e-6:
                    true_boxes_coco[:, 0] = (true_boxes[:, 0] - true_boxes[:, 2] / 2) * img_w
                    true_boxes_coco[:, 1] = (true_boxes[:, 1] - true_boxes[:, 3] / 2) * img_h
                    true_boxes_coco[:, 2] = true_boxes[:, 2] * img_w
                    true_boxes_coco[:, 3] = true_boxes[:, 3] * img_h
                else:
                    true_boxes_coco = true_boxes.clone()
                
                true_boxes_coco[:, 0] = torch.clamp(true_boxes_coco[:, 0], 0, img_w)
                true_boxes_coco[:, 1] = torch.clamp(true_boxes_coco[:, 1], 0, img_h)
                true_boxes_coco[:, 2] = torch.clamp(true_boxes_coco[:, 2], 1, img_w)
                true_boxes_coco[:, 3] = torch.clamp(true_boxes_coco[:, 3], 1, img_h)
                
                has_iscrowd = 'iscrowd' in targets[i]
                iscrowd_values = targets[i].get('iscrowd', torch.zeros(len(true_labels), dtype=torch.int64))
                
                for j in range(len(true_labels)):
                    x, y, w, h = true_boxes_coco[j].cpu().numpy()
                    ann_dict = {
                        'id': len(all_targets),
                        'image_id': img_id,
                        'category_id': int(true_labels[j].item()) + 1,
                        'bbox': [float(x), float(y), float(w), float(h)],
                        'area': float(w * h)
                    }
                    if has_iscrowd:
                        ann_dict['iscrowd'] = int(iscrowd_values[j].item())
                    all_targets.append(ann_dict)


def evaluate_accuracy(model, config_path: str, device: str = "cuda", 
                      model_type: str = "dset", max_samples: int = 300) -> Dict[str, float]:
    """ä½¿ç”¨ pycocotools åœ¨éªŒè¯é›†ä¸Šè¿è¡Œ COCO è¯„ä¼°ï¼ˆä»…ç²¾åº¦ï¼Œæ— æ€§èƒ½æµ‹è¯•ï¼‰"""
    try:
        # å¯¼å…¥ Trainer
        if model_type == "dset":
            try:
                from experiments.dset.train import DSETTrainer
            except ImportError:
                dset_dir = Path(config_path).parent.parent
                if str(dset_dir) not in sys.path:
                    sys.path.insert(0, str(dset_dir))
                from train import DSETTrainer
            TrainerClass = DSETTrainer
        elif model_type == "rtdetr":
            rtdetr_dir = Path(config_path).parent.parent
            if str(rtdetr_dir) not in sys.path:
                sys.path.insert(0, str(rtdetr_dir))
            from train import RTDETRTrainer
            TrainerClass = RTDETRTrainer
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        # åŠ è½½é…ç½®ï¼ˆä¸å¼ºåˆ¶ batch_sizeï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ä»¥åŠ é€Ÿè¯„ä¼°ï¼‰
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        if 'misc' not in config:
            config['misc'] = {}
        config['misc']['device'] = device
        
        # åˆ›å»º DataLoaderï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ batch_sizeï¼‰
        if model_type == "dset":
            trainer = TrainerClass(config, config_file_path=str(config_path))
            _, val_loader = trainer._create_data_loaders()
        else:
            trainer = TrainerClass(config)
            trainer.model = model
            trainer.criterion = trainer.create_criterion()
            _, val_loader = trainer.create_datasets()
        
        dataset_size = len(val_loader.dataset)
        dataloader_length = len(val_loader)
        actual_batch_size = val_loader.batch_size if hasattr(val_loader, 'batch_size') else None
        
        print(f"  âœ“ DataLoader: {dataloader_length} batches, {dataset_size} samples (batch_size={actual_batch_size})")
        
        model.eval()
        model = model.to(device)
        
        # éªŒè¯ token pruning çŠ¶æ€
        if hasattr(model, 'encoder') and hasattr(model.encoder, 'token_pruners'):
            if model.encoder.token_pruners:
                pruner = model.encoder.token_pruners[0]
                if hasattr(pruner, 'pruning_enabled'):
                    print(f"  âœ“ Token Pruning: {'å·²æ¿€æ´»' if pruner.pruning_enabled else 'æœªæ¿€æ´»'}")
        
        all_predictions = []
        all_targets = []
        
        print(f"  è¿è¡Œè¯„ä¼°å¾ªç¯ï¼ˆä»…ç²¾åº¦è¯„ä¼°ï¼Œæ— æ€§èƒ½æµ‹è¯•ï¼‰...")
        
        with torch.no_grad():
            for batch_idx, (images, targets) in enumerate(val_loader):
                B, C, H_tensor, W_tensor = images.shape
                images = images.to(device)
                targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v 
                           for k, v in t.items()} for t in targets]
                
                # å•æ¬¡å‰å‘ä¼ æ’­è·å–è¾“å‡º
                outputs = model(images)
                
                has_predictions = isinstance(outputs, dict) and (
                    ('class_scores' in outputs and 'bboxes' in outputs) or
                    ('pred_logits' in outputs and 'pred_boxes' in outputs)
                )
                
                if has_predictions:
                    # åŠ¨æ€è·å–å½“å‰ batch çš„çœŸå®å›¾ç‰‡æ•°é‡ B
                    current_batch_actual_size = images.shape[0]
                    
                    _collect_predictions_for_coco(
                        outputs, targets, batch_idx, all_predictions, all_targets,
                        W_tensor, H_tensor, current_batch_actual_size  # ä¿®å¤ï¼šä¼ å…¥çœŸå®çš„ B
                    )
                
                # è¿›åº¦æ‰“å°ï¼šæ¯ 100 ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡
                if (batch_idx + 1) % 100 == 0:
                    print(f"    è¿›åº¦: {batch_idx + 1}/{dataloader_length}")
        
        print(f"  âœ“ å®Œæˆ: {len(val_loader)} æ ·æœ¬, {len(all_predictions)} é¢„æµ‹æ¡†")
        
        # COCO è¯„ä¼°ï¼ˆä»…ç²¾åº¦æŒ‡æ ‡ï¼‰
        metrics = _compute_coco_metrics(all_predictions, all_targets, H_tensor, W_tensor)
        
        return metrics
        
    except Exception as e:
        print(f"  âš  COCO è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}


def _compute_coco_metrics(predictions: List[Dict], targets: List[Dict],
                          img_h: int = 736, img_w: int = 1280) -> Dict[str, float]:
    """ä½¿ç”¨ pycocotools è®¡ç®— COCO æŒ‡æ ‡"""
    try:
        if len(predictions) == 0:
            return {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}
        
        categories = [
            {'id': 1, 'name': 'Car'}, {'id': 2, 'name': 'Truck'},
            {'id': 3, 'name': 'Van'}, {'id': 4, 'name': 'Bus'},
            {'id': 5, 'name': 'Pedestrian'}, {'id': 6, 'name': 'Cyclist'},
            {'id': 7, 'name': 'Motorcyclist'}, {'id': 8, 'name': 'Trafficcone'}
        ]
        
        image_ids = set(t['image_id'] for t in targets)
        coco_gt = {
            'images': [{'id': img_id, 'width': img_w, 'height': img_h} for img_id in image_ids],
            'annotations': targets,
            'categories': categories,
            'info': {'description': 'DAIR-V2X Dataset', 'version': '1.0', 'year': 2024}
        }
        
        coco_gt_obj = COCO()
        coco_gt_obj.dataset = coco_gt
        
        old_stdout = sys.stdout
        sys.stdout = StringIO()
        try:
            coco_gt_obj.createIndex()
            coco_dt = coco_gt_obj.loadRes(predictions)
        finally:
            sys.stdout = old_stdout
        
        coco_eval = COCOeval(coco_gt_obj, coco_dt, 'bbox')
        
        sys.stdout = StringIO()
        try:
            coco_eval.evaluate()
            coco_eval.accumulate()
            coco_eval.summarize()
        finally:
            sys.stdout = old_stdout
        
        stats = coco_eval.stats
        metrics = {
            'mAP': float(stats[0]) if len(stats) > 0 else 0.0,
            'AP50': float(stats[1]) if len(stats) > 1 else 0.0,
            'APS': float(stats[3]) if len(stats) > 3 else 0.0
        }
        
        print(f"  âœ“ mAP: {metrics['mAP']:.3f}, AP50: {metrics['AP50']:.3f}, APS: {metrics['APS']:.3f}")
        return metrics
        
    except Exception as e:
        print(f"  âš  COCO æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        return {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}


def _resolve_path(path_str: str, project_root: Path) -> Path:
    """è§£æè·¯å¾„"""
    if not path_str:
        return None
    path = Path(path_str)
    return path if path.is_absolute() else project_root / path


def _get_yolo_data_path(model, model_config: Dict, project_root: Path) -> Optional[Path]:
    """è·å– YOLO æ¨¡å‹çš„æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„"""
    data_config = model_config.get('data', None)
    if data_config:
        data_path = _resolve_path(data_config, project_root)
        if data_path and data_path.exists():
            return data_path
    
    try:
        if hasattr(model, 'ckpt') and model.ckpt and hasattr(model.ckpt, 'data'):
            data_path = _resolve_path(model.ckpt.data, project_root)
            if data_path and data_path.exists():
                return data_path
    except:
        pass
    
    return None


def evaluate_single_model(model_name: str, model_config: Dict, args, project_root: Path, debug: bool = False) -> Optional[Dict]:
    """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
    print("\n" + "=" * 80)
    print(f"è¯„ä¼°æ¨¡å‹: {model_name}")
    print("=" * 80)
    
    model_type = model_config.get('type', args.model_type)
    config_path_str = model_config.get('config', args.config)
    checkpoint_path_str = model_config.get('checkpoint', None)
    input_size_override = model_config.get('input_size', None)
    
    # ç¡®å®š input_size
    if input_size_override is not None:
        input_size = input_size_override
    elif "yolo" in model_type.lower():
        input_size = [1280, 1280]
    else:
        input_size = [736, 1280]
    
    config_path = _resolve_path(config_path_str, project_root) if config_path_str else None
    
    # å¤„ç†æ£€æŸ¥ç‚¹è·¯å¾„
    if checkpoint_path_str:
        checkpoint_path = _resolve_path(checkpoint_path_str, project_root)
        if not checkpoint_path.exists():
            print(f"âš  æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            return None
    else:
        logs_dir = _resolve_path(args.logs_dir, project_root)
        checkpoint_path = find_latest_best_model(logs_dir, model_type)
        if checkpoint_path is None:
            print(f"âš  æ— æ³•æ‰¾åˆ°æ£€æŸ¥ç‚¹")
            return None
    
    print(f"  ç±»å‹: {model_type}, è¾“å…¥: {input_size[0]}x{input_size[1]}")
    print(f"  æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # åŠ è½½æ¨¡å‹å’Œé…ç½®
    is_yolo_model = model_type.startswith("yolov8") or model_type.startswith("yolov10")
    config = None
    try:
        if model_type == "dset":
            model, config = load_dset_model(str(config_path), str(checkpoint_path), args.device)
        elif model_type == "rtdetr":
            model, config = load_rtdetr_model(str(config_path), str(checkpoint_path), args.device)
        elif model_type == "deformable-detr":
            model = load_deformable_detr_model(str(checkpoint_path), args.device, 
                                               config_path=str(config_path) if config_path else None)
            # åŠ è½½é…ç½®
            if config_path:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
        elif model_type.startswith("yolov8"):
            model, config = load_yolov8_model(str(checkpoint_path), args.device)
        elif model_type.startswith("yolov10"):
            model, config = load_yolov10_model(str(checkpoint_path), args.device)
        else:
            print(f"  âš  ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
            return None
        print("  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"  âš  æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    # è®¡ç®—å‚æ•°é‡å’Œç†è®º FLOPs
    input_size_tuple = (1, 3, input_size[0], input_size[1])
    total_params_m, active_params_m, base_flops_g, theory_flops_g = get_model_info(
        model, input_size_tuple, is_yolo=is_yolo_model, config=config, model_type=model_type, debug=debug
    )
    print(f"  âœ“ Total Params: {total_params_m:.2f}M, Active Params: {active_params_m:.2f}M")
    print(f"  âœ“ Base FLOPs: {base_flops_g:.2f}G, Theory FLOPs: {theory_flops_g:.2f}G")
    
    # è¯„ä¼°ï¼ˆä»…ç²¾åº¦ï¼‰
    metrics = {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}
    
    if model_type in ["dset", "rtdetr"] and config_path:
        metrics = evaluate_accuracy(model, str(config_path), args.device, 
                                    model_type=model_type, max_samples=999999)
    elif model_type == "deformable-detr" and config_path:
        metrics = evaluate_deformable_detr_full(
            config_path=str(config_path),
            checkpoint_path=str(checkpoint_path),
            device=args.device,
        )
    elif is_yolo_model:
        data_config_path = _get_yolo_data_path(model, model_config, project_root)
        if data_config_path:
            metrics = evaluate_yolo_accuracy(model, str(data_config_path), args.device, 999999)
        else:
            print(f"  âš  YOLO æ¨¡å‹éœ€è¦æ•°æ®é›†é…ç½®æ–‡ä»¶")
    
    # æ¸…ç†æ˜¾å­˜
    del model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return {
        'model_name': model_name,
        'model_type': model_type,
        'total_params_m': total_params_m,
        'active_params_m': active_params_m,
        'base_flops_g': base_flops_g,
        'theory_flops_g': theory_flops_g,
        'mAP': metrics.get('mAP', 0.0),
        'AP50': metrics.get('AP50', 0.0),
        'APS': metrics.get('APS', 0.0),
        'input_size': f"{input_size[0]}x{input_size[1]}"
    }


def print_summary_table(results: List[Dict], gpu_name: str = "GPU", save_csv: bool = True, max_samples: int = 300):
    """æ‰“å°ç»“æœæ±‡æ€»è¡¨æ ¼å¹¶ä¿å­˜ä¸º CSVï¼ˆç†è®ºæ•ˆç‡è§†è§’ï¼‰"""
    if not results:
        print("\nâš  æ²¡æœ‰è¯„ä¼°ç»“æœ")
        return
    
    print("\n" + "=" * 140)
    print("THEORETICAL EFFICIENCY".center(140))
    print("=" * 140)
    
    header = f"{'Model':<25} {'Total':<10} {'Active':<10} {'Theory':<10} {'Resolution':<12} {'mAP':<8} {'AP50':<8} {'APS':<8}"
    print(header)
    print("-" * 140)
    print(f"{'':<25} {'Params':<10} {'Params':<10} {'GFLOPs':<10} {'':<12} {'':<8} {'':<8} {'':<8}")
    print(f"{'':<25} {'(M)':<10} {'(M)':<10} {'':<10} {'':<12} {'':<8} {'':<8} {'':<8}")
    print("-" * 140)
    
    csv_rows = [['Model', 'Type', 'Total Params(M)', 'Active Params(M)', 'Theory GFLOPs', 
                 'Resolution', 'mAP', 'AP50', 'APS', 'Input']]
    
    for r in results:
        name = r.get('model_name', 'Unknown')[:24]
        total_params = f"{r.get('total_params_m', 0):.2f}" if r.get('total_params_m', 0) > 0 else "N/A"
        active_params = f"{r.get('active_params_m', 0):.2f}" if r.get('active_params_m', 0) > 0 else "N/A"
        theory_flops = f"{r.get('theory_flops_g', 0):.2f}" if r.get('theory_flops_g', 0) > 0 else "N/A"
        resolution = r.get('input_size', 'N/A')
        
        mAP = f"{r.get('mAP', 0):.3f}" if r.get('mAP', 0) > 0 else "N/A"
        ap50 = f"{r.get('AP50', 0):.3f}" if r.get('AP50', 0) > 0 else "N/A"
        aps = f"{r.get('APS', 0):.3f}" if r.get('APS', 0) > 0 else "N/A"
        
        print(f"{name:<25} {total_params:<10} {active_params:<10} {theory_flops:<10} {resolution:<12} {mAP:<8} {ap50:<8} {aps:<8}")
        
        csv_rows.append([name, r.get('model_type', ''), total_params, active_params, theory_flops, 
                        resolution, mAP, ap50, aps, r.get('input_size', '')])
    
    print("-" * 140)
    print("Note: Theoretical FLOPs are calculated based on sparsity-aware projection (MoE activation ratio: top_k/expert_num, and token pruning ratio).")
    print("=" * 140)
    
    if save_csv:
        import csv
        csv_path = Path(project_root) / "benchmark_results.csv"
        try:
            with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                csv.writer(f).writerows(csv_rows)
            print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
        except Exception as e:
            print(f"\nâš  ä¿å­˜ CSV å¤±è´¥: {e}")


def find_latest_best_model(logs_dir: Path, model_type: str = "dset") -> Optional[Path]:
    """åœ¨ logs ç›®å½•ä¸‹æ‰¾åˆ°æœ€æ–°çš„ best_model.pth æˆ– best.pt
    
    ä¼˜å…ˆæŸ¥æ‰¾å„å®éªŒç›®å½•ä¸‹ weights/ æ–‡ä»¶å¤¹å†…çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå¹¶æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºè¿”å›æœ€æ–°è€…ã€‚
    """
    best_models = []
    
    if model_type == "deformable-detr":
        # Deformable-DETR: æŸ¥æ‰¾ best_*.pth
        # ä¼˜å…ˆæŸ¥æ‰¾ weights/ ç›®å½•
        best_models.extend(list(logs_dir.rglob("weights/best_*.pth")))
        best_models.extend(list(logs_dir.rglob("best_*.pth")))
    elif model_type.startswith("yolov"):
        # YOLO: æŸ¥æ‰¾ best.pt æˆ– best_model.pth
        # ä¼˜å…ˆæŸ¥æ‰¾ weights/ ç›®å½•
        best_models.extend(list(logs_dir.rglob("weights/best.pt")))
        best_models.extend(list(logs_dir.rglob("weights/best_model.pth")))
        best_models.extend(list(logs_dir.rglob("best.pt")))
        best_models.extend(list(logs_dir.rglob("best_model.pth")))
    else:
        # DSET/RT-DETR: æŸ¥æ‰¾ best_model.pth
        # ä¼˜å…ˆæŸ¥æ‰¾ weights/ ç›®å½•
        best_models.extend(list(logs_dir.rglob("weights/best_model.pth")))
        best_models.extend(list(logs_dir.rglob("best_model.pth")))
    
    # å»é‡å¹¶è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
    best_models = list(set(best_models))
    best_models = [p for p in best_models if p.exists() and p.is_file()]
    
    if not best_models:
        return None
    
    # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
    best_models.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return best_models[0]


def _format_evaluation_results(model_type: str, total_params_m: float, active_params_m: float, 
                               base_flops_g: float, theory_flops_g: float,
                               metrics: Dict[str, float],
                               input_resolution: Tuple[int, int], is_yolo: bool = False,
                               gpu_name: str = "GPU") -> None:
    """æ ¼å¼åŒ–å¹¶è¾“å‡ºè¯„ä¼°ç»“æœï¼ˆç†è®ºæ•ˆç‡è§†è§’ï¼‰"""
    model_names = {
        'dset': 'DSET', 'rtdetr': 'RT-DETRv2', 'deformable-detr': 'Deformable-DETR',
        'yolov8s': 'YOLOv8-s', 'yolov8m': 'YOLOv8-m',
        'yolov10s': 'YOLOv10-s', 'yolov10m': 'YOLOv10-m'
    }
    name = model_names.get(model_type, model_type.upper())
    
    if is_yolo:
        res = f"{max(input_resolution)}x{max(input_resolution)}"
    else:
        res = f"{input_resolution[0]}x{input_resolution[1]}"
    
    print("\n" + "=" * 70)
    print(f"Model: {name} | Input: {res}")
    print("-" * 70)
    print(f"Total Params: {total_params_m:.2f}M | Active Params: {active_params_m:.2f}M")
    print(f"Base FLOPs: {base_flops_g:.2f}G | Theory FLOPs: {theory_flops_g:.2f}G")
    print(f"mAP: {metrics['mAP']:.3f} | AP50: {metrics['AP50']:.3f} | APS: {metrics['APS']:.3f}")
    print("=" * 70)
    print("Note: Theoretical FLOPs are calculated based on sparsity-aware projection (MoE activation ratio: top_k/expert_num, and token pruning ratio).")


def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨')
    parser.add_argument('--logs_dir', type=str, default='experiments/dset/logs')
    parser.add_argument('--config', type=str, default='experiments/dset/logs/dset6_r18_20260126_173526/config.yaml')
    parser.add_argument('--checkpoint', type=str, default=None)
    parser.add_argument('--input_size', type=int, nargs=2, default=None)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--model_type', type=str, default='dset',
                       choices=['dset', 'rtdetr', 'deformable-detr', 
                               'yolov8s', 'yolov8m', 'yolov10s', 'yolov10m'])
    parser.add_argument('--rtdetr_config', type=str, default=None)
    parser.add_argument('--deformable_work_dir', type=str, default=None)
    parser.add_argument('--deformable_config', type=str, default=None)
    parser.add_argument('--models_config', type=str, default=None)
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°æ‰€æœ‰æ¨¡å—å±‚åä»¥ä¾¿è°ƒè¯• MoE å±‚è¯†åˆ«')
    
    args = parser.parse_args()
    
    # GPU/CPU åç§°
    if args.device == 'cuda' and torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
    else:
        import platform
        gpu_name = f"CPU ({platform.processor() or 'Unknown'})"
    
    print("=" * 80)
    print("æ€§èƒ½å¯¹æ¯”è¡¨ç”Ÿæˆè„šæœ¬")
    print("=" * 80)
    
    # æ„é€ é…ç½®
    if args.models_config:
        json_config_path = _resolve_path(args.models_config, project_root)
        if not json_config_path.exists():
            print(f"é”™è¯¯: JSON é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {json_config_path}")
            return
        with open(json_config_path, 'r', encoding='utf-8') as f:
            json_config = json.load(f)
        print(f"âœ“ æ‰¹é‡è¯„ä¼°: {len(json_config)} ä¸ªæ¨¡å‹\n")
    else:
        if "yolo" in args.model_type.lower():
            input_size = [1280, 1280]
        else:
            input_size = [736, 1280]
        input_size = args.input_size or input_size
        
        single_config = {
            'type': args.model_type,
            'config': args.rtdetr_config or args.deformable_config or args.config,
            'checkpoint': args.checkpoint,
            'input_size': input_size
        }
        json_config = {'single_model': single_config}
        print(f"âœ“ å•æ¨¡å‹è¯„ä¼°\n")
    
    # è¯„ä¼°
    all_results = []
    for model_name, model_config in json_config.items():
        if not isinstance(model_config, dict):
            continue
        result = evaluate_single_model(model_name, model_config, args, project_root, debug=args.debug)
        if result:
            all_results.append(result)
    
    # è¾“å‡ºç»“æœ
    if len(all_results) > 1:
        print_summary_table(all_results, gpu_name, save_csv=True, max_samples=0)  # max_samples ä¸å†ä½¿ç”¨ï¼Œè®¾ä¸º 0
    elif all_results:
        r = all_results[0]
        _format_evaluation_results(
            r['model_type'], 
            r.get('total_params_m', 0), r.get('active_params_m', 0),
            r.get('base_flops_g', 0), r.get('theory_flops_g', 0),
            {'mAP': r['mAP'], 'AP50': r['AP50'], 'APS': r['APS']},
            (int(r['input_size'].split('x')[1]), int(r['input_size'].split('x')[0])),
            r['model_type'].startswith("yolov"), gpu_name
        )


if __name__ == '__main__':
    main()
