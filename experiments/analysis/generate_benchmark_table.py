#!/usr/bin/env python3
"""
æ¨¡å‹ç†è®ºæ•ˆç‡è¯„ä¼°è„šæœ¬ - æ”¯æŒ DSET, RT-DETR, Deformable-DETR, YOLOv8, YOLOv10

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨ä» logs/ ç›®å½•æŸ¥æ‰¾æœ€æ–°çš„ best_model.pth æˆ–ä½¿ç”¨æŒ‡å®šçš„æ£€æŸ¥ç‚¹
2. ä½¿ç”¨ pycocotools åœ¨éªŒè¯é›†ä¸Šè¿è¡Œ COCO è¯„ä¼°ï¼ˆä»…ç²¾åº¦æŒ‡æ ‡ï¼‰
3. è®¡ç®—æ¨¡å‹å‚æ•°é‡å’Œç†è®º FLOPsï¼ˆè€ƒè™‘ token pruning å’Œ MoE ç¨€ç–æ€§ï¼‰
4. ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ batch_sizeï¼ˆCOCO è¯„ä¼°ç»“æœä¸ batch_size æ— å…³ï¼Œå¯ä½¿ç”¨æ›´å¤§ batch_size åŠ é€Ÿï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python generate_benchmark_table.py --model_type dset
    python generate_benchmark_table.py --models_config models.json
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import time
import json
from pathlib import Path
from typing import Dict, Optional, Tuple, List
from io import StringIO
from pycocotools.cocoeval import COCOeval
from pycocotools.coco import COCO

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
_project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)
project_root = _project_root

# å°è¯•å¯¼å…¥ thop
try:
    from thop import profile
    HAS_THOP = True
except ImportError:
    HAS_THOP = False
    print("è­¦å‘Š: thop æœªå®‰è£…ï¼Œå°†æ— æ³•è®¡ç®— FLOPsã€‚è¯·è¿è¡Œ: pip install thop")


def _cuda_sync_if_available(device: str):
    """åŒæ­¥ CUDAï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    if device == "cuda" and torch.cuda.is_available():
        torch.cuda.synchronize()


def _load_checkpoint(checkpoint_path: str) -> dict:
    """åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    try:
        return torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    except TypeError:
        return torch.load(checkpoint_path, map_location='cpu')


def _extract_state_dict(checkpoint: dict) -> dict:
    """ä»æ£€æŸ¥ç‚¹ä¸­æå–çŠ¶æ€å­—å…¸"""
    if 'ema_state_dict' in checkpoint:
        state_dict = checkpoint['ema_state_dict']
        if isinstance(state_dict, dict) and 'module' in state_dict:
            state_dict = state_dict['module']
        return state_dict
    elif 'ema' in checkpoint and 'module' in checkpoint['ema']:
        return checkpoint['ema']['module']
    elif 'model_state_dict' in checkpoint:
        return checkpoint['model_state_dict']
    elif 'model' in checkpoint:
        return checkpoint['model']
    return checkpoint


def get_model_info(model, input_size: Tuple[int, int, int, int] = (1, 3, 736, 1280), 
                   is_yolo: bool = False, config: Dict = None, model_type: str = "dset",
                   debug: bool = False) -> Tuple[float, float, float, float]:
    """
    è®¡ç®—æ¨¡å‹çš„å‚æ•°é‡å’Œç†è®º FLOPs
    
    Returns:
        total_params_m: æ€»å‚æ•°é‡ (M)
        active_params_m: æ¿€æ´»å‚æ•°é‡ (M) - è€ƒè™‘ MoE åçš„å®é™…å‚æ•°
        base_flops_g: åŸºå‡† FLOPs (G) - å…¨é‡è¿è¡Œæ—¶çš„è®¡ç®—é‡
        theory_flops_g: ç†è®º FLOPs (G) - è€ƒè™‘ token pruning å’Œ MoE åçš„ç†è®ºè®¡ç®—é‡
    """
    # è®¡ç®—å‚æ•°é‡
    if is_yolo and hasattr(model, 'model'):
        pytorch_model = model.model
    else:
        pytorch_model = model
    total_params = sum(p.numel() for p in pytorch_model.parameters())
    total_params_m = total_params / 1e6
    
    # è®¡ç®—æ¿€æ´»å‚æ•°é‡ï¼ˆè€ƒè™‘ MoEï¼‰
    active_params = total_params
    if model_type == "dset" and config is not None:
        # ä»é…ç½®è·å– num_experts å’Œ top_k
        dset_config = config.get('model', {}).get('dset', {})
        encoder_experts = dset_config.get('encoder_moe_num_experts', 1)
        encoder_top_k = dset_config.get('encoder_moe_top_k', 1)
        decoder_experts = config.get('model', {}).get('num_experts', 1)
        decoder_top_k = config.get('model', {}).get('top_k', 3)
        
        # åˆ†åˆ«ç»Ÿè®¡ Encoder å’Œ Decoder çš„ä¸“å®¶å‚æ•°
        encoder_expert_params = 0
        decoder_expert_params = 0
        
        # éå†æ‰€æœ‰å‚æ•°ï¼Œè¯†åˆ«ä¸“å®¶å‚æ•°
        for name, param in pytorch_model.named_parameters():
            # Encoder ä¸“å®¶å‚æ•°ï¼šå‚æ•°ååŒ…å« 'encoder' ä¸”å« 'expert' æˆ– 'encoder_moe'
            if 'encoder' in name.lower() and ('expert' in name.lower() or 'encoder_moe' in name.lower()):
                encoder_expert_params += param.numel()
            # Decoder ä¸“å®¶å‚æ•°ï¼šå‚æ•°ååŒ…å« 'decoder' ä¸”å« 'moe_layer'
            elif 'decoder' in name.lower() and 'moe_layer' in name.lower():
                decoder_expert_params += param.numel()
        
        # è®¡ç®—æ¿€æ´»å‚æ•°
        # Encoder: Top-K è·¯ç”±ï¼Œæ¿€æ´»å‚æ•° = Expert_Params Ã— min(top_k, experts) / Num_Experts
        # Decoder: Top-K è·¯ç”±ï¼Œæ¿€æ´»å‚æ•° = Expert_Params Ã— min(top_k, experts) / Num_Experts
        encoder_active_ratio = min(encoder_top_k, encoder_experts) / max(encoder_experts, 1) if encoder_experts > 1 else 1.0
        decoder_active_ratio = min(decoder_top_k, decoder_experts) / max(decoder_experts, 1) if decoder_experts > 1 else 1.0
        encoder_active = encoder_expert_params * encoder_active_ratio
        decoder_active = decoder_expert_params * decoder_active_ratio
        
        # æ€»æ¿€æ´»å‚æ•° = æ€»å‚æ•° - ä¸“å®¶æ€»å‚æ•° + Encoderæ¿€æ´»éƒ¨åˆ† + Decoderæ¿€æ´»éƒ¨åˆ†
        total_expert_params = encoder_expert_params + decoder_expert_params
        active_params = (total_params - total_expert_params) + encoder_active + decoder_active
    
    active_params_m = active_params / 1e6
    
    # è®¡ç®—åŸºå‡† FLOPsï¼ˆå…¨é‡è¿è¡Œï¼‰
    base_flops_g = 0.0
    if is_yolo:
        try:
            from copy import deepcopy
            if hasattr(model, 'model'):
                pytorch_model = model.model
                h, w = input_size[2], input_size[3]
                imgsz = [h, w] if h != w else h
                try:
                    from ultralytics.utils.torch_utils import get_flops
                    base_flops_g = get_flops(pytorch_model, imgsz=imgsz)
                except (ImportError, AttributeError):
                    if HAS_THOP:
                        pytorch_model = pytorch_model.eval()
                        device = next(pytorch_model.parameters()).device
                        dummy_input = torch.randn(input_size).to(device)
                        flops, _ = profile(deepcopy(pytorch_model), inputs=(dummy_input,), verbose=False)
                        base_flops_g = flops / 1e9
        except Exception as e:
            print(f"  âš  YOLO FLOPs è®¡ç®—å¤±è´¥: {e}")
    elif HAS_THOP:
        try:
            model.eval()
            device = next(model.parameters()).device
            dummy_input = torch.randn(input_size).to(device)
            flops, _ = profile(model, inputs=(dummy_input,), verbose=False)
            base_flops_g = flops / 1e9
            print(f"  âœ“ Base FLOPs: {base_flops_g:.2f} G")
        except Exception as e:
            # mmdet æ¨¡å‹é€šå¸¸æ— æ³•ç›´æ¥ profile(model, (tensor,))ï¼Œéœ€è¦æ„é€  wrapper
            try:
                is_mmdet_model = isinstance(getattr(model, '__class__', None), type) and \
                                ('mmdet' in getattr(model.__class__, '__module__', ''))
            except Exception:
                is_mmdet_model = False
            
            if is_mmdet_model:
                try:
                    # ä¼˜å…ˆä½¿ç”¨ mmengine çš„å¤æ‚åº¦åˆ†æå·¥å…·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    from mmengine.analysis import get_model_complexity_info
                    _, _, H, W = input_size
                    # get_model_complexity_info çš„ input_shape ä¸€èˆ¬æ˜¯ (C, H, W)
                    analysis = get_model_complexity_info(
                        model, (3, int(H), int(W)),
                        as_strings=False,
                        print_per_layer_stat=False
                    )
                    # ä¸åŒç‰ˆæœ¬è¿”å›å€¼å¯èƒ½æ˜¯ (flops, params) æˆ– dict
                    if isinstance(analysis, tuple) and len(analysis) >= 1:
                        flops = analysis[0]
                        base_flops_g = float(flops) / 1e9
                        print(f"  âœ“ Base FLOPs(mmengine): {base_flops_g:.2f} G")
                    elif isinstance(analysis, dict) and 'flops' in analysis:
                        base_flops_g = float(analysis['flops']) / 1e9
                        print(f"  âœ“ Base FLOPs(mmengine): {base_flops_g:.2f} G")
                except Exception:
                    try:
                        # å›é€€ï¼šç”¨ thop å¯¹ mmdet detector åš wrapper profileï¼ˆæ„é€ æœ€å° data_samplesï¼‰
                        from copy import deepcopy
                        from mmdet.structures import DetDataSample
                        _, _, H, W = input_size
                        sample = DetDataSample()
                        sample.set_metainfo({
                            'ori_shape': (int(H), int(W), 3),
                            'img_shape': (int(H), int(W), 3),
                            'pad_shape': (int(H), int(W), 3),
                            'scale_factor': (1.0, 1.0),
                            'batch_input_shape': (int(H), int(W)),
                        })
                        
                        class _MMDetWrapper(nn.Module):
                            def __init__(self, det_model, data_sample):
                                super().__init__()
                                self.det_model = det_model
                                self.data_sample = data_sample
                            def forward(self, x):
                                # mode='tensor' é€šå¸¸è¿”å› head çš„åŸå§‹ tensor è¾“å‡ºï¼ˆä¾¿äº profileï¼‰
                                return self.det_model(x, [self.data_sample], mode='tensor')
                        
                        wrapped = _MMDetWrapper(deepcopy(model).eval().to(device), sample)
                        flops, _ = profile(wrapped, inputs=(dummy_input,), verbose=False)
                        base_flops_g = flops / 1e9
                        print(f"  âœ“ Base FLOPs(thop+mmdet wrapper): {base_flops_g:.2f} G")
                    except Exception as e2:
                        print(f"  âš  FLOPs è®¡ç®—å¤±è´¥(mmdet): {e2!r}")
            else:
                print(f"  âš  FLOPs è®¡ç®—å¤±è´¥: {e!r}")
    
    # è®¡ç®—ç†è®º FLOPsï¼ˆè€ƒè™‘ token pruning å’Œ MoEï¼‰
    theory_flops_g = base_flops_g
    if model_type == "dset" and config is not None and HAS_THOP:
        dset_config = config.get('model', {}).get('dset', {})
        token_keep_ratio = dset_config.get('token_keep_ratio', 1.0)
        encoder_experts = dset_config.get('encoder_moe_num_experts', 1)
        encoder_top_k = dset_config.get('encoder_moe_top_k', 1)
        decoder_experts = config.get('model', {}).get('num_experts', 1)
        decoder_top_k = config.get('model', {}).get('top_k', 3)
        
        # å¦‚æœ token_keep_ratio æ˜¯å­—å…¸ï¼Œå–å¹³å‡å€¼æˆ–ä¸»è¦å±‚çš„å€¼
        if isinstance(token_keep_ratio, dict):
            # å–æœ€å¤§ key å¯¹åº”çš„å€¼ï¼ˆé€šå¸¸æ˜¯ P5 å±‚ï¼Œå³ layer 2ï¼‰
            if token_keep_ratio:
                token_keep_ratio = max(token_keep_ratio.values())
            else:
                token_keep_ratio = 1.0
        
        try:
            model.eval()
            device = next(model.parameters()).device
            dummy_img = torch.randn(input_size).to(device)
            
            from copy import deepcopy
            
            # ========== 1. Backbone: å¯†é›†è®¡ç®— ==========
            if not hasattr(model, 'backbone'):
                raise AttributeError("æ¨¡å‹ç¼ºå°‘ backbone å±æ€§")
            
            backbone_model = deepcopy(model.backbone).eval()
            backbone_flops, _ = profile(backbone_model, inputs=(dummy_img,), verbose=False)
            print(f"  âœ“ Backbone FLOPs: {backbone_flops / 1e9:.2f} G")
            
            # ========== 2. è·å–ç‰¹å¾å›¾ä½œä¸º Encoder çš„è¾“å…¥ ==========
            with torch.no_grad():
                backbone_feats = model.backbone(dummy_img)
                # Encoder é€šå¸¸å¤„ç†æœ€åä¸€ä¸ªç‰¹å¾å›¾ï¼ˆS5/P5ï¼‰
                if isinstance(backbone_feats, (list, tuple)):
                    encoder_feat = backbone_feats[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ªç‰¹å¾å›¾
                else:
                    encoder_feat = backbone_feats
            
            # ========== 3. è§£æ„ Encoder (é’ˆå¯¹ AIFI æ¨¡å—) ==========
            if not hasattr(model, 'encoder'):
                raise AttributeError("æ¨¡å‹ç¼ºå°‘ encoder å±æ€§")
            
            if encoder_feat is None:
                raise ValueError("æ— æ³•è·å– encoder è¾“å…¥ç‰¹å¾å›¾")
            
            encoder_model = deepcopy(model.encoder).eval()
            
            # å‡†å¤‡ encoder è¾“å…¥
            if isinstance(backbone_feats, (list, tuple)):
                encoder_input = backbone_feats
            else:
                encoder_input = [backbone_feats]
            
            # å…ˆæµ‹é‡æ•´ä¸ª encoder_model çš„ total_enc_base
            total_enc_base, _ = profile(encoder_model, inputs=(encoder_input,), verbose=False)
            
            # 1. æ˜ç¡®å®šä¹‰éšè—ç»´åº¦ï¼ˆDSET æ ‡å‡†æ˜¯ 256ï¼‰
            hidden_dim = config.get('model', {}).get('hidden_dim', 256)
            
            enc_attn_base = 0
            enc_ffn_base = 0
            
            # è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°æ‰€æœ‰ Encoder å±‚å
            if debug:
                print("\n" + "=" * 80)
                print("ğŸ” [DEBUG] æ­£åœ¨æ‰«æ Encoder æ‰€æœ‰å­æ¨¡å—åç§°")
                print("=" * 80)
                for name, module in encoder_model.named_modules():
                    if isinstance(module, nn.Linear):
                        print(f"  Linearå±‚: {name:<60} | ç±»å: {module.__class__.__name__}")
                    elif isinstance(module, nn.MultiheadAttention) or "Attention" in module.__class__.__name__:
                        print(f"  Attentionå±‚: {name:<60} | ç±»å: {module.__class__.__name__}")
                print("=" * 80 + "\n")
            
            # éå† encoder çš„å­æ¨¡å—ï¼Œè¯†åˆ« Attention å’Œ FFN
            for name, module in encoder_model.named_modules():
                # é’ˆå¯¹ Attention å±‚
                if isinstance(module, nn.MultiheadAttention) or "Attention" in module.__class__.__name__:
                    B, _, H, W = encoder_feat.shape  # åªå– Batch, Height, Width
                    # ä¿®å¤ç‚¹ï¼šæ„é€  inputs æ—¶ï¼Œæœ€åä¸€ç»´å¿…é¡»æ˜¯ hidden_dim (256)
                    dummy_seq = torch.randn(B, H * W, hidden_dim).to(device)
                    
                    attn_flops, _ = profile(module, inputs=(dummy_seq, dummy_seq, dummy_seq), verbose=False)
                    enc_attn_base += attn_flops
                
                # é’ˆå¯¹ FFN/MoE å±‚
                elif isinstance(module, nn.Linear) and any(k in name.lower() for k in ['expert', 'ffn', 'moe']):
                    B, _, H, W = encoder_feat.shape
                    # ä¿®å¤ç‚¹ï¼šè¾“å…¥ç»´åº¦å¯¹é½ 256
                    dummy_flat = torch.randn(B, H * W, hidden_dim).to(device)
                    
                    ffn_flops, _ = profile(module, inputs=(dummy_flat,), verbose=False)
                    enc_ffn_base += ffn_flops
            
            if enc_attn_base == 0 and enc_ffn_base == 0:
                raise RuntimeError("æ— æ³•è¯†åˆ« Encoder ä¸­çš„ Attention æˆ– FFN å±‚ï¼Œè§£æ„å¤±è´¥")
            
            # è®¡ç®—å…¶ä»–éƒ¨åˆ†ï¼ˆNorm å±‚ã€Add å±‚ç­‰ï¼‰ï¼štotal_enc_base - enc_attn_base - enc_ffn_base
            enc_others_base = max(0, total_enc_base - enc_attn_base - enc_ffn_base)
            
            print(f"  âœ“ Encoder è§£æ„: Total={total_enc_base/1e9:.2f}G, Attn={enc_attn_base/1e9:.2f}G, FFN={enc_ffn_base/1e9:.2f}G, Others={enc_others_base/1e9:.2f}G")
            
            # Encoder ç†è®ºå€¼ï¼šå®Œå–„å…¬å¼ï¼Œç¡®ä¿ Norm å±‚å’Œ Add å±‚ä¹Ÿè¢«è€ƒè™‘
            # Others (Norm, Add): FLOPs âˆ Nï¼Œéš r çº¿æ€§ç¼©æ”¾
            # Attention: FLOPs âˆ NÂ²ï¼Œéš rÂ² ç¼©æ”¾
            # FFN: FLOPs âˆ Nï¼Œä¸” MoE æ¿€æ´» Top-K expertsï¼Œæ‰€ä»¥éš r Ã— min(top_k, experts) / experts ç¼©æ”¾
            encoder_moe_ratio = min(encoder_top_k, encoder_experts) / max(encoder_experts, 1)
            theory_enc_flops = (enc_others_base * token_keep_ratio) + (enc_attn_base * (token_keep_ratio ** 2)) + (enc_ffn_base * token_keep_ratio * encoder_moe_ratio)
            
            # ========== 4. Decoder: åªæœ‰ FFN éƒ¨åˆ†æ˜¯ MoE (Top-3) ==========
            if not hasattr(model, 'decoder'):
                raise AttributeError("æ¨¡å‹ç¼ºå°‘ decoder å±æ€§")
            
            decoder_model = deepcopy(model.decoder).eval()
            
            # å‡†å¤‡ decoder è¾“å…¥ï¼šå…ˆé€šè¿‡ encoder å¤„ç† backbone ç‰¹å¾ï¼Œå¾—åˆ° encoder_features
            # decoder.forward(feats, targets=None) æœŸæœ›çš„æ˜¯ç»è¿‡ encoder å¤„ç†åçš„ç‰¹å¾
            with torch.no_grad():
                encoder_features, _ = model.encoder(backbone_feats, return_encoder_info=True)
            
            # ä»é…ç½®ä¸­è¯»å– hidden_dimï¼ˆdecoder çš„éšè—ç»´åº¦ï¼Œé€šå¸¸æ˜¯ 256ï¼‰
            hidden_dim = config.get('model', {}).get('hidden_dim', 256)
            
            # ç»Ÿè®¡æ•´ä¸ª decoder çš„ FLOPs
            # decoder.forward(feats, targets=None) åªéœ€è¦ encoder_features
            dec_base_flops, _ = profile(decoder_model, inputs=(encoder_features, None), verbose=False)
            
            dec_moe_flops = 0
            processed_moe_paths = set()  # è®°å½•å·²å¤„ç†çš„ MoE æ¨¡å—è·¯å¾„ï¼Œé¿å…é‡å¤ç»Ÿè®¡
            
            # è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°æ‰€æœ‰ Decoder å±‚å
            if debug:
                print("\n" + "=" * 80)
                print("ğŸ” [DEBUG] æ­£åœ¨æ‰«æ Decoder æ‰€æœ‰å­æ¨¡å—åç§°")
                print("=" * 80)
                for name, module in decoder_model.named_modules():
                    if isinstance(module, nn.Linear):
                        print(f"  Linearå±‚: {name:<60} | ç±»å: {module.__class__.__name__}")
                    elif 'moe' in name.lower() or 'MoE' in module.__class__.__name__:
                        print(f"  â­ MoEç›¸å…³å±‚: {name:<60} | ç±»å: {module.__class__.__name__}")
                print("=" * 80 + "\n")
            
            # éå† decoder çš„å­æ¨¡å—ï¼Œè¯†åˆ« MoE å±‚
            for name, module in decoder_model.named_modules():
                # æ£€æŸ¥å½“å‰æ¨¡å—æ˜¯å¦æ˜¯å·²å¤„ç† MoE æ¨¡å—çš„å­æ¨¡å—
                # å¦‚æœå½“å‰è·¯å¾„æ˜¯å·²å¤„ç†è·¯å¾„çš„å­è·¯å¾„ï¼ˆå‰ç¼€åŒ¹é…ï¼‰ï¼Œåˆ™è·³è¿‡
                is_child_of_processed = any(
                    name.startswith(processed_path + '.') for processed_path in processed_moe_paths
                )
                
                if is_child_of_processed:
                    continue  # è·³è¿‡å·²å¤„ç† MoE æ¨¡å—çš„å­æ¨¡å—
                
                # è¯†åˆ« MoE å±‚ï¼šæ£€æŸ¥ç±»åæˆ–å‚æ•°å
                is_moe_layer = False
                if 'moe_layer' in name.lower():
                    is_moe_layer = True
                elif hasattr(module, '__class__'):
                    class_name = module.__class__.__name__
                    if 'MoE' in class_name or 'MoELayer' in class_name:
                        is_moe_layer = True
                
                if is_moe_layer:
                    # Profile MoE å±‚
                    # MoE å±‚åœ¨ decoder çš„ FFN éƒ¨åˆ†ï¼Œè¾“å…¥ç»´åº¦åº”è¯¥ä¸ decoder hidden_dim ä¸€è‡´
                    # æ„é€  dummy è¾“å…¥ï¼šMoE å±‚æœŸæœ›çš„è¾“å…¥æ˜¯ [B, num_tokens, hidden_dim]
                    B = 1
                    # ä» decoder çš„è¾“å‡ºç»´åº¦æ¨æ–­ï¼ˆé€šå¸¸æ˜¯ num_queriesï¼‰
                    num_queries = getattr(decoder_model, 'num_queries', 300)
                    dummy_moe_input = torch.randn(B, num_queries, hidden_dim).to(device)
                    moe_flops, _ = profile(module, inputs=(dummy_moe_input,), verbose=False)
                    dec_moe_flops += moe_flops
                    processed_moe_paths.add(name)  # è®°å½•å·²å¤„ç†çš„ MoE æ¨¡å—è·¯å¾„
            
            if dec_moe_flops == 0:
                raise RuntimeError("æ— æ³•è¯†åˆ« Decoder ä¸­çš„ MoE å±‚ï¼Œè§£æ„å¤±è´¥")
            
            dec_other_flops = max(0, dec_base_flops - dec_moe_flops)
            print(f"  âœ“ Decoder è§£æ„: Total={dec_base_flops/1e9:.2f}G, MoE={dec_moe_flops/1e9:.2f}G, Other={dec_other_flops/1e9:.2f}G")
            
            # Decoder ç†è®ºå€¼ï¼šMoE éƒ¨åˆ†æŒ‰ Top-K è·¯ç”±æŠ˜ç®—ï¼Œå…¶ä½™éƒ¨åˆ†ä¸å˜
            # Top-K è·¯ç”±ï¼šæ¿€æ´» min(top_k, experts) ä¸ªä¸“å®¶ï¼Œæ‰€ä»¥ FLOPs = åŸå§‹å€¼ Ã— min(top_k, experts) / experts
            # åŠ å›ºè¾¹ç•Œæ£€æŸ¥ï¼šç¡®ä¿æ¯”ä¾‹è®¡ç®—æ­£ç¡®
            dec_moe_ratio = min(decoder_top_k, decoder_experts) / max(decoder_experts, 1)
            theory_dec_flops = dec_other_flops + (dec_moe_flops * dec_moe_ratio)
            
            # ========== 5. æ±‡æ€»ï¼šæœ€ç»ˆ T-GFLOPs ==========
            total_theory_flops = backbone_flops + theory_enc_flops + theory_dec_flops
            theory_flops_g = total_theory_flops / 1e9
            
            print(f"  âœ“ Theory FLOPs (åˆ†æ¨¡å—): {theory_flops_g:.2f} G")
            print(f"    - Backbone: {backbone_flops/1e9:.2f} G")
            print(f"    - Encoder (r={token_keep_ratio:.2f}, e={encoder_experts}, top-{encoder_top_k}): {theory_enc_flops/1e9:.2f} G")
            print(f"    - Decoder (e={decoder_experts}, top-{decoder_top_k}): {theory_dec_flops/1e9:.2f} G")
        except Exception as e:
            print(f"  âš  ç†è®º FLOPs è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨åŸºå‡† FLOPs: {e}")
            theory_flops_g = base_flops_g
    else:
        # é DSET æ¨¡å‹æˆ–æ²¡æœ‰ thopï¼šç†è®º FLOPs = åŸºå‡† FLOPs
        theory_flops_g = base_flops_g
    
    return total_params_m, active_params_m, base_flops_g, theory_flops_g


def load_dset_model(config_path: str, checkpoint_path: str, device: str = "cuda"):
    """åŠ è½½ DSET æ¨¡å‹"""
    try:
        from experiments.dset.train import DSETTrainer
    except ImportError:
        dset_dir = Path(config_path).parent.parent
        if str(dset_dir) not in sys.path:
            sys.path.insert(0, str(dset_dir))
        from train import DSETTrainer
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    if 'misc' not in config:
        config['misc'] = {}
    config['misc']['device'] = device
    
    trainer = DSETTrainer(config, config_file_path=str(config_path))
    model = trainer.model
    
    checkpoint = _load_checkpoint(checkpoint_path)
    state_dict = _extract_state_dict(checkpoint)
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    
    # å¯ç”¨ token pruning - å¼ºåˆ¶è®¾ç½® epoch=100 ä»¥è·¨è¶Š warmup é˜¶æ®µ
    if hasattr(model, 'encoder') and hasattr(model.encoder, 'set_epoch'):
        dset_config = config.get('model', {}).get('dset', {})
        warmup_epochs = dset_config.get('token_pruning_warmup_epochs', 10)
        target_keep_ratio = dset_config.get('token_keep_ratio', 1.0)
        
        # å¼ºåˆ¶ epoch=100 ä»¥ç¡®ä¿å‰ªæå®Œå…¨æ¿€æ´»ï¼ˆprogress=1.0ï¼‰
        forced_epoch = 100
        model.encoder.set_epoch(forced_epoch)
        
        # éªŒè¯å‰ªæçŠ¶æ€
        if hasattr(model.encoder, 'token_pruners') and model.encoder.token_pruners:
            pruner = model.encoder.token_pruners[0]
            actual_keep_ratio = pruner.get_current_keep_ratio() if hasattr(pruner, 'get_current_keep_ratio') else None
            pruning_enabled = pruner.pruning_enabled if hasattr(pruner, 'pruning_enabled') else False
            print(f"  âœ“ Token Pruning: epoch={forced_epoch}, warmup={warmup_epochs}")
            print(f"    - pruning_enabled: {pruning_enabled}")
            print(f"    - target_keep_ratio: {target_keep_ratio}")
            print(f"    - actual_keep_ratio: {actual_keep_ratio}")
        else:
            print(f"  âœ“ å·²å¯ç”¨ token pruning (epoch={forced_epoch})")
    
    return model, config


def load_rtdetr_model(config_path: str, checkpoint_path: str, device: str = "cuda"):
    """åŠ è½½ RT-DETRv2 æ¨¡å‹"""
    rtdetr_dir = Path(config_path).parent.parent
    if str(rtdetr_dir) not in sys.path:
        sys.path.insert(0, str(rtdetr_dir))
    from train import RTDETRTrainer
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    if 'misc' not in config:
        config['misc'] = {}
    config['misc']['device'] = device
    
    trainer = RTDETRTrainer(config)
    if trainer.logger is None:
        class SimpleLogger:
            def info(self, msg): pass
        trainer.logger = SimpleLogger()
    
    model = trainer.create_model()
    checkpoint = _load_checkpoint(checkpoint_path)
    state_dict = _extract_state_dict(checkpoint)
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    model = model.to(device)
    
    return model, config


def load_deformable_detr_model(checkpoint_path: str, device: str = "cuda", config_path: str = None):
    """åŠ è½½ Deformable-DETR æ¨¡å‹"""
    try:
        from mmengine.config import Config
        from mmdet.registry import MODELS
    except ImportError:
        raise ImportError("éœ€è¦å®‰è£… mmengine å’Œ mmdet")

    # å…³é”®ï¼šç¡®ä¿ mmdet çš„æ‰€æœ‰æ¨¡å—éƒ½å·²æ³¨å†Œåˆ° mmengine Registryï¼Œå¦åˆ™ä¼šå‡ºç°
    # "DetDataPreprocessor is not in the mmengine::model registry" ä¹‹ç±»çš„é”™è¯¯ã€‚
    try:
        from mmdet.utils import register_all_modules
        try:
            # æ–°ç‰ˆ mmdet æ¨èï¼šåŒæ—¶åˆå§‹åŒ– default scope
            register_all_modules(init_default_scope=True)
        except TypeError:
            # å…¼å®¹æ—§ç‰ˆç­¾å
            register_all_modules()
    except Exception:
        # æŸäº›ç¯å¢ƒå¯èƒ½æ²¡æœ‰è¯¥å·¥å…·å‡½æ•°ï¼Œä½†æ­£å¸¸ import mmdet ä¹Ÿä¼šè§¦å‘æ³¨å†Œ
        try:
            import mmdet  # noqa: F401
        except Exception:
            pass
    
    checkpoint = _load_checkpoint(checkpoint_path)
    state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))
    
    cfg = None
    # ä¼˜å…ˆä½¿ç”¨æ˜¾å¼æä¾›çš„ config_pathï¼ˆæ›´ç¨³å®šã€å¯å¤ç°ï¼‰
    if config_path and os.path.exists(config_path):
        cfg = Config.fromfile(config_path)
    # å›é€€ï¼šå°è¯•ä» checkpoint meta ä¸­æ¢å¤é…ç½®
    elif 'meta' in checkpoint and 'cfg' in checkpoint['meta']:
        meta_cfg = checkpoint['meta']['cfg']
        # å…¼å®¹ï¼šæœ‰äº› mmdet/mmengine checkpoint ä¼šæŠŠ cfg ä¿å­˜ä¸º dictï¼›ä¹Ÿæœ‰å¾ˆå¤šä¿å­˜ä¸º strï¼ˆæ–‡ä»¶è·¯å¾„æˆ–æ–‡æœ¬å†…å®¹ï¼‰
        if isinstance(meta_cfg, dict):
            cfg = Config(meta_cfg)
        elif isinstance(meta_cfg, str):
            # 1) å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„
            if os.path.exists(meta_cfg):
                cfg = Config.fromfile(meta_cfg)
            else:
                # 2) å¯èƒ½æ˜¯ python config æ–‡æœ¬å†…å®¹
                if hasattr(Config, 'fromstring'):
                    try:
                        cfg = Config.fromstring(meta_cfg, file_format='.py')
                    except TypeError:
                        # å…¼å®¹æ—§ç‰ˆæœ¬ç­¾å
                        cfg = Config.fromstring(meta_cfg)
                else:
                    # 3) æç«¯å…¼å®¹ï¼šå†™å…¥ä¸´æ—¶æ–‡ä»¶å† fromfile
                    import tempfile
                    with tempfile.NamedTemporaryFile('w', suffix='.py', delete=False, encoding='utf-8') as f:
                        f.write(meta_cfg)
                        tmp_cfg_path = f.name
                    cfg = Config.fromfile(tmp_cfg_path)
    
    if cfg is None:
        raise FileNotFoundError("æ— æ³•æ‰¾åˆ° Deformable-DETR é…ç½®æ–‡ä»¶")
    
    model = MODELS.build(cfg.model)
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    model = model.to(device)
    
    # å°† Config å¯¹è±¡è½¬æ¢ä¸º dict ä»¥ä¾¿åç»­å¤„ç†
    config_dict = {}
    try:
        if hasattr(cfg, '_cfg_dict'):
            config_dict = cfg._cfg_dict
        elif hasattr(cfg, 'to_dict'):
            config_dict = cfg.to_dict()
        elif hasattr(cfg, '__dict__'):
            config_dict = dict(cfg.__dict__)
    except:
        pass
    
    return model, config_dict


def _ensure_yolo_checkpoint_path(checkpoint_path: str) -> str:
    """ç¡®ä¿ YOLO æ£€æŸ¥ç‚¹è·¯å¾„ä½¿ç”¨ .pt åç¼€"""
    checkpoint_path_obj = Path(checkpoint_path)
    
    if checkpoint_path_obj.suffix.lower() == '.pt':
        return str(checkpoint_path_obj)
    
    if checkpoint_path_obj.suffix.lower() == '.pth':
        pt_path = checkpoint_path_obj.with_suffix('.pt')
        if pt_path.exists():
            return str(pt_path)
        if not checkpoint_path_obj.exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path_obj}")
        
        try:
            pt_path.symlink_to(checkpoint_path_obj)
            return str(pt_path)
        except OSError:
            import shutil
            shutil.copy2(checkpoint_path_obj, pt_path)
            return str(pt_path)
    
    return str(checkpoint_path_obj)


def load_yolov8_model(checkpoint_path: str, device: str = "cuda"):
    """åŠ è½½ YOLOv8 æ¨¡å‹"""
    yolov8_dir = Path(__file__).parent.parent.parent / "experiments" / "yolov8"
    if str(yolov8_dir) not in sys.path:
        sys.path.insert(0, str(yolov8_dir))
    from ultralytics import YOLO
    
    checkpoint_path_pt = _ensure_yolo_checkpoint_path(checkpoint_path)
    model = YOLO(checkpoint_path_pt)
    model.to(device)
    model.eval()
    
    # æ„å»ºé…ç½®å­—å…¸
    config = {
        'model_type': 'yolov8',
        'data': None
    }
    
    # å°è¯•ä»æ¨¡å‹å¯¹è±¡ä¸­è·å–æ•°æ®é›†è·¯å¾„
    try:
        if hasattr(model, 'ckpt') and model.ckpt and hasattr(model.ckpt, 'data'):
            config['data'] = model.ckpt.data
    except:
        pass
    
    return model, config


def load_yolov10_model(checkpoint_path: str, device: str = "cuda"):
    """åŠ è½½ YOLOv10 æ¨¡å‹"""
    yolov10_dir = Path(__file__).parent.parent.parent / "experiments" / "yolov10"
    if str(yolov10_dir) not in sys.path:
        sys.path.insert(0, str(yolov10_dir))
    from ultralytics import YOLO
    
    checkpoint_path_pt = _ensure_yolo_checkpoint_path(checkpoint_path)
    model = YOLO(checkpoint_path_pt)
    model.to(device)
    model.eval()
    
    # æ„å»ºé…ç½®å­—å…¸
    config = {
        'model_type': 'yolov10',
        'data': None
    }
    
    # å°è¯•ä»æ¨¡å‹å¯¹è±¡ä¸­è·å–æ•°æ®é›†è·¯å¾„
    try:
        if hasattr(model, 'ckpt') and model.ckpt and hasattr(model.ckpt, 'data'):
            config['data'] = model.ckpt.data
    except:
        pass
    
    return model, config


def evaluate_yolo_accuracy(model, config_path: str, device: str = "cuda", max_samples: int = 300) -> Dict[str, float]:
    """ä½¿ç”¨ YOLO model.val() è¿›è¡Œè¯„ä¼°ï¼ˆä»…ç²¾åº¦ï¼Œæ— æ€§èƒ½æµ‹è¯•ï¼‰"""
    try:
        print(f"  âœ“ ä½¿ç”¨ YOLO model.val() è¿›è¡Œè¯„ä¼°")
        
        results = model.val(
            data=str(config_path),
            device=device,
            verbose=False,
            max_det=300,
            batch=1
        )
        
        metrics = {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}
        
        if hasattr(results, 'box'):
            if hasattr(results.box, 'map'):
                metrics['mAP'] = float(results.box.map)
            if hasattr(results.box, 'map50'):
                metrics['AP50'] = float(results.box.map50)
            if hasattr(results.box, 'maps') and len(results.box.maps) > 1:
                metrics['APS'] = float(results.box.maps[1])
        
        print(f"  âœ“ mAP: {metrics['mAP']:.3f}, AP50: {metrics['AP50']:.3f}, APS: {metrics['APS']:.3f}")
        return metrics
        
    except Exception as e:
        print(f"  âš  YOLO è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}


def evaluate_deformable_detr_accuracy(model, config_path: str, device: str = "cuda") -> Dict[str, float]:
    """è¯„ä¼° Deformable-DETR æ¨¡å‹ï¼ˆä½¿ç”¨ mmdet/mmengine Runner.test()ï¼Œä»…ç²¾åº¦ï¼‰"""
    metrics = evaluate_deformable_detr_full(
        config_path=config_path,
        checkpoint_path=None,
        device=device,
    )
    return metrics


def _safe_get_metric(metrics: Dict, keys: List[str], default: float = 0.0) -> float:
    """Robust metric key lookup across mmdet versions."""
    for k in keys:
        if k in metrics:
            try:
                return float(metrics[k])
            except Exception:
                pass
    # å…¼å®¹ï¼šæœ‰äº› evaluator ä¼šæŠŠ key å†™æˆ 'coco/bbox_mAP' æˆ– 'bbox_mAP'
    for k, v in metrics.items():
        if isinstance(k, str):
            for cand in keys:
                if k.endswith(cand) or cand in k:
                    try:
                        return float(v)
                    except Exception:
                        pass
    return float(default)


def _move_to_device(obj, device: str):
    """Recursively move tensors to device (for mmengine/mmdet batch dict)."""
    if torch.is_tensor(obj):
        return obj.to(device)
    if isinstance(obj, dict):
        return {k: _move_to_device(v, device) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        moved = [_move_to_device(v, device) for v in obj]
        return type(obj)(moved) if not isinstance(obj, tuple) else tuple(moved)
    return obj


def evaluate_deformable_detr_full(config_path: str,
                                  checkpoint_path: Optional[str],
                                  device: str = "cuda") -> Dict[str, float]:
    """è¯„ä¼° Deformable-DETR æ¨¡å‹ï¼ˆä»…ç²¾åº¦ï¼Œä½¿ç”¨ mmdet Runner.test()ï¼‰"""
    try:
        from mmengine.config import Config
        from mmengine.runner import Runner
        from mmdet.utils import register_all_modules
    except Exception as e:
        print(f"  âš  Deformable-DETR è¯„ä¼°ä¾èµ–å¯¼å…¥å¤±è´¥: {e!r}")
        return {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}
    
    # æ³¨å†Œ mmdet æ¨¡å—ï¼ˆä¸åŒç‰ˆæœ¬ç­¾åç•¥æœ‰å·®å¼‚ï¼‰
    try:
        register_all_modules(init_default_scope=True)
    except TypeError:
        register_all_modules()
    
    cfg = Config.fromfile(config_path)
    if checkpoint_path:
        cfg.load_from = checkpoint_path
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ batch_sizeï¼ˆä¸å¼ºåˆ¶ä¸º 1ï¼ŒCOCO è¯„ä¼°ç»“æœä¸ batch_size æ— å…³ï¼‰
    # å…¶ä½™é…ç½®ï¼ˆpipeline/resize/evaluator/proposal_nums/metric_items/num_workers ç­‰ï¼‰
    # ä¸¥æ ¼æ²¿ç”¨è®­ç»ƒè„šæœ¬ï¼ˆå¦‚ train_deformable_r18.pyï¼‰ç”Ÿæˆçš„ configï¼Œä¿è¯ä¸€è‡´æ€§ã€‚
    
    # é¿å…å†™æ—¥å¿—åˆ° work_dirï¼ˆRunner éœ€è¦ä½†æˆ‘ä»¬ä¸å…³å¿ƒï¼‰
    try:
        import tempfile
        cfg.work_dir = tempfile.mkdtemp(prefix='bench_deformable_detr_')
    except Exception:
        cfg.work_dir = cfg.get('work_dir', './work_dirs/bench_deformable_detr')
    
    runner = Runner.from_cfg(cfg)
    runner.model = runner.model.to(device)
    runner.model.eval()
    
    # ç²¾åº¦è¯„ä¼°ï¼ˆCOCO mAPï¼‰
    test_metrics = runner.test()
    
    mAP = _safe_get_metric(test_metrics, ['coco/bbox_mAP', 'bbox_mAP', 'mAP'], 0.0)
    AP50 = _safe_get_metric(test_metrics, ['coco/bbox_mAP_50', 'bbox_mAP_50', 'AP50', 'mAP_50'], 0.0)
    APS = _safe_get_metric(test_metrics, ['coco/bbox_mAP_s', 'bbox_mAP_s', 'APS', 'mAP_s'], 0.0)
    
    return {'mAP': mAP, 'AP50': AP50, 'APS': APS}


def _get_outputs_info(outputs: Dict) -> Tuple[torch.Tensor, torch.Tensor, bool]:
    """ä»æ¨¡å‹è¾“å‡ºä¸­æå– logits å’Œ boxes
    
    æ³¨æ„ï¼šDSET å’Œ RT-DETR å‡ä½¿ç”¨ Focal Lossï¼Œæ¨ç†æ—¶åº”ä½¿ç”¨ Sigmoid æ¿€æ´»
    """
    if 'pred_logits' in outputs:
        return outputs['pred_logits'], outputs['pred_boxes'], True  # RT-DETR: sigmoid
    elif 'class_scores' in outputs:
        return outputs['class_scores'], outputs['bboxes'], True  # DSET: sigmoid (Focal Loss)
    return None, None, False


def _collect_predictions_for_coco(outputs: Dict, targets: List[Dict], batch_idx: int,
                                  all_predictions: List, all_targets: List,
                                  img_w: int, img_h: int, batch_size: int) -> None:
    """æ”¶é›†é¢„æµ‹ç»“æœç”¨äº COCO è¯„ä¼°"""
    pred_logits, pred_boxes, use_sigmoid = _get_outputs_info(outputs)
    if pred_logits is None:
        return
    
    batch_size_actual = pred_logits.shape[0]
    
    for i in range(batch_size_actual):
        # å°è¯•ä» target ä¸­ç›´æ¥è·å–åŸå§‹ ID
        if i < len(targets) and 'image_id' in targets[i]:
            img_id = int(targets[i]['image_id'].item())
        else:
            # å¦‚æœæ²¡æœ‰ï¼Œå†é€€å›åˆ°ç´¢å¼•è®¡ç®—é€»è¾‘ï¼Œä½†è¦ç¡®ä¿ batch_size æ­£ç¡®
            img_id = batch_idx * batch_size + i
        
        if use_sigmoid:
            pred_scores = torch.sigmoid(pred_logits[i])
        else:
            pred_scores = torch.softmax(pred_logits[i], dim=-1)
        max_scores, pred_classes = torch.max(pred_scores, dim=-1)
        
        valid_boxes_mask = ~torch.all(pred_boxes[i] == 1.0, dim=1)
        valid_indices = torch.where(valid_boxes_mask)[0]
        
        if len(valid_indices) > 0:
            filtered_boxes = pred_boxes[i][valid_indices]
            filtered_classes = pred_classes[valid_indices]
            filtered_scores = max_scores[valid_indices]
            
            if filtered_boxes.shape[0] > 0:
                boxes_coco = torch.zeros_like(filtered_boxes)
                if filtered_boxes.max() <= 1.0:
                    boxes_coco[:, 0] = (filtered_boxes[:, 0] - filtered_boxes[:, 2] / 2) * img_w
                    boxes_coco[:, 1] = (filtered_boxes[:, 1] - filtered_boxes[:, 3] / 2) * img_h
                    boxes_coco[:, 2] = filtered_boxes[:, 2] * img_w
                    boxes_coco[:, 3] = filtered_boxes[:, 3] * img_h
                else:
                    boxes_coco = filtered_boxes.clone()
                
                boxes_coco[:, 0] = torch.clamp(boxes_coco[:, 0], 0, img_w)
                boxes_coco[:, 1] = torch.clamp(boxes_coco[:, 1], 0, img_h)
                boxes_coco[:, 2] = torch.clamp(boxes_coco[:, 2], 1, img_w)
                boxes_coco[:, 3] = torch.clamp(boxes_coco[:, 3], 1, img_h)
                
                for j in range(boxes_coco.shape[0]):
                    x, y, w, h = boxes_coco[j].cpu().numpy()
                    all_predictions.append({
                        'image_id': img_id,
                        'category_id': int(filtered_classes[j].item()) + 1,
                        'bbox': [float(x), float(y), float(w), float(h)],
                        'score': float(filtered_scores[j].item())
                    })
        
        # å¤„ç†çœŸå®æ ‡ç­¾
        if i < len(targets) and 'labels' in targets[i] and 'boxes' in targets[i]:
            true_labels = targets[i]['labels']
            true_boxes = targets[i]['boxes']
            
            if len(true_labels) > 0:
                max_val = float(true_boxes.max().item()) if true_boxes.numel() > 0 else 0.0
                true_boxes_coco = torch.zeros_like(true_boxes)
                
                if max_val <= 1.0 + 1e-6:
                    true_boxes_coco[:, 0] = (true_boxes[:, 0] - true_boxes[:, 2] / 2) * img_w
                    true_boxes_coco[:, 1] = (true_boxes[:, 1] - true_boxes[:, 3] / 2) * img_h
                    true_boxes_coco[:, 2] = true_boxes[:, 2] * img_w
                    true_boxes_coco[:, 3] = true_boxes[:, 3] * img_h
                else:
                    true_boxes_coco = true_boxes.clone()
                
                true_boxes_coco[:, 0] = torch.clamp(true_boxes_coco[:, 0], 0, img_w)
                true_boxes_coco[:, 1] = torch.clamp(true_boxes_coco[:, 1], 0, img_h)
                true_boxes_coco[:, 2] = torch.clamp(true_boxes_coco[:, 2], 1, img_w)
                true_boxes_coco[:, 3] = torch.clamp(true_boxes_coco[:, 3], 1, img_h)
                
                has_iscrowd = 'iscrowd' in targets[i]
                iscrowd_values = targets[i].get('iscrowd', torch.zeros(len(true_labels), dtype=torch.int64))
                
                for j in range(len(true_labels)):
                    x, y, w, h = true_boxes_coco[j].cpu().numpy()
                    ann_dict = {
                        'id': len(all_targets),
                        'image_id': img_id,
                        'category_id': int(true_labels[j].item()) + 1,
                        'bbox': [float(x), float(y), float(w), float(h)],
                        'area': float(w * h)
                    }
                    if has_iscrowd:
                        ann_dict['iscrowd'] = int(iscrowd_values[j].item())
                    all_targets.append(ann_dict)


def evaluate_accuracy(model, config_path: str, device: str = "cuda", 
                      model_type: str = "dset", max_samples: int = 300) -> Dict[str, float]:
    """ä½¿ç”¨ pycocotools åœ¨éªŒè¯é›†ä¸Šè¿è¡Œ COCO è¯„ä¼°ï¼ˆä»…ç²¾åº¦ï¼Œæ— æ€§èƒ½æµ‹è¯•ï¼‰"""
    try:
        # å¯¼å…¥ Trainer
        if model_type == "dset":
            try:
                from experiments.dset.train import DSETTrainer
            except ImportError:
                dset_dir = Path(config_path).parent.parent
                if str(dset_dir) not in sys.path:
                    sys.path.insert(0, str(dset_dir))
                from train import DSETTrainer
            TrainerClass = DSETTrainer
        elif model_type == "rtdetr":
            rtdetr_dir = Path(config_path).parent.parent
            if str(rtdetr_dir) not in sys.path:
                sys.path.insert(0, str(rtdetr_dir))
            from train import RTDETRTrainer
            TrainerClass = RTDETRTrainer
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        # åŠ è½½é…ç½®ï¼ˆä¸å¼ºåˆ¶ batch_sizeï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ä»¥åŠ é€Ÿè¯„ä¼°ï¼‰
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        if 'misc' not in config:
            config['misc'] = {}
        config['misc']['device'] = device
        
        # åˆ›å»º DataLoaderï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ batch_sizeï¼‰
        if model_type == "dset":
            trainer = TrainerClass(config, config_file_path=str(config_path))
            _, val_loader = trainer._create_data_loaders()
        else:
            trainer = TrainerClass(config)
            trainer.model = model
            trainer.criterion = trainer.create_criterion()
            _, val_loader = trainer.create_datasets()
        
        dataset_size = len(val_loader.dataset)
        dataloader_length = len(val_loader)
        actual_batch_size = val_loader.batch_size if hasattr(val_loader, 'batch_size') else None
        
        print(f"  âœ“ DataLoader: {dataloader_length} batches, {dataset_size} samples (batch_size={actual_batch_size})")
        
        model.eval()
        model = model.to(device)
        
        # éªŒè¯ token pruning çŠ¶æ€
        if hasattr(model, 'encoder') and hasattr(model.encoder, 'token_pruners'):
            if model.encoder.token_pruners:
                pruner = model.encoder.token_pruners[0]
                if hasattr(pruner, 'pruning_enabled'):
                    print(f"  âœ“ Token Pruning: {'å·²æ¿€æ´»' if pruner.pruning_enabled else 'æœªæ¿€æ´»'}")
        
        all_predictions = []
        all_targets = []
        
        print(f"  è¿è¡Œè¯„ä¼°å¾ªç¯ï¼ˆä»…ç²¾åº¦è¯„ä¼°ï¼Œæ— æ€§èƒ½æµ‹è¯•ï¼‰...")
        
        with torch.no_grad():
            for batch_idx, (images, targets) in enumerate(val_loader):
                B, C, H_tensor, W_tensor = images.shape
                images = images.to(device)
                targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v 
                           for k, v in t.items()} for t in targets]
                
                # å•æ¬¡å‰å‘ä¼ æ’­è·å–è¾“å‡º
                outputs = model(images)
                
                has_predictions = isinstance(outputs, dict) and (
                    ('class_scores' in outputs and 'bboxes' in outputs) or
                    ('pred_logits' in outputs and 'pred_boxes' in outputs)
                )
                
                if has_predictions:
                    # åŠ¨æ€è·å–å½“å‰ batch çš„çœŸå®å›¾ç‰‡æ•°é‡ B
                    current_batch_actual_size = images.shape[0]
                    
                    _collect_predictions_for_coco(
                        outputs, targets, batch_idx, all_predictions, all_targets,
                        W_tensor, H_tensor, current_batch_actual_size  # ä¿®å¤ï¼šä¼ å…¥çœŸå®çš„ B
                    )
                
                # è¿›åº¦æ‰“å°ï¼šæ¯ 100 ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡
                if (batch_idx + 1) % 100 == 0:
                    print(f"    è¿›åº¦: {batch_idx + 1}/{dataloader_length}")
        
        print(f"  âœ“ å®Œæˆ: {len(val_loader)} æ ·æœ¬, {len(all_predictions)} é¢„æµ‹æ¡†")
        
        # COCO è¯„ä¼°ï¼ˆä»…ç²¾åº¦æŒ‡æ ‡ï¼‰
        metrics = _compute_coco_metrics(all_predictions, all_targets, H_tensor, W_tensor)
        
        return metrics
        
    except Exception as e:
        print(f"  âš  COCO è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}


def _compute_coco_metrics(predictions: List[Dict], targets: List[Dict],
                          img_h: int = 736, img_w: int = 1280) -> Dict[str, float]:
    """ä½¿ç”¨ pycocotools è®¡ç®— COCO æŒ‡æ ‡"""
    try:
        if len(predictions) == 0:
            return {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}
        
        categories = [
            {'id': 1, 'name': 'Car'}, {'id': 2, 'name': 'Truck'},
            {'id': 3, 'name': 'Van'}, {'id': 4, 'name': 'Bus'},
            {'id': 5, 'name': 'Pedestrian'}, {'id': 6, 'name': 'Cyclist'},
            {'id': 7, 'name': 'Motorcyclist'}, {'id': 8, 'name': 'Trafficcone'}
        ]
        
        image_ids = set(t['image_id'] for t in targets)
        coco_gt = {
            'images': [{'id': img_id, 'width': img_w, 'height': img_h} for img_id in image_ids],
            'annotations': targets,
            'categories': categories,
            'info': {'description': 'DAIR-V2X Dataset', 'version': '1.0', 'year': 2024}
        }
        
        coco_gt_obj = COCO()
        coco_gt_obj.dataset = coco_gt
        
        old_stdout = sys.stdout
        sys.stdout = StringIO()
        try:
            coco_gt_obj.createIndex()
            coco_dt = coco_gt_obj.loadRes(predictions)
        finally:
            sys.stdout = old_stdout
        
        coco_eval = COCOeval(coco_gt_obj, coco_dt, 'bbox')
        
        sys.stdout = StringIO()
        try:
            coco_eval.evaluate()
            coco_eval.accumulate()
            coco_eval.summarize()
        finally:
            sys.stdout = old_stdout
        
        stats = coco_eval.stats
        metrics = {
            'mAP': float(stats[0]) if len(stats) > 0 else 0.0,
            'AP50': float(stats[1]) if len(stats) > 1 else 0.0,
            'APS': float(stats[3]) if len(stats) > 3 else 0.0
        }
        
        print(f"  âœ“ mAP: {metrics['mAP']:.3f}, AP50: {metrics['AP50']:.3f}, APS: {metrics['APS']:.3f}")
        return metrics
        
    except Exception as e:
        print(f"  âš  COCO æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        return {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}


def _resolve_path(path_str: str, project_root: Path) -> Path:
    """è§£æè·¯å¾„"""
    if not path_str:
        return None
    path = Path(path_str)
    return path if path.is_absolute() else project_root / path


def _get_yolo_data_path(model, model_config: Dict, project_root: Path) -> Optional[Path]:
    """è·å– YOLO æ¨¡å‹çš„æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„"""
    data_config = model_config.get('data', None)
    if data_config:
        data_path = _resolve_path(data_config, project_root)
        if data_path and data_path.exists():
            return data_path
    
    try:
        if hasattr(model, 'ckpt') and model.ckpt and hasattr(model.ckpt, 'data'):
            data_path = _resolve_path(model.ckpt.data, project_root)
            if data_path and data_path.exists():
                return data_path
    except:
        pass
    
    return None


def evaluate_single_model(model_name: str, model_config: Dict, args, project_root: Path, debug: bool = False) -> Optional[Dict]:
    """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
    print("\n" + "=" * 80)
    print(f"è¯„ä¼°æ¨¡å‹: {model_name}")
    print("=" * 80)
    
    model_type = model_config.get('type', args.model_type)
    config_path_str = model_config.get('config', args.config)
    checkpoint_path_str = model_config.get('checkpoint', None)
    input_size_override = model_config.get('input_size', None)
    
    # ç¡®å®š input_size
    if input_size_override is not None:
        input_size = input_size_override
    elif "yolo" in model_type.lower():
        input_size = [1280, 1280]
    else:
        input_size = [736, 1280]
    
    config_path = _resolve_path(config_path_str, project_root) if config_path_str else None
    
    # å¤„ç†æ£€æŸ¥ç‚¹è·¯å¾„
    if checkpoint_path_str:
        checkpoint_path = _resolve_path(checkpoint_path_str, project_root)
        if not checkpoint_path.exists():
            print(f"âš  æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            return None
    else:
        logs_dir = _resolve_path(args.logs_dir, project_root)
        checkpoint_path = find_latest_best_model(logs_dir, model_type)
        if checkpoint_path is None:
            print(f"âš  æ— æ³•æ‰¾åˆ°æ£€æŸ¥ç‚¹")
            return None
    
    print(f"  ç±»å‹: {model_type}, è¾“å…¥: {input_size[0]}x{input_size[1]}")
    print(f"  æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # åŠ è½½æ¨¡å‹å’Œé…ç½®
    is_yolo_model = model_type.startswith("yolov8") or model_type.startswith("yolov10")
    config = None
    try:
        if model_type == "dset":
            model, config = load_dset_model(str(config_path), str(checkpoint_path), args.device)
        elif model_type == "rtdetr":
            model, config = load_rtdetr_model(str(config_path), str(checkpoint_path), args.device)
        elif model_type == "deformable-detr":
            model = load_deformable_detr_model(str(checkpoint_path), args.device, 
                                               config_path=str(config_path) if config_path else None)
            # åŠ è½½é…ç½®
            if config_path:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
        elif model_type.startswith("yolov8"):
            model, config = load_yolov8_model(str(checkpoint_path), args.device)
        elif model_type.startswith("yolov10"):
            model, config = load_yolov10_model(str(checkpoint_path), args.device)
        else:
            print(f"  âš  ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
            return None
        print("  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"  âš  æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    # è®¡ç®—å‚æ•°é‡å’Œç†è®º FLOPs
    input_size_tuple = (1, 3, input_size[0], input_size[1])
    total_params_m, active_params_m, base_flops_g, theory_flops_g = get_model_info(
        model, input_size_tuple, is_yolo=is_yolo_model, config=config, model_type=model_type, debug=debug
    )
    print(f"  âœ“ Total Params: {total_params_m:.2f}M, Active Params: {active_params_m:.2f}M")
    print(f"  âœ“ Base FLOPs: {base_flops_g:.2f}G, Theory FLOPs: {theory_flops_g:.2f}G")
    
    # è¯„ä¼°ï¼ˆä»…ç²¾åº¦ï¼‰
    metrics = {'mAP': 0.0, 'AP50': 0.0, 'APS': 0.0}
    
    if model_type in ["dset", "rtdetr"] and config_path:
        metrics = evaluate_accuracy(model, str(config_path), args.device, 
                                    model_type=model_type, max_samples=999999)
    elif model_type == "deformable-detr" and config_path:
        metrics = evaluate_deformable_detr_full(
            config_path=str(config_path),
            checkpoint_path=str(checkpoint_path),
            device=args.device,
        )
    elif is_yolo_model:
        data_config_path = _get_yolo_data_path(model, model_config, project_root)
        if data_config_path:
            metrics = evaluate_yolo_accuracy(model, str(data_config_path), args.device, 999999)
        else:
            print(f"  âš  YOLO æ¨¡å‹éœ€è¦æ•°æ®é›†é…ç½®æ–‡ä»¶")
    
    # æ¸…ç†æ˜¾å­˜
    del model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return {
        'model_name': model_name,
        'model_type': model_type,
        'total_params_m': total_params_m,
        'active_params_m': active_params_m,
        'base_flops_g': base_flops_g,
        'theory_flops_g': theory_flops_g,
        'mAP': metrics.get('mAP', 0.0),
        'AP50': metrics.get('AP50', 0.0),
        'APS': metrics.get('APS', 0.0),
        'input_size': f"{input_size[0]}x{input_size[1]}"
    }


def print_summary_table(results: List[Dict], gpu_name: str = "GPU", save_csv: bool = True, max_samples: int = 300):
    """æ‰“å°ç»“æœæ±‡æ€»è¡¨æ ¼å¹¶ä¿å­˜ä¸º CSVï¼ˆç†è®ºæ•ˆç‡è§†è§’ï¼‰"""
    if not results:
        print("\nâš  æ²¡æœ‰è¯„ä¼°ç»“æœ")
        return
    
    print("\n" + "=" * 140)
    print("THEORETICAL EFFICIENCY".center(140))
    print("=" * 140)
    
    header = f"{'Model':<25} {'Total':<10} {'Active':<10} {'Theory':<10} {'Resolution':<12} {'mAP':<8} {'AP50':<8} {'APS':<8}"
    print(header)
    print("-" * 140)
    print(f"{'':<25} {'Params':<10} {'Params':<10} {'GFLOPs':<10} {'':<12} {'':<8} {'':<8} {'':<8}")
    print(f"{'':<25} {'(M)':<10} {'(M)':<10} {'':<10} {'':<12} {'':<8} {'':<8} {'':<8}")
    print("-" * 140)
    
    csv_rows = [['Model', 'Type', 'Total Params(M)', 'Active Params(M)', 'Theory GFLOPs', 
                 'Resolution', 'mAP', 'AP50', 'APS', 'Input']]
    
    for r in results:
        name = r.get('model_name', 'Unknown')[:24]
        total_params = f"{r.get('total_params_m', 0):.2f}" if r.get('total_params_m', 0) > 0 else "N/A"
        active_params = f"{r.get('active_params_m', 0):.2f}" if r.get('active_params_m', 0) > 0 else "N/A"
        theory_flops = f"{r.get('theory_flops_g', 0):.2f}" if r.get('theory_flops_g', 0) > 0 else "N/A"
        resolution = r.get('input_size', 'N/A')
        
        mAP = f"{r.get('mAP', 0):.3f}" if r.get('mAP', 0) > 0 else "N/A"
        ap50 = f"{r.get('AP50', 0):.3f}" if r.get('AP50', 0) > 0 else "N/A"
        aps = f"{r.get('APS', 0):.3f}" if r.get('APS', 0) > 0 else "N/A"
        
        print(f"{name:<25} {total_params:<10} {active_params:<10} {theory_flops:<10} {resolution:<12} {mAP:<8} {ap50:<8} {aps:<8}")
        
        csv_rows.append([name, r.get('model_type', ''), total_params, active_params, theory_flops, 
                        resolution, mAP, ap50, aps, r.get('input_size', '')])
    
    print("-" * 140)
    print("Note: Theoretical FLOPs are calculated based on sparsity-aware projection (Top-K expert and token pruning ratio).")
    print("=" * 140)
    
    if save_csv:
        import csv
        csv_path = Path(project_root) / "benchmark_results.csv"
        try:
            with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                csv.writer(f).writerows(csv_rows)
            print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
        except Exception as e:
            print(f"\nâš  ä¿å­˜ CSV å¤±è´¥: {e}")


def find_latest_best_model(logs_dir: Path, model_type: str = "dset") -> Optional[Path]:
    """åœ¨ logs ç›®å½•ä¸‹æ‰¾åˆ°æœ€æ–°çš„ best_model.pth æˆ– best.pt
    
    ä¼˜å…ˆæŸ¥æ‰¾å„å®éªŒç›®å½•ä¸‹ weights/ æ–‡ä»¶å¤¹å†…çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå¹¶æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºè¿”å›æœ€æ–°è€…ã€‚
    """
    best_models = []
    
    if model_type == "deformable-detr":
        # Deformable-DETR: æŸ¥æ‰¾ best_*.pth
        # ä¼˜å…ˆæŸ¥æ‰¾ weights/ ç›®å½•
        best_models.extend(list(logs_dir.rglob("weights/best_*.pth")))
        best_models.extend(list(logs_dir.rglob("best_*.pth")))
    elif model_type.startswith("yolov"):
        # YOLO: æŸ¥æ‰¾ best.pt æˆ– best_model.pth
        # ä¼˜å…ˆæŸ¥æ‰¾ weights/ ç›®å½•
        best_models.extend(list(logs_dir.rglob("weights/best.pt")))
        best_models.extend(list(logs_dir.rglob("weights/best_model.pth")))
        best_models.extend(list(logs_dir.rglob("best.pt")))
        best_models.extend(list(logs_dir.rglob("best_model.pth")))
    else:
        # DSET/RT-DETR: æŸ¥æ‰¾ best_model.pth
        # ä¼˜å…ˆæŸ¥æ‰¾ weights/ ç›®å½•
        best_models.extend(list(logs_dir.rglob("weights/best_model.pth")))
        best_models.extend(list(logs_dir.rglob("best_model.pth")))
    
    # å»é‡å¹¶è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
    best_models = list(set(best_models))
    best_models = [p for p in best_models if p.exists() and p.is_file()]
    
    if not best_models:
        return None
    
    # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
    best_models.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return best_models[0]


def _format_evaluation_results(model_type: str, total_params_m: float, active_params_m: float, 
                               base_flops_g: float, theory_flops_g: float,
                               metrics: Dict[str, float],
                               input_resolution: Tuple[int, int], is_yolo: bool = False,
                               gpu_name: str = "GPU") -> None:
    """æ ¼å¼åŒ–å¹¶è¾“å‡ºè¯„ä¼°ç»“æœï¼ˆç†è®ºæ•ˆç‡è§†è§’ï¼‰"""
    model_names = {
        'dset': 'DSET', 'rtdetr': 'RT-DETRv2', 'deformable-detr': 'Deformable-DETR',
        'yolov8s': 'YOLOv8-s', 'yolov8m': 'YOLOv8-m',
        'yolov10s': 'YOLOv10-s', 'yolov10m': 'YOLOv10-m'
    }
    name = model_names.get(model_type, model_type.upper())
    
    if is_yolo:
        res = f"{max(input_resolution)}x{max(input_resolution)}"
    else:
        res = f"{input_resolution[0]}x{input_resolution[1]}"
    
    print("\n" + "=" * 70)
    print(f"Model: {name} | Input: {res}")
    print("-" * 70)
    print(f"Total Params: {total_params_m:.2f}M | Active Params: {active_params_m:.2f}M")
    print(f"Base FLOPs: {base_flops_g:.2f}G | Theory FLOPs: {theory_flops_g:.2f}G")
    print(f"mAP: {metrics['mAP']:.3f} | AP50: {metrics['AP50']:.3f} | APS: {metrics['APS']:.3f}")
    print("=" * 70)
    print("Note: Theoretical FLOPs are calculated based on sparsity-aware projection (Top-1 expert and token pruning ratio).")


def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨')
    parser.add_argument('--logs_dir', type=str, default='experiments/dset/logs')
    parser.add_argument('--config', type=str, default='experiments/dset/configs/dset4_r18_ratio0.5.yaml')
    parser.add_argument('--checkpoint', type=str, default=None)
    parser.add_argument('--input_size', type=int, nargs=2, default=None)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--model_type', type=str, default='dset',
                       choices=['dset', 'rtdetr', 'deformable-detr', 
                               'yolov8s', 'yolov8m', 'yolov10s', 'yolov10m'])
    parser.add_argument('--rtdetr_config', type=str, default=None)
    parser.add_argument('--deformable_work_dir', type=str, default=None)
    parser.add_argument('--deformable_config', type=str, default=None)
    parser.add_argument('--models_config', type=str, default=None)
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°æ‰€æœ‰æ¨¡å—å±‚åä»¥ä¾¿è°ƒè¯• MoE å±‚è¯†åˆ«')
    
    args = parser.parse_args()
    
    # GPU/CPU åç§°
    if args.device == 'cuda' and torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
    else:
        import platform
        gpu_name = f"CPU ({platform.processor() or 'Unknown'})"
    
    print("=" * 80)
    print("æ€§èƒ½å¯¹æ¯”è¡¨ç”Ÿæˆè„šæœ¬")
    print("=" * 80)
    
    # æ„é€ é…ç½®
    if args.models_config:
        json_config_path = _resolve_path(args.models_config, project_root)
        if not json_config_path.exists():
            print(f"é”™è¯¯: JSON é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {json_config_path}")
            return
        with open(json_config_path, 'r', encoding='utf-8') as f:
            json_config = json.load(f)
        print(f"âœ“ æ‰¹é‡è¯„ä¼°: {len(json_config)} ä¸ªæ¨¡å‹\n")
    else:
        if "yolo" in args.model_type.lower():
            input_size = [1280, 1280]
        else:
            input_size = [736, 1280]
        input_size = args.input_size or input_size
        
        single_config = {
            'type': args.model_type,
            'config': args.rtdetr_config or args.deformable_config or args.config,
            'checkpoint': args.checkpoint,
            'input_size': input_size
        }
        json_config = {'single_model': single_config}
        print(f"âœ“ å•æ¨¡å‹è¯„ä¼°\n")
    
    # è¯„ä¼°
    all_results = []
    for model_name, model_config in json_config.items():
        if not isinstance(model_config, dict):
            continue
        result = evaluate_single_model(model_name, model_config, args, project_root, debug=args.debug)
        if result:
            all_results.append(result)
    
    # è¾“å‡ºç»“æœ
    if len(all_results) > 1:
        print_summary_table(all_results, gpu_name, save_csv=True, max_samples=0)  # max_samples ä¸å†ä½¿ç”¨ï¼Œè®¾ä¸º 0
    elif all_results:
        r = all_results[0]
        _format_evaluation_results(
            r['model_type'], 
            r.get('total_params_m', 0), r.get('active_params_m', 0),
            r.get('base_flops_g', 0), r.get('theory_flops_g', 0),
            {'mAP': r['mAP'], 'AP50': r['AP50'], 'APS': r['APS']},
            (int(r['input_size'].split('x')[1]), int(r['input_size'].split('x')[0])),
            r['model_type'].startswith("yolov"), gpu_name
        )


if __name__ == '__main__':
    main()
