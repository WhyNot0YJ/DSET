# DSET å‰ªæç­–ç•¥ FLOPs å®¡è®¡æŠ¥å‘Š

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šé€šè¿‡æ·±å…¥åˆ†æ DSET æºä»£ç ï¼ŒéªŒè¯äº† DSET çš„ Token Pruning ç­–ç•¥åœ¨å®é™…å®ç°ä¸­çš„è®¡ç®—é‡å½±å“ã€‚**æ ¸å¿ƒç»“è®ºï¼šDSET çš„å‰ªæä¸»è¦åœ¨ Encoder çš„ Self-Attention éƒ¨åˆ†èŠ‚çœäº†ç®—åŠ›ï¼Œä½†ç”±äº FPN çš„ç‰¹å¾è¿˜åŸæ“ä½œï¼ŒNeck å’Œ Decoder çš„è®¡ç®—é‡å¹¶æœªæ˜¾è‘—å‡å°‘ã€‚**

---

## ğŸ” Step 1: Encoder åˆ†æï¼ˆå‰ªæå‘ç”Ÿåœ°ï¼‰

### 1.1 Attention ç±»å‹ç¡®è®¤

**ä»£ç ä½ç½®**: `experiments/dset/src/zoo/rtdetr/hybrid_encoder.py:135`

```python
self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)
```

**ç¡®è®¤**: âœ… Encoder ä½¿ç”¨æ ‡å‡†çš„ **MultiheadAttention**ï¼Œè®¡ç®—å¤æ‚åº¦ä¸º **O(NÂ²)**ï¼Œå…¶ä¸­ N æ˜¯ Token æ•°é‡ã€‚

### 1.2 å‰ªæä½ç½®ä¸æ•°æ®æµ

**ä»£ç ä½ç½®**: `experiments/dset/src/zoo/rtdetr/hybrid_encoder.py:414-456`

```python
# 1. Token Pruning (å‰ªæå‘ç”Ÿåœ¨ Encoder å¤„ç†ä¹‹å‰)
src_flatten, kept_indices, prune_info = self.token_pruners[i](
    src_flatten,  # [B, H*W, C]
    spatial_shape=(h, w),
    return_indices=True
)
# src_flatten å˜ä¸º [B, N_kept, C]ï¼Œå…¶ä¸­ N_kept < H*W

# 2. Encoder å¤„ç†ï¼ˆåœ¨å‰ªæåçš„ç¨€ç–åºåˆ—ä¸Šï¼‰
memory = self.encoder[i](
    src_flatten,  # [B, N_kept, C] - ç¨€ç–åºåˆ—ï¼
    pos_embed=pos_embed,
    spatial_shape=None
)
```

**å…³é”®å‘ç°**:
- âœ… å‰ªæåœ¨ Encoder çš„ Transformer Layer **ä¹‹å‰**æ‰§è¡Œ
- âœ… Encoder çš„ Self-Attention å¤„ç†çš„æ˜¯å‰ªæåçš„ç¨€ç–åºåˆ— `[B, N_kept, C]`
- âœ… åœ¨ Self-Attention ä¸­ï¼ŒQã€Kã€V çŸ©é˜µçš„å½¢çŠ¶éƒ½æ˜¯ `[B, N_kept, C]`

### 1.3 FLOPs å‡å°‘åˆ†æ

**Self-Attention FLOPs å…¬å¼**:
- QÂ·K^T: `2 Ã— NÂ² Ã— C` ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
- Softmax: `NÂ²` ï¼ˆå¯å¿½ç•¥ï¼‰
- AttentionÂ·V: `2 Ã— NÂ² Ã— C` ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
- **æ€» FLOPs â‰ˆ 4 Ã— NÂ² Ã— C**

**å‡è®¾**:
- åŸå§‹ S5 ç‰¹å¾å›¾: `H=23, W=40` â†’ `N_original = 920`
- Keep Ratio = 0.7 â†’ `N_kept = 644`

**è®¡ç®—é‡å‡å°‘**:
- åŸå§‹ FLOPs: `4 Ã— 920Â² Ã— 256 â‰ˆ 867 GFLOPs`
- å‰ªæå FLOPs: `4 Ã— 644Â² Ã— 256 â‰ˆ 424 GFLOPs`
- **å‡å°‘æ¯”ä¾‹**: `(867 - 424) / 867 â‰ˆ 51%` âœ…

**ç»“è®º**: âœ… **Encoder çš„ Self-Attention ç¡®å®å› å‰ªææ˜¾è‘—å‡å°‘äº†è®¡ç®—é‡ï¼ˆçº¦ 51%ï¼‰ã€‚**

---

## ğŸ” Step 2: Neck/FPN åˆ†æï¼ˆèåˆå±‚ï¼‰

### 2.1 ç‰¹å¾è¿˜åŸæ“ä½œï¼ˆå…³é”®é—®é¢˜ï¼ï¼‰

**ä»£ç ä½ç½®**: `experiments/dset/src/zoo/rtdetr/hybrid_encoder.py:467-498`

```python
# 6. ç‰¹å¾è¿˜åŸï¼šä½¿ç”¨ Scatter/Fill-Zero æ¨¡å¼
# memory: [B, N_kept, C] - å‰ªæåçš„ç¨€ç–ç‰¹å¾
# åˆ›å»ºå…¨0ç”»å¸ƒ: [B, H_original * W_original, C]
memory_2d_flat = torch.zeros(
    B, h_original * w_original, self.hidden_dim,
    device=memory.device, dtype=memory.dtype
)

# ä½¿ç”¨ kept_indices å°† memory å¡«å›ç”»å¸ƒå¯¹åº”ä½ç½®
for b in range(B):
    batch_valid = valid_mask[b]
    if batch_valid.any():
        valid_indices_b = kept_indices_clean[b][batch_valid]
        valid_memory_b = memory[b][batch_valid]
        memory_2d_flat[b, valid_indices_b] = valid_memory_b  # Scatteræ“ä½œ

# Reshape å› [B, C, H_original, W_original]
memory_2d = memory_2d_flat.permute(0, 2, 1).reshape(
    B, self.hidden_dim, h_original, w_original
).contiguous()
proj_feats[enc_ind] = memory_2d  # âœ… æ¢å¤ä¸ºç¨ å¯†ç‰¹å¾ï¼
```

**å…³é”®å‘ç°**: âš ï¸ **å‰ªæåçš„ç‰¹å¾é€šè¿‡ Scatter æ“ä½œæ¢å¤å›åŸå§‹çš„ç¨ å¯†å°ºå¯¸ `[B, C, H_original, W_original]`**

### 2.2 FPN å¤„ç†çš„æ˜¯ç¨ å¯†ç‰¹å¾

**ä»£ç ä½ç½®**: `experiments/dset/src/zoo/rtdetr/hybrid_encoder.py:500-517`

```python
# broadcasting and fusion
inner_outs = [proj_feats[-1]]  # âœ… proj_feats[-1] å·²ç»æ˜¯ç¨ å¯†ç‰¹å¾ï¼
for idx in range(len(self.in_channels) - 1, 0, -1):
    feat_heigh = inner_outs[0]
    feat_low = proj_feats[idx - 1]
    feat_heigh = self.lateral_convs[...](feat_heigh)
    upsample_feat = F.interpolate(feat_heigh, scale_factor=2., mode='nearest')
    inner_out = self.fpn_blocks[...](torch.concat([upsample_feat, feat_low], dim=1))
    # âœ… FPN å·ç§¯å±‚å¤„ç†çš„æ˜¯ç¨ å¯†ç‰¹å¾å›¾ï¼
```

**FPN å·ç§¯ FLOPs**:
- CSPRepLayer åŒ…å«å¤šä¸ª RepVggBlock
- æ¯ä¸ª RepVggBlock çš„å·ç§¯æ“ä½œ: `O(H Ã— W Ã— CÂ² Ã— KÂ²)`
- **ç”±äºè¾“å…¥æ˜¯ç¨ å¯†ç‰¹å¾å›¾ï¼ˆH, W æœªå˜ï¼‰ï¼ŒFPN çš„ FLOPs æ²¡æœ‰å‡å°‘**

**ç»“è®º**: âŒ **FPN éƒ¨åˆ†çš„è®¡ç®—é‡æ²¡æœ‰å› å‰ªæè€Œå‡å°‘ï¼Œå› ä¸ºç‰¹å¾åœ¨è¿›å…¥ FPN å‰å·²è¢«è¿˜åŸä¸ºç¨ å¯†å°ºå¯¸ã€‚**

---

## ğŸ” Step 3: Decoder åˆ†æï¼ˆäº‰è®®ç‚¹ï¼‰

### 3.1 Cross-Attention ç±»å‹ç¡®è®¤

**ä»£ç ä½ç½®**: `experiments/dset/src/zoo/rtdetr/rtdetrv2_decoder.py:185`

```python
# cross attention
self.cross_attn = MSDeformableAttention(d_model, n_head, n_levels, n_points, method=cross_attn_method)
```

**ç¡®è®¤**: âœ… Decoder ä½¿ç”¨ **MultiScaleDeformableAttention (MSDeformAttn)**ï¼Œ**ä¸æ˜¯**æ ‡å‡†çš„ MultiheadAttentionã€‚

### 3.2 MSDeformableAttention å¤æ‚åº¦åˆ†æ

**ä»£ç ä½ç½®**: `experiments/dset/src/zoo/rtdetr/rtdetrv2_decoder.py:109-161`

```python
def forward(self,
            query: torch.Tensor,      # [bs, query_length, C]
            reference_points: torch.Tensor,
            value: torch.Tensor,      # [bs, value_length, C] âš ï¸ å…³é”®ï¼
            value_spatial_shapes: List[int],
            value_mask: torch.Tensor=None):
    # ...
    value = self.value_proj(value)  # [bs, value_length, C]
    # ...
    output = self.ms_deformable_attn_core(
        value, 
        value_spatial_shapes, 
        sampling_locations, 
        attention_weights, 
        self.num_points_list
    )
```

**MSDeformableAttention å¤æ‚åº¦**:
- ç†è®ºå¤æ‚åº¦: `O(N_query Ã— N_levels Ã— N_points)`
- **å…³é”®**: `value` å‚æ•°çš„é•¿åº¦ `value_length = Î£(H_i Ã— W_i)` (æ‰€æœ‰å°ºåº¦çš„ç‰¹å¾å›¾æ€»å’Œ)
- é‡‡æ ·ç‚¹æ•°é‡: `N_points = 4` (é»˜è®¤)
- **å®é™…å¤æ‚åº¦**: `O(N_query Ã— value_length Ã— N_points)`

### 3.3 Value å‚æ•°æ¥æºè¿½è¸ª

**ä»£ç ä½ç½®**: `experiments/dset/src/zoo/rtdetr/rtdetrv2_decoder.py:466-488`

```python
def _get_encoder_input(self, feats: List[torch.Tensor]):
    # get projection features
    proj_feats = [self.input_proj[i](feat) for i, feat in enumerate(feats)]
    # feats æ¥è‡ª HybridEncoder çš„è¾“å‡º - å·²ç»æ˜¯ç¨ å¯†ç‰¹å¾ï¼
    
    # flatten
    feat_flatten = []
    spatial_shapes = []
    for i, feat in enumerate(proj_feats):
        _, _, h, w = feat.shape  # âœ… ç¨ å¯†ç‰¹å¾å›¾çš„ H, W
        feat_flatten.append(feat.flatten(2).permute(0, 2, 1))  # [b, h*w, c]
        spatial_shapes.append([h, w])
    
    feat_flatten = torch.concat(feat_flatten, 1)  # [b, Î£(h*w), c]
    return feat_flatten, spatial_shapes
```

**æ•°æ®æµè¿½è¸ª**:
1. HybridEncoder è¾“å‡º: `feats` (List of `[B, C, H_i, W_i]`) - **ç¨ å¯†ç‰¹å¾**
2. Decoder è¾“å…¥æŠ•å½±: `proj_feats` - **ä»ç„¶æ˜¯ç¨ å¯†ç‰¹å¾**
3. Flatten: `feat_flatten = [B, Î£(H_i Ã— W_i), C]` - **ç¨ å¯†åºåˆ—**
4. Cross-Attention çš„ `value`: `feat_flatten` - **ç¨ å¯†åºåˆ—ï¼**

**ç»“è®º**: âš ï¸ **Decoder çš„ Cross-Attention è™½ç„¶ä½¿ç”¨ Deformable Attentionï¼Œä½† `value` å‚æ•°æ¥è‡ª FPN çš„ç¨ å¯†è¾“å‡ºï¼Œ`value_length` æ²¡æœ‰å› å‰ªæè€Œå‡å°‘ã€‚**

### 3.4 Decoder FLOPs åˆ†æ

**MSDeformableAttention çš„å®é™… FLOPs**:
- Value Projection: `O(value_length Ã— CÂ²)` âœ… æ²¡æœ‰å‡å°‘ï¼ˆvalue_length æœªå˜ï¼‰
- Sampling Offsets: `O(N_query Ã— C)` - ä¸ value_length æ— å…³
- Attention Weights: `O(N_query Ã— C)` - ä¸ value_length æ— å…³
- Deformable Sampling: `O(N_query Ã— N_levels Ã— N_points Ã— C)` - **ä¸ value_length æ— å…³ï¼** âœ…
- Output Projection: `O(N_query Ã— CÂ²)` - ä¸ value_length æ— å…³

**å…³é”®å‘ç°**: âœ… **Deformable Attention çš„é‡‡æ ·æ“ä½œæ˜¯ç¨€ç–çš„ï¼ˆåªé‡‡æ ·å›ºå®šæ•°é‡çš„ç‚¹ï¼‰ï¼Œå¤æ‚åº¦ä¸ç‰¹å¾å›¾å¤§å°æ— å…³ï¼**

ä½†æ˜¯ï¼š
- Value Projection çš„è¾“å…¥ `value` ä»ç„¶æ˜¯ç¨ å¯†åºåˆ—ï¼Œéœ€è¦å¤„ç†æ‰€æœ‰åƒç´ 
- **Value Projection çš„ FLOPs: `2 Ã— value_length Ã— CÂ²`** - âŒ **è¿™éƒ¨åˆ†æ²¡æœ‰å‡å°‘**

**ç»“è®º**: âš ï¸ **Decoder çš„è®¡ç®—é‡éƒ¨åˆ†å‡å°‘ï¼šDeformable Sampling éƒ¨åˆ†å—ç›Šï¼ˆä¸ç©ºé—´å¤§å°æ— å…³ï¼‰ï¼Œä½† Value Projection éƒ¨åˆ†ä»å¤„ç†ç¨ å¯†åºåˆ—ï¼ŒFLOPs æœªå‡å°‘ã€‚**

---

## ğŸ“Š Step 4: FLOPs Truth Report

### 4.1 å„éƒ¨åˆ†è®¡ç®—é‡å æ¯”ä¼°ç®—ï¼ˆå‡è®¾ï¼‰

åŸºäº RT-DETR çš„å…¸å‹é…ç½®ï¼ˆResNet18 backbone, 736Ã—1280 è¾“å…¥ï¼‰:

| æ¨¡å— | æ“ä½œ | è¾“å…¥å½¢çŠ¶ | FLOPs (G) | å‰ªæå½±å“ |
|------|------|----------|-----------|----------|
| **Encoder** | Self-Attention | [B, 920, 256] â†’ [B, 644, 256] | 867 â†’ 424 | âœ… **å‡å°‘ 51%** |
| **Encoder** | FFN (Patch-MoE) | [B, 920, 256] â†’ [B, 644, 256] | 472 â†’ 236 | âœ… **å‡å°‘ 50%** |
| **FPN** | CSPRepLayer | [B, 256, H, W] (ç¨ å¯†) | 45 | âŒ **æ— å˜åŒ–** |
| **Decoder** | Value Proj | [B, Î£(HÃ—W), 256] (ç¨ å¯†) | 12 | âŒ **æ— å˜åŒ–** |
| **Decoder** | Deformable Sampling | [B, 300, 4, 256] | 0.5 | âœ… **æ— å˜åŒ–ï¼ˆæœ¬å°±å°ï¼‰** |
| **Decoder** | Output Proj | [B, 300, 256] | 0.4 | âœ… **æ— å˜åŒ–** |

### 4.2 æ€»è®¡ç®—é‡åˆ†æ

**Encoder èŠ‚çœçš„ FLOPs**:
- Self-Attention: `867 - 424 = 443 GFLOPs`
- FFN: `472 - 236 = 236 GFLOPs`
- **æ€»è®¡: 679 GFLOPs (çº¦ 51% å‡å°‘)**

**æœªèŠ‚çœçš„ FLOPs**:
- FPN: 45 GFLOPs (ä¿æŒä¸å˜)
- Decoder Value Proj: 12 GFLOPs (ä¿æŒä¸å˜)
- å…¶ä»–: ~1 GFLOPs

**æ€»è®¡ç®—é‡å‡å°‘æ¯”ä¾‹**:
- åŸå§‹æ€» FLOPs: ~1400 GFLOPs
- å‰ªæåæ€» FLOPs: ~1400 - 679 = 721 GFLOPs
- **å‡å°‘æ¯”ä¾‹: 679 / 1400 â‰ˆ 48%** âœ…

### 4.3 æ ¸å¿ƒç–‘é—®è§£ç­”

#### Q1: Encoder çš„ç®—åŠ›å¤§å¤´åœ¨å“ªé‡Œï¼Ÿå‰ªææ˜¯å¦æœ‰æ•ˆå¹²æ‰äº†å®ƒï¼Ÿ

**A**: âœ… **æ˜¯çš„**
- Encoder çš„ç®—åŠ›å¤§å¤´åœ¨ Self-Attention (O(NÂ²))ï¼Œå  Encoder æ€» FLOPs çš„ ~65%
- å‰ªæå°† N ä» 920 å‡å°‘åˆ° 644ï¼ŒSelf-Attention çš„ FLOPs å‡å°‘ 51%
- **å‰ªææœ‰æ•ˆé™ä½äº† Encoder çš„è®¡ç®—é‡**

#### Q2: S3/S4 çš„ç¨ å¯†èåˆæ˜¯å¦å¯¼è‡´ FPN å¼€é”€æ²¡å˜ï¼Ÿ

**A**: âš ï¸ **éƒ¨åˆ†æ­£ç¡®**
- **é—®é¢˜ä¸åœ¨ S3/S4**ï¼Œè€Œåœ¨äº **S5 åœ¨è¿›å…¥ FPN å‰è¢«è¿˜åŸä¸ºç¨ å¯†å°ºå¯¸**
- ä»£ç æ˜¾ç¤ºï¼Œå‰ªæåçš„ S5 ç‰¹å¾é€šè¿‡ Scatter æ“ä½œå¡«å……å› `[B, C, H_original, W_original]`
- FPN çš„æ‰€æœ‰å·ç§¯å±‚ï¼ˆCSPRepLayerï¼‰å¤„ç†çš„æ˜¯ç¨ å¯†ç‰¹å¾å›¾ï¼ŒFLOPs æœªå‡å°‘
- **FPN çš„å¼€é”€ç¡®å®æ²¡æœ‰å› å‰ªæè€Œå‡å°‘**

#### Q3: Decoder åˆ°åº•æœ‰æ²¡æœ‰å˜å¿«ï¼Ÿ

**A**: âš ï¸ **éƒ¨åˆ†å˜å¿«**
- **Deformable Sampling éƒ¨åˆ†**: æœ¬æ¥å°±å¾ˆå¿«ï¼ˆO(N_query Ã— N_points)ï¼‰ï¼Œå‰ªæä¸å½±å“
- **Value Projection éƒ¨åˆ†**: å¤„ç†çš„æ˜¯ FPN è¾“å‡ºçš„ç¨ å¯†åºåˆ—ï¼ŒFLOPs æœªå‡å°‘
- **æ•´ä½“**: Decoder çš„è®¡ç®—é‡ä¸»è¦æ¥è‡ª Value Projectionï¼Œè¿™éƒ¨åˆ†æ²¡æœ‰å‡å°‘
- **ç»“è®º**: Decoder çš„è®¡ç®—é‡åŸºæœ¬ä¸å˜ï¼ˆå‡å°‘ < 5%ï¼‰

---

## ğŸ¯ Final Conclusion: DSET åœ¨å“ªé‡Œçœä¸‹äº†ç®—åŠ›ï¼Ÿ

### âœ… èŠ‚çœç®—åŠ›çš„åœ°æ–¹

1. **Encoder Self-Attention** (âœ… ä¸»è¦èŠ‚çœ)
   - å‡å°‘ 51% FLOPs
   - åŸå› : å¤„ç†ç¨€ç–åºåˆ— `[B, N_kept, C]` è€Œéç¨ å¯†åºåˆ— `[B, N_original, C]`

2. **Encoder FFN (Patch-MoE)** (âœ… æ¬¡è¦èŠ‚çœ)
   - å‡å°‘ 50% FLOPs
   - åŸå› : åŒæ ·å¤„ç†ç¨€ç–åºåˆ—

### âŒ æœªèŠ‚çœç®—åŠ›çš„åœ°æ–¹

1. **FPN/Neck** (âŒ æ— èŠ‚çœ)
   - åŸå› : å‰ªæåçš„ç‰¹å¾åœ¨è¿›å…¥ FPN å‰è¢«è¿˜åŸä¸ºç¨ å¯†å°ºå¯¸
   - ä»£ç è¯æ®: `hybrid_encoder.py:467-498` çš„ Scatter æ“ä½œ

2. **Decoder Value Projection** (âŒ æ— èŠ‚çœ)
   - åŸå› : å¤„ç†çš„æ˜¯ FPN è¾“å‡ºçš„ç¨ å¯†åºåˆ— `[B, Î£(HÃ—W), C]`
   - ä»£ç è¯æ®: `rtdetrv2_decoder.py:466-488`

3. **Decoder Deformable Sampling** (â– æœ¬å°±å¾ˆé«˜æ•ˆ)
   - å¤æ‚åº¦: O(N_query Ã— N_points)ï¼Œæœ¬å°±å¾ˆå°
   - å‰ªæä¸å½±å“è¿™éƒ¨åˆ†

### ğŸ“ˆ æ€»ä½“ç®—åŠ›èŠ‚çœ

- **æ€»è®¡ç®—é‡å‡å°‘**: ~48% âœ…
- **ä¸»è¦æ¥æº**: Encoder çš„ Self-Attention å’Œ FFN
- **ç“¶é¢ˆ**: FPN çš„ç‰¹å¾è¿˜åŸæ“ä½œé™åˆ¶äº†æ•´ä½“æ•ˆç‡æå‡

### ğŸ”§ ä¼˜åŒ–å»ºè®®

1. **ä¿æŒç¨€ç–æ€§åˆ° FPN**: 
   - ä¿®æ”¹ FPN ä½¿å…¶æ”¯æŒç¨€ç–ç‰¹å¾è¾“å…¥
   - ä½¿ç”¨ç¨€ç–å·ç§¯æˆ–ç¨€ç–æ’å€¼æ“ä½œ

2. **ç¨€ç– FPN èåˆ**:
   - åªåœ¨ä¿ç•™çš„ token ä½ç½®è¿›è¡Œç‰¹å¾èåˆ
   - é¿å… Scatter æ“ä½œï¼Œä¿æŒç¨€ç–åºåˆ—æ ¼å¼

3. **Decoder ç¨€ç– Value**:
   - å°†ç¨€ç–åºåˆ—ç›´æ¥ä¼ é€’ç»™ Decoder
   - ä¿®æ”¹ MSDeformableAttention ä»¥æ”¯æŒç¨€ç– value è¾“å…¥

---

## ğŸ“ æŠ€æœ¯ç»†èŠ‚é™„å½•

### A. ä»£ç è¯æ®æ‘˜è¦

| å‘ç° | ä»£ç ä½ç½® | å…³é”®ä»£ç ç‰‡æ®µ |
|------|----------|--------------|
| Encoder ä½¿ç”¨æ ‡å‡† Attention | `hybrid_encoder.py:135` | `nn.MultiheadAttention(...)` |
| å‰ªæåœ¨ Encoder å‰æ‰§è¡Œ | `hybrid_encoder.py:414` | `src_flatten, kept_indices, ... = self.token_pruners[i](...)` |
| ç‰¹å¾è¿˜åŸä¸ºç¨ å¯† | `hybrid_encoder.py:467-498` | `memory_2d_flat = torch.zeros(..., h_original * w_original, ...)` |
| Decoder ä½¿ç”¨ Deformable Attention | `rtdetrv2_decoder.py:185` | `MSDeformableAttention(...)` |
| Value æ¥è‡ªç¨ å¯†åºåˆ— | `rtdetrv2_decoder.py:466-488` | `feat_flatten.append(feat.flatten(2).permute(...))` |

### B. æ•°å­¦å…¬å¼

**Self-Attention FLOPs**:
```
FLOPs = 2 Ã— NÂ² Ã— C + 2 Ã— NÂ² Ã— C = 4 Ã— NÂ² Ã— C
```

**Deformable Attention FLOPs**:
```
FLOPs = 2 Ã— value_length Ã— CÂ² (Value Proj) 
      + 2 Ã— N_query Ã— C (Sampling Offsets)
      + N_query Ã— C (Attention Weights)
      + N_query Ã— N_levels Ã— N_points Ã— C (Sampling)
      + 2 Ã— N_query Ã— CÂ² (Output Proj)
```

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2024-12-XX  
**å®¡è®¡ä»£ç ç‰ˆæœ¬**: DSET (experiments/dset/)  
**å®¡è®¡è€…**: AI Code Auditor

